From fa9700e7cd14fc9f96f80cdff4334f8a7bca3fd3 Mon Sep 17 00:00:00 2001
From: Haryo Pramudito <haryo@localhost.localdomain>
Date: Mon, 16 May 2022 20:03:16 +0700
Subject: [PATCH] completed challenge

---
 challenge-devops/Dockerfile                   |   8 +-
 challenge-devops/code/.gitignore              |  84 +++++
 challenge-devops/{ => code}/.ruby-gemset      |   0
 challenge-devops/{ => code}/.ruby-version     |   0
 challenge-devops/{ => code}/.tool-versions    |   0
 challenge-devops/{ => code}/Gemfile           |   4 +-
 challenge-devops/{ => code}/Gemfile.lock      |   0
 challenge-devops/{ => code}/README.md         |   0
 challenge-devops/{ => code}/Rakefile          |   0
 .../{ => code}/app/assets/config/manifest.js  |   0
 .../{ => code}/app/assets/images/.keep        |   0
 .../app/assets/javascripts/application.js     |   0
 .../app/assets/javascripts/cable.js           |   0
 .../app/assets/javascripts/channels/.keep     |   0
 .../stylesheets/1st_load_framework.css.scss   |   0
 .../assets/stylesheets/application.css.scss   |   0
 .../app/channels/application_cable/channel.rb |   0
 .../channels/application_cable/connection.rb  |   0
 .../app/controllers/application_controller.rb |   0
 .../{ => code}/app/controllers/concerns/.keep |   0
 .../app/controllers/products_controller.rb    |   0
 .../app/controllers/thank_you_controller.rb   |   0
 .../app/controllers/users_controller.rb       |   0
 .../app/controllers/visitors_controller.rb    |   0
 .../app/helpers/application_helper.rb         |   0
 .../{ => code}/app/jobs/application_job.rb    |   0
 .../app/mailers/application_mailer.rb         |   0
 .../app/models/application_record.rb          |   0
 .../{ => code}/app/models/concerns/.keep      |   0
 .../{ => code}/app/models/user.rb             |   0
 .../app/services/create_admin_service.rb      |   0
 .../app/views/devise/passwords/edit.html.erb  |   0
 .../app/views/devise/passwords/new.html.erb   |   0
 .../views/devise/registrations/edit.html.erb  |   0
 .../views/devise/registrations/new.html.erb   |   0
 .../app/views/devise/sessions/new.html.erb    |   0
 .../app/views/layouts/_messages.html.erb      |   0
 .../layouts/_nav_links_for_auth.html.erb      |   0
 .../app/views/layouts/_navigation.html.erb    |   0
 .../views/layouts/_navigation_links.html.erb  |   0
 .../app/views/layouts/application.html.erb    |   0
 .../app/views/layouts/mailer.html.erb         |   0
 .../app/views/layouts/mailer.text.erb         |   0
 .../{ => code}/app/views/pages/about.html.erb |   0
 .../{ => code}/app/views/products/product.pdf | Bin
 .../app/views/thank_you/index.html.erb        |   0
 .../{ => code}/app/views/users/_user.html.erb |   0
 .../{ => code}/app/views/users/index.html.erb |   0
 .../{ => code}/app/views/users/show.html.erb  |   0
 .../app/views/visitors/index.html.erb         |   0
 challenge-devops/{ => code}/bin/bundle        |   0
 challenge-devops/{ => code}/bin/rails         |   0
 challenge-devops/{ => code}/bin/rake          |   0
 challenge-devops/{ => code}/bin/setup         |   0
 challenge-devops/{ => code}/bin/update        |   0
 challenge-devops/{ => code}/bin/yarn          |   0
 challenge-devops/{ => code}/config.ru         |   0
 .../{ => code}/config/application.rb          |   0
 challenge-devops/{ => code}/config/boot.rb    |   0
 challenge-devops/{ => code}/config/cable.yml  |   0
 .../{ => code}/config/credentials.yml.enc     |   0
 .../{ => code}/config/database.yml            |  15 +-
 .../{ => code}/config/environment.rb          |   0
 .../config/environments/development.rb        |   0
 .../config/environments/production.rb         |   0
 .../{ => code}/config/environments/test.rb    |   0
 .../application_controller_renderer.rb        |   0
 .../{ => code}/config/initializers/assets.rb  |   0
 .../initializers/backtrace_silencers.rb       |   0
 .../initializers/content_security_policy.rb   |   0
 .../config/initializers/cookies_serializer.rb |   0
 .../{ => code}/config/initializers/devise.rb  |   0
 .../initializers/filter_parameter_logging.rb  |   0
 .../config/initializers/inflections.rb        |   0
 .../config/initializers/mime_types.rb         |   0
 .../config/initializers/wrap_parameters.rb    |   0
 .../{ => code}/config/locales/devise.en.yml   |   0
 .../{ => code}/config/locales/en.yml          |   0
 challenge-devops/{ => code}/config/master.key |   0
 challenge-devops/{ => code}/config/puma.rb    |   0
 challenge-devops/{ => code}/config/routes.rb  |   0
 .../{ => code}/config/secrets.yml             |   0
 .../{ => code}/config/storage.yml             |   0
 .../20180514085158_devise_create_users.rb     |   0
 .../20180514085200_add_name_to_users.rb       |   0
 .../20180514085203_add_role_to_users.rb       |   0
 challenge-devops/{ => code}/db/schema.rb      |   0
 challenge-devops/{ => code}/db/seeds.rb       |   0
 challenge-devops/{ => code}/lib/assets/.keep  |   0
 challenge-devops/{ => code}/lib/tasks/.keep   |   0
 challenge-devops/{ => code}/package.json      |   0
 challenge-devops/{ => code}/public/404.html   |   0
 challenge-devops/{ => code}/public/422.html   |   0
 challenge-devops/{ => code}/public/500.html   |   0
 .../public/apple-touch-icon-precomposed.png   |   0
 .../{ => code}/public/apple-touch-icon.png    |   0
 .../{ => code}/public/favicon.ico             |   0
 challenge-devops/{ => code}/public/humans.txt |   0
 challenge-devops/{ => code}/public/robots.txt |   0
 .../controllers/products_controller_spec.rb   |   0
 .../users/product_acquisition_spec.rb         |   0
 challenge-devops/{ => code}/storage/.keep     |   0
 .../test/application_system_test_case.rb      |   0
 .../{ => code}/test/controllers/.keep         |   0
 .../{ => code}/test/fixtures/.keep            |   0
 .../{ => code}/test/fixtures/files/.keep      |   0
 .../{ => code}/test/fixtures/users.yml        |   0
 .../{ => code}/test/helpers/.keep             |   0
 .../{ => code}/test/integration/.keep         |   0
 .../{ => code}/test/mailers/.keep             |   0
 challenge-devops/{ => code}/test/models/.keep |   0
 .../{ => code}/test/models/user_test.rb       |   0
 challenge-devops/{ => code}/test/system/.keep |   0
 .../{ => code}/test/test_helper.rb            |   0
 challenge-devops/{ => code}/vendor/.keep      |   0
 challenge-devops/deploy.sh                    |  54 ++++
 challenge-devops/docker-compose.yml           |  16 +
 challenge-devops/helm/opn-devops/.helmignore  |  23 ++
 challenge-devops/helm/opn-devops/Chart.yaml   |  24 ++
 .../helm/opn-devops/templates/NOTES.txt       |  22 ++
 .../helm/opn-devops/templates/_helpers.tpl    |  62 ++++
 .../helm/opn-devops/templates/deployment.yaml |  77 +++++
 .../helm/opn-devops/templates/hpa.yaml        |  28 ++
 .../helm/opn-devops/templates/ingress.yaml    |  61 ++++
 .../helm/opn-devops/templates/service.yaml    |  15 +
 .../opn-devops/templates/serviceaccount.yaml  |  12 +
 .../templates/tests/test-connection.yaml      |  15 +
 .../helm/opn-devops/values-dev.yaml           |  86 +++++
 .../helm/opn-devops/values-prod.yaml          |  86 +++++
 challenge-devops/helm/patroni/Chart.yaml      |  10 +
 challenge-devops/helm/patroni/README.md       | 156 ++++++++++
 .../helm/patroni/charts/consul/Chart.yaml     |  13 +
 .../helm/patroni/charts/consul/README.md      | 289 +++++++++++++++++
 .../patroni/charts/consul/templates/NOTES.txt |   7 +
 .../charts/consul/templates/_helpers.tpl      |  32 ++
 .../charts/consul/templates/basic-acls.yaml   |  46 +++
 .../consul/templates/consul-ingress.yaml      |  35 +++
 .../consul/templates/consul-service.yaml      |  39 +++
 .../consul/templates/consul-statefulset.yaml  | 202 ++++++++++++
 .../templates/consul-test-clusterrole.yaml    |  18 ++
 .../consul-test-clusterrolebinding.yaml       |  19 ++
 .../templates/consul-test-serviceaccount.yaml |  11 +
 .../charts/consul/templates/consul-test.yaml  |  40 +++
 .../consul/templates/gossip-secret.yaml       |  18 ++
 .../consul/templates/pod-dist-budget.yaml     |  16 +
 .../charts/consul/templates/test-config.yaml  |  24 ++
 .../charts/consul/templates/ui-service.yaml   |  22 ++
 .../helm/patroni/charts/consul/values.yaml    | 139 +++++++++
 .../helm/patroni/charts/etcd/.helmignore      |  21 ++
 .../helm/patroni/charts/etcd/Chart.yaml       |  12 +
 .../helm/patroni/charts/etcd/README.md        | 172 ++++++++++
 .../patroni/charts/etcd/templates/NOTES.txt   |   1 +
 .../charts/etcd/templates/_helpers.tpl        |  32 ++
 .../charts/etcd/templates/service.yaml        |  22 ++
 .../charts/etcd/templates/statefulset.yaml    | 217 +++++++++++++
 .../helm/patroni/charts/etcd/values.yaml      |  49 +++
 .../helm/patroni/charts/zookeeper/.helmignore |  21 ++
 .../helm/patroni/charts/zookeeper/Chart.yaml  |  15 +
 .../helm/patroni/charts/zookeeper/OWNERS      |   6 +
 .../helm/patroni/charts/zookeeper/README.md   | 140 +++++++++
 .../charts/zookeeper/templates/NOTES.txt      |   7 +
 .../charts/zookeeper/templates/_helpers.tpl   |  32 ++
 .../templates/config-jmx-exporter.yaml        |  19 ++
 .../zookeeper/templates/job-chroots.yaml      |  62 ++++
 .../templates/poddisruptionbudget.yaml        |  17 +
 .../zookeeper/templates/service-headless.yaml |  21 ++
 .../charts/zookeeper/templates/service.yaml   |  23 ++
 .../zookeeper/templates/statefulset.yaml      | 183 +++++++++++
 .../helm/patroni/charts/zookeeper/values.yaml | 294 ++++++++++++++++++
 .../helm/patroni/requirements.lock            |  12 +
 .../helm/patroni/requirements.yaml            |  13 +
 .../helm/patroni/templates/NOTES.txt          |  25 ++
 .../helm/patroni/templates/_helpers.tpl       |  43 +++
 .../helm/patroni/templates/ep-patroni.yaml    |  10 +
 .../helm/patroni/templates/role-patroni.yaml  |  48 +++
 .../templates/rolebinding-patroni.yaml        |  18 ++
 .../helm/patroni/templates/sec-patroni.yaml   |  14 +
 .../templates/serviceaccount-patroni.yaml     |  11 +
 .../templates/statefulset-patroni.yaml        | 208 +++++++++++++
 .../helm/patroni/templates/svc-patroni.yaml   |  16 +
 challenge-devops/helm/patroni/values-dev.yaml | 127 ++++++++
 .../helm/patroni/values-prod.yaml             | 127 ++++++++
 182 files changed, 3854 insertions(+), 12 deletions(-)
 create mode 100644 challenge-devops/code/.gitignore
 rename challenge-devops/{ => code}/.ruby-gemset (100%)
 rename challenge-devops/{ => code}/.ruby-version (100%)
 rename challenge-devops/{ => code}/.tool-versions (100%)
 rename challenge-devops/{ => code}/Gemfile (96%)
 rename challenge-devops/{ => code}/Gemfile.lock (100%)
 rename challenge-devops/{ => code}/README.md (100%)
 rename challenge-devops/{ => code}/Rakefile (100%)
 rename challenge-devops/{ => code}/app/assets/config/manifest.js (100%)
 rename challenge-devops/{ => code}/app/assets/images/.keep (100%)
 rename challenge-devops/{ => code}/app/assets/javascripts/application.js (100%)
 rename challenge-devops/{ => code}/app/assets/javascripts/cable.js (100%)
 rename challenge-devops/{ => code}/app/assets/javascripts/channels/.keep (100%)
 rename challenge-devops/{ => code}/app/assets/stylesheets/1st_load_framework.css.scss (100%)
 rename challenge-devops/{ => code}/app/assets/stylesheets/application.css.scss (100%)
 rename challenge-devops/{ => code}/app/channels/application_cable/channel.rb (100%)
 rename challenge-devops/{ => code}/app/channels/application_cable/connection.rb (100%)
 rename challenge-devops/{ => code}/app/controllers/application_controller.rb (100%)
 rename challenge-devops/{ => code}/app/controllers/concerns/.keep (100%)
 rename challenge-devops/{ => code}/app/controllers/products_controller.rb (100%)
 rename challenge-devops/{ => code}/app/controllers/thank_you_controller.rb (100%)
 rename challenge-devops/{ => code}/app/controllers/users_controller.rb (100%)
 rename challenge-devops/{ => code}/app/controllers/visitors_controller.rb (100%)
 rename challenge-devops/{ => code}/app/helpers/application_helper.rb (100%)
 rename challenge-devops/{ => code}/app/jobs/application_job.rb (100%)
 rename challenge-devops/{ => code}/app/mailers/application_mailer.rb (100%)
 rename challenge-devops/{ => code}/app/models/application_record.rb (100%)
 rename challenge-devops/{ => code}/app/models/concerns/.keep (100%)
 rename challenge-devops/{ => code}/app/models/user.rb (100%)
 rename challenge-devops/{ => code}/app/services/create_admin_service.rb (100%)
 rename challenge-devops/{ => code}/app/views/devise/passwords/edit.html.erb (100%)
 rename challenge-devops/{ => code}/app/views/devise/passwords/new.html.erb (100%)
 rename challenge-devops/{ => code}/app/views/devise/registrations/edit.html.erb (100%)
 rename challenge-devops/{ => code}/app/views/devise/registrations/new.html.erb (100%)
 rename challenge-devops/{ => code}/app/views/devise/sessions/new.html.erb (100%)
 rename challenge-devops/{ => code}/app/views/layouts/_messages.html.erb (100%)
 rename challenge-devops/{ => code}/app/views/layouts/_nav_links_for_auth.html.erb (100%)
 rename challenge-devops/{ => code}/app/views/layouts/_navigation.html.erb (100%)
 rename challenge-devops/{ => code}/app/views/layouts/_navigation_links.html.erb (100%)
 rename challenge-devops/{ => code}/app/views/layouts/application.html.erb (100%)
 rename challenge-devops/{ => code}/app/views/layouts/mailer.html.erb (100%)
 rename challenge-devops/{ => code}/app/views/layouts/mailer.text.erb (100%)
 rename challenge-devops/{ => code}/app/views/pages/about.html.erb (100%)
 rename challenge-devops/{ => code}/app/views/products/product.pdf (100%)
 rename challenge-devops/{ => code}/app/views/thank_you/index.html.erb (100%)
 rename challenge-devops/{ => code}/app/views/users/_user.html.erb (100%)
 rename challenge-devops/{ => code}/app/views/users/index.html.erb (100%)
 rename challenge-devops/{ => code}/app/views/users/show.html.erb (100%)
 rename challenge-devops/{ => code}/app/views/visitors/index.html.erb (100%)
 rename challenge-devops/{ => code}/bin/bundle (100%)
 rename challenge-devops/{ => code}/bin/rails (100%)
 rename challenge-devops/{ => code}/bin/rake (100%)
 rename challenge-devops/{ => code}/bin/setup (100%)
 rename challenge-devops/{ => code}/bin/update (100%)
 rename challenge-devops/{ => code}/bin/yarn (100%)
 rename challenge-devops/{ => code}/config.ru (100%)
 rename challenge-devops/{ => code}/config/application.rb (100%)
 rename challenge-devops/{ => code}/config/boot.rb (100%)
 rename challenge-devops/{ => code}/config/cable.yml (100%)
 rename challenge-devops/{ => code}/config/credentials.yml.enc (100%)
 rename challenge-devops/{ => code}/config/database.yml (69%)
 rename challenge-devops/{ => code}/config/environment.rb (100%)
 rename challenge-devops/{ => code}/config/environments/development.rb (100%)
 rename challenge-devops/{ => code}/config/environments/production.rb (100%)
 rename challenge-devops/{ => code}/config/environments/test.rb (100%)
 rename challenge-devops/{ => code}/config/initializers/application_controller_renderer.rb (100%)
 rename challenge-devops/{ => code}/config/initializers/assets.rb (100%)
 rename challenge-devops/{ => code}/config/initializers/backtrace_silencers.rb (100%)
 rename challenge-devops/{ => code}/config/initializers/content_security_policy.rb (100%)
 rename challenge-devops/{ => code}/config/initializers/cookies_serializer.rb (100%)
 rename challenge-devops/{ => code}/config/initializers/devise.rb (100%)
 rename challenge-devops/{ => code}/config/initializers/filter_parameter_logging.rb (100%)
 rename challenge-devops/{ => code}/config/initializers/inflections.rb (100%)
 rename challenge-devops/{ => code}/config/initializers/mime_types.rb (100%)
 rename challenge-devops/{ => code}/config/initializers/wrap_parameters.rb (100%)
 rename challenge-devops/{ => code}/config/locales/devise.en.yml (100%)
 rename challenge-devops/{ => code}/config/locales/en.yml (100%)
 rename challenge-devops/{ => code}/config/master.key (100%)
 rename challenge-devops/{ => code}/config/puma.rb (100%)
 rename challenge-devops/{ => code}/config/routes.rb (100%)
 rename challenge-devops/{ => code}/config/secrets.yml (100%)
 rename challenge-devops/{ => code}/config/storage.yml (100%)
 rename challenge-devops/{ => code}/db/migrate/20180514085158_devise_create_users.rb (100%)
 rename challenge-devops/{ => code}/db/migrate/20180514085200_add_name_to_users.rb (100%)
 rename challenge-devops/{ => code}/db/migrate/20180514085203_add_role_to_users.rb (100%)
 rename challenge-devops/{ => code}/db/schema.rb (100%)
 rename challenge-devops/{ => code}/db/seeds.rb (100%)
 rename challenge-devops/{ => code}/lib/assets/.keep (100%)
 rename challenge-devops/{ => code}/lib/tasks/.keep (100%)
 rename challenge-devops/{ => code}/package.json (100%)
 rename challenge-devops/{ => code}/public/404.html (100%)
 rename challenge-devops/{ => code}/public/422.html (100%)
 rename challenge-devops/{ => code}/public/500.html (100%)
 rename challenge-devops/{ => code}/public/apple-touch-icon-precomposed.png (100%)
 rename challenge-devops/{ => code}/public/apple-touch-icon.png (100%)
 rename challenge-devops/{ => code}/public/favicon.ico (100%)
 rename challenge-devops/{ => code}/public/humans.txt (100%)
 rename challenge-devops/{ => code}/public/robots.txt (100%)
 rename challenge-devops/{ => code}/spec/controllers/products_controller_spec.rb (100%)
 rename challenge-devops/{ => code}/spec/features/users/product_acquisition_spec.rb (100%)
 rename challenge-devops/{ => code}/storage/.keep (100%)
 rename challenge-devops/{ => code}/test/application_system_test_case.rb (100%)
 rename challenge-devops/{ => code}/test/controllers/.keep (100%)
 rename challenge-devops/{ => code}/test/fixtures/.keep (100%)
 rename challenge-devops/{ => code}/test/fixtures/files/.keep (100%)
 rename challenge-devops/{ => code}/test/fixtures/users.yml (100%)
 rename challenge-devops/{ => code}/test/helpers/.keep (100%)
 rename challenge-devops/{ => code}/test/integration/.keep (100%)
 rename challenge-devops/{ => code}/test/mailers/.keep (100%)
 rename challenge-devops/{ => code}/test/models/.keep (100%)
 rename challenge-devops/{ => code}/test/models/user_test.rb (100%)
 rename challenge-devops/{ => code}/test/system/.keep (100%)
 rename challenge-devops/{ => code}/test/test_helper.rb (100%)
 rename challenge-devops/{ => code}/vendor/.keep (100%)
 create mode 100755 challenge-devops/deploy.sh
 create mode 100644 challenge-devops/docker-compose.yml
 create mode 100644 challenge-devops/helm/opn-devops/.helmignore
 create mode 100644 challenge-devops/helm/opn-devops/Chart.yaml
 create mode 100644 challenge-devops/helm/opn-devops/templates/NOTES.txt
 create mode 100644 challenge-devops/helm/opn-devops/templates/_helpers.tpl
 create mode 100644 challenge-devops/helm/opn-devops/templates/deployment.yaml
 create mode 100644 challenge-devops/helm/opn-devops/templates/hpa.yaml
 create mode 100644 challenge-devops/helm/opn-devops/templates/ingress.yaml
 create mode 100644 challenge-devops/helm/opn-devops/templates/service.yaml
 create mode 100644 challenge-devops/helm/opn-devops/templates/serviceaccount.yaml
 create mode 100644 challenge-devops/helm/opn-devops/templates/tests/test-connection.yaml
 create mode 100644 challenge-devops/helm/opn-devops/values-dev.yaml
 create mode 100644 challenge-devops/helm/opn-devops/values-prod.yaml
 create mode 100755 challenge-devops/helm/patroni/Chart.yaml
 create mode 100755 challenge-devops/helm/patroni/README.md
 create mode 100755 challenge-devops/helm/patroni/charts/consul/Chart.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/consul/README.md
 create mode 100755 challenge-devops/helm/patroni/charts/consul/templates/NOTES.txt
 create mode 100755 challenge-devops/helm/patroni/charts/consul/templates/_helpers.tpl
 create mode 100755 challenge-devops/helm/patroni/charts/consul/templates/basic-acls.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/consul/templates/consul-ingress.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/consul/templates/consul-service.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/consul/templates/consul-statefulset.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/consul/templates/consul-test-clusterrole.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/consul/templates/consul-test-clusterrolebinding.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/consul/templates/consul-test-serviceaccount.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/consul/templates/consul-test.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/consul/templates/gossip-secret.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/consul/templates/pod-dist-budget.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/consul/templates/test-config.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/consul/templates/ui-service.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/consul/values.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/etcd/.helmignore
 create mode 100755 challenge-devops/helm/patroni/charts/etcd/Chart.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/etcd/README.md
 create mode 100755 challenge-devops/helm/patroni/charts/etcd/templates/NOTES.txt
 create mode 100755 challenge-devops/helm/patroni/charts/etcd/templates/_helpers.tpl
 create mode 100755 challenge-devops/helm/patroni/charts/etcd/templates/service.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/etcd/templates/statefulset.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/etcd/values.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/zookeeper/.helmignore
 create mode 100755 challenge-devops/helm/patroni/charts/zookeeper/Chart.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/zookeeper/OWNERS
 create mode 100755 challenge-devops/helm/patroni/charts/zookeeper/README.md
 create mode 100755 challenge-devops/helm/patroni/charts/zookeeper/templates/NOTES.txt
 create mode 100755 challenge-devops/helm/patroni/charts/zookeeper/templates/_helpers.tpl
 create mode 100755 challenge-devops/helm/patroni/charts/zookeeper/templates/config-jmx-exporter.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/zookeeper/templates/job-chroots.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/zookeeper/templates/poddisruptionbudget.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/zookeeper/templates/service-headless.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/zookeeper/templates/service.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/zookeeper/templates/statefulset.yaml
 create mode 100755 challenge-devops/helm/patroni/charts/zookeeper/values.yaml
 create mode 100755 challenge-devops/helm/patroni/requirements.lock
 create mode 100755 challenge-devops/helm/patroni/requirements.yaml
 create mode 100755 challenge-devops/helm/patroni/templates/NOTES.txt
 create mode 100755 challenge-devops/helm/patroni/templates/_helpers.tpl
 create mode 100755 challenge-devops/helm/patroni/templates/ep-patroni.yaml
 create mode 100755 challenge-devops/helm/patroni/templates/role-patroni.yaml
 create mode 100755 challenge-devops/helm/patroni/templates/rolebinding-patroni.yaml
 create mode 100755 challenge-devops/helm/patroni/templates/sec-patroni.yaml
 create mode 100755 challenge-devops/helm/patroni/templates/serviceaccount-patroni.yaml
 create mode 100755 challenge-devops/helm/patroni/templates/statefulset-patroni.yaml
 create mode 100755 challenge-devops/helm/patroni/templates/svc-patroni.yaml
 create mode 100755 challenge-devops/helm/patroni/values-dev.yaml
 create mode 100755 challenge-devops/helm/patroni/values-prod.yaml

diff --git a/challenge-devops/Dockerfile b/challenge-devops/Dockerfile
index 0d55cc0..970db7a 100644
--- a/challenge-devops/Dockerfile
+++ b/challenge-devops/Dockerfile
@@ -1,10 +1,10 @@
 FROM ruby:2.5.1-slim
-RUN apt-get update -qq && apt-get install -y build-essential libpq-dev nodejs libsqlite3-dev \
+RUN apt-get update -qq && apt-get install -y build-essential libpq-dev nodejs libpq-dev \
     && apt-get clean autoclean \
     && apt-get autoremove -y
 RUN mkdir /app
 WORKDIR /app
-COPY Gemfile Gemfile.lock /app/
+COPY ./code/Gemfile ./code/Gemfile.lock /app/
 RUN bundle install 
-COPY . /app
-CMD bundle exec rails s -p 3000 -b '0.0.0.0'
+COPY ./code/ /app
+CMD bundle exec rails s -p 3000 -b '0.0.0.0'
\ No newline at end of file
diff --git a/challenge-devops/code/.gitignore b/challenge-devops/code/.gitignore
new file mode 100644
index 0000000..485b8e8
--- /dev/null
+++ b/challenge-devops/code/.gitignore
@@ -0,0 +1,84 @@
+#----------------------------------------------------------------------------
+# Ignore these files when committing to a git repository.
+#
+# See http://help.github.com/ignore-files/ for more about ignoring files.
+#
+# The original version of this file is found here:
+# https://github.com/RailsApps/rails-composer/blob/master/files/gitignore.txt
+#
+# Corrections? Improvements? Create a GitHub issue:
+# http://github.com/RailsApps/rails-composer/issues
+#----------------------------------------------------------------------------
+
+# bundler state
+/.bundle
+/vendor/bundle/
+/vendor/ruby/
+
+# minimal Rails specific artifacts
+db/*.sqlite3
+/db/*.sqlite3-journal
+/log/*
+/tmp/*
+
+# add /config/database.yml if it contains passwords
+# /config/database.yml
+
+# various artifacts
+**.war
+*.rbc
+*.sassc
+.redcar/
+.sass-cache
+/config/config.yml
+/coverage.data
+/coverage/
+/db/*.javadb/
+/db/*.sqlite3
+/doc/api/
+/doc/app/
+/doc/features.html
+/doc/specs.html
+/public/cache
+/public/stylesheets/compiled
+/public/system/*
+/spec/tmp/*
+/cache
+/capybara*
+/capybara-*.html
+/gems
+/specifications
+rerun.txt
+pickle-email-*.html
+.zeus.sock
+
+# If you find yourself ignoring temporary files generated by your text editor
+# or operating system, you probably want to add a global ignore instead:
+#   git config --global core.excludesfile ~/.gitignore_global
+#
+# Here are some files you may want to ignore globally:
+
+# scm revert files
+**.orig
+
+# Mac finder artifacts
+.DS_Store
+
+# Netbeans project directory
+/nbproject/
+
+# RubyMine project files
+.idea
+
+# Textmate project files
+/*.tmproj
+
+# vim artifacts
+**.swp
+
+# Environment files that may contain sensitive data
+.env
+.powenv
+
+# tilde files are usually backup files from a text editor
+*~
diff --git a/challenge-devops/.ruby-gemset b/challenge-devops/code/.ruby-gemset
similarity index 100%
rename from challenge-devops/.ruby-gemset
rename to challenge-devops/code/.ruby-gemset
diff --git a/challenge-devops/.ruby-version b/challenge-devops/code/.ruby-version
similarity index 100%
rename from challenge-devops/.ruby-version
rename to challenge-devops/code/.ruby-version
diff --git a/challenge-devops/.tool-versions b/challenge-devops/code/.tool-versions
similarity index 100%
rename from challenge-devops/.tool-versions
rename to challenge-devops/code/.tool-versions
diff --git a/challenge-devops/Gemfile b/challenge-devops/code/Gemfile
similarity index 96%
rename from challenge-devops/Gemfile
rename to challenge-devops/code/Gemfile
index 371f020..e45251b 100644
--- a/challenge-devops/Gemfile
+++ b/challenge-devops/code/Gemfile
@@ -1,9 +1,9 @@
 source 'https://rubygems.org'
 git_source(:github) { |repo| "https://github.com/#{repo}.git" }
-ruby '2.6.3'
+ruby '2.5.1'
 gem 'rails'
 gem 'activerecord', '>= 6.0.3.5'
-gem 'sqlite3'
+gem 'pg'
 gem 'puma'
 gem 'sass-rails'
 gem 'uglifier'
diff --git a/challenge-devops/Gemfile.lock b/challenge-devops/code/Gemfile.lock
similarity index 100%
rename from challenge-devops/Gemfile.lock
rename to challenge-devops/code/Gemfile.lock
diff --git a/challenge-devops/README.md b/challenge-devops/code/README.md
similarity index 100%
rename from challenge-devops/README.md
rename to challenge-devops/code/README.md
diff --git a/challenge-devops/Rakefile b/challenge-devops/code/Rakefile
similarity index 100%
rename from challenge-devops/Rakefile
rename to challenge-devops/code/Rakefile
diff --git a/challenge-devops/app/assets/config/manifest.js b/challenge-devops/code/app/assets/config/manifest.js
similarity index 100%
rename from challenge-devops/app/assets/config/manifest.js
rename to challenge-devops/code/app/assets/config/manifest.js
diff --git a/challenge-devops/app/assets/images/.keep b/challenge-devops/code/app/assets/images/.keep
similarity index 100%
rename from challenge-devops/app/assets/images/.keep
rename to challenge-devops/code/app/assets/images/.keep
diff --git a/challenge-devops/app/assets/javascripts/application.js b/challenge-devops/code/app/assets/javascripts/application.js
similarity index 100%
rename from challenge-devops/app/assets/javascripts/application.js
rename to challenge-devops/code/app/assets/javascripts/application.js
diff --git a/challenge-devops/app/assets/javascripts/cable.js b/challenge-devops/code/app/assets/javascripts/cable.js
similarity index 100%
rename from challenge-devops/app/assets/javascripts/cable.js
rename to challenge-devops/code/app/assets/javascripts/cable.js
diff --git a/challenge-devops/app/assets/javascripts/channels/.keep b/challenge-devops/code/app/assets/javascripts/channels/.keep
similarity index 100%
rename from challenge-devops/app/assets/javascripts/channels/.keep
rename to challenge-devops/code/app/assets/javascripts/channels/.keep
diff --git a/challenge-devops/app/assets/stylesheets/1st_load_framework.css.scss b/challenge-devops/code/app/assets/stylesheets/1st_load_framework.css.scss
similarity index 100%
rename from challenge-devops/app/assets/stylesheets/1st_load_framework.css.scss
rename to challenge-devops/code/app/assets/stylesheets/1st_load_framework.css.scss
diff --git a/challenge-devops/app/assets/stylesheets/application.css.scss b/challenge-devops/code/app/assets/stylesheets/application.css.scss
similarity index 100%
rename from challenge-devops/app/assets/stylesheets/application.css.scss
rename to challenge-devops/code/app/assets/stylesheets/application.css.scss
diff --git a/challenge-devops/app/channels/application_cable/channel.rb b/challenge-devops/code/app/channels/application_cable/channel.rb
similarity index 100%
rename from challenge-devops/app/channels/application_cable/channel.rb
rename to challenge-devops/code/app/channels/application_cable/channel.rb
diff --git a/challenge-devops/app/channels/application_cable/connection.rb b/challenge-devops/code/app/channels/application_cable/connection.rb
similarity index 100%
rename from challenge-devops/app/channels/application_cable/connection.rb
rename to challenge-devops/code/app/channels/application_cable/connection.rb
diff --git a/challenge-devops/app/controllers/application_controller.rb b/challenge-devops/code/app/controllers/application_controller.rb
similarity index 100%
rename from challenge-devops/app/controllers/application_controller.rb
rename to challenge-devops/code/app/controllers/application_controller.rb
diff --git a/challenge-devops/app/controllers/concerns/.keep b/challenge-devops/code/app/controllers/concerns/.keep
similarity index 100%
rename from challenge-devops/app/controllers/concerns/.keep
rename to challenge-devops/code/app/controllers/concerns/.keep
diff --git a/challenge-devops/app/controllers/products_controller.rb b/challenge-devops/code/app/controllers/products_controller.rb
similarity index 100%
rename from challenge-devops/app/controllers/products_controller.rb
rename to challenge-devops/code/app/controllers/products_controller.rb
diff --git a/challenge-devops/app/controllers/thank_you_controller.rb b/challenge-devops/code/app/controllers/thank_you_controller.rb
similarity index 100%
rename from challenge-devops/app/controllers/thank_you_controller.rb
rename to challenge-devops/code/app/controllers/thank_you_controller.rb
diff --git a/challenge-devops/app/controllers/users_controller.rb b/challenge-devops/code/app/controllers/users_controller.rb
similarity index 100%
rename from challenge-devops/app/controllers/users_controller.rb
rename to challenge-devops/code/app/controllers/users_controller.rb
diff --git a/challenge-devops/app/controllers/visitors_controller.rb b/challenge-devops/code/app/controllers/visitors_controller.rb
similarity index 100%
rename from challenge-devops/app/controllers/visitors_controller.rb
rename to challenge-devops/code/app/controllers/visitors_controller.rb
diff --git a/challenge-devops/app/helpers/application_helper.rb b/challenge-devops/code/app/helpers/application_helper.rb
similarity index 100%
rename from challenge-devops/app/helpers/application_helper.rb
rename to challenge-devops/code/app/helpers/application_helper.rb
diff --git a/challenge-devops/app/jobs/application_job.rb b/challenge-devops/code/app/jobs/application_job.rb
similarity index 100%
rename from challenge-devops/app/jobs/application_job.rb
rename to challenge-devops/code/app/jobs/application_job.rb
diff --git a/challenge-devops/app/mailers/application_mailer.rb b/challenge-devops/code/app/mailers/application_mailer.rb
similarity index 100%
rename from challenge-devops/app/mailers/application_mailer.rb
rename to challenge-devops/code/app/mailers/application_mailer.rb
diff --git a/challenge-devops/app/models/application_record.rb b/challenge-devops/code/app/models/application_record.rb
similarity index 100%
rename from challenge-devops/app/models/application_record.rb
rename to challenge-devops/code/app/models/application_record.rb
diff --git a/challenge-devops/app/models/concerns/.keep b/challenge-devops/code/app/models/concerns/.keep
similarity index 100%
rename from challenge-devops/app/models/concerns/.keep
rename to challenge-devops/code/app/models/concerns/.keep
diff --git a/challenge-devops/app/models/user.rb b/challenge-devops/code/app/models/user.rb
similarity index 100%
rename from challenge-devops/app/models/user.rb
rename to challenge-devops/code/app/models/user.rb
diff --git a/challenge-devops/app/services/create_admin_service.rb b/challenge-devops/code/app/services/create_admin_service.rb
similarity index 100%
rename from challenge-devops/app/services/create_admin_service.rb
rename to challenge-devops/code/app/services/create_admin_service.rb
diff --git a/challenge-devops/app/views/devise/passwords/edit.html.erb b/challenge-devops/code/app/views/devise/passwords/edit.html.erb
similarity index 100%
rename from challenge-devops/app/views/devise/passwords/edit.html.erb
rename to challenge-devops/code/app/views/devise/passwords/edit.html.erb
diff --git a/challenge-devops/app/views/devise/passwords/new.html.erb b/challenge-devops/code/app/views/devise/passwords/new.html.erb
similarity index 100%
rename from challenge-devops/app/views/devise/passwords/new.html.erb
rename to challenge-devops/code/app/views/devise/passwords/new.html.erb
diff --git a/challenge-devops/app/views/devise/registrations/edit.html.erb b/challenge-devops/code/app/views/devise/registrations/edit.html.erb
similarity index 100%
rename from challenge-devops/app/views/devise/registrations/edit.html.erb
rename to challenge-devops/code/app/views/devise/registrations/edit.html.erb
diff --git a/challenge-devops/app/views/devise/registrations/new.html.erb b/challenge-devops/code/app/views/devise/registrations/new.html.erb
similarity index 100%
rename from challenge-devops/app/views/devise/registrations/new.html.erb
rename to challenge-devops/code/app/views/devise/registrations/new.html.erb
diff --git a/challenge-devops/app/views/devise/sessions/new.html.erb b/challenge-devops/code/app/views/devise/sessions/new.html.erb
similarity index 100%
rename from challenge-devops/app/views/devise/sessions/new.html.erb
rename to challenge-devops/code/app/views/devise/sessions/new.html.erb
diff --git a/challenge-devops/app/views/layouts/_messages.html.erb b/challenge-devops/code/app/views/layouts/_messages.html.erb
similarity index 100%
rename from challenge-devops/app/views/layouts/_messages.html.erb
rename to challenge-devops/code/app/views/layouts/_messages.html.erb
diff --git a/challenge-devops/app/views/layouts/_nav_links_for_auth.html.erb b/challenge-devops/code/app/views/layouts/_nav_links_for_auth.html.erb
similarity index 100%
rename from challenge-devops/app/views/layouts/_nav_links_for_auth.html.erb
rename to challenge-devops/code/app/views/layouts/_nav_links_for_auth.html.erb
diff --git a/challenge-devops/app/views/layouts/_navigation.html.erb b/challenge-devops/code/app/views/layouts/_navigation.html.erb
similarity index 100%
rename from challenge-devops/app/views/layouts/_navigation.html.erb
rename to challenge-devops/code/app/views/layouts/_navigation.html.erb
diff --git a/challenge-devops/app/views/layouts/_navigation_links.html.erb b/challenge-devops/code/app/views/layouts/_navigation_links.html.erb
similarity index 100%
rename from challenge-devops/app/views/layouts/_navigation_links.html.erb
rename to challenge-devops/code/app/views/layouts/_navigation_links.html.erb
diff --git a/challenge-devops/app/views/layouts/application.html.erb b/challenge-devops/code/app/views/layouts/application.html.erb
similarity index 100%
rename from challenge-devops/app/views/layouts/application.html.erb
rename to challenge-devops/code/app/views/layouts/application.html.erb
diff --git a/challenge-devops/app/views/layouts/mailer.html.erb b/challenge-devops/code/app/views/layouts/mailer.html.erb
similarity index 100%
rename from challenge-devops/app/views/layouts/mailer.html.erb
rename to challenge-devops/code/app/views/layouts/mailer.html.erb
diff --git a/challenge-devops/app/views/layouts/mailer.text.erb b/challenge-devops/code/app/views/layouts/mailer.text.erb
similarity index 100%
rename from challenge-devops/app/views/layouts/mailer.text.erb
rename to challenge-devops/code/app/views/layouts/mailer.text.erb
diff --git a/challenge-devops/app/views/pages/about.html.erb b/challenge-devops/code/app/views/pages/about.html.erb
similarity index 100%
rename from challenge-devops/app/views/pages/about.html.erb
rename to challenge-devops/code/app/views/pages/about.html.erb
diff --git a/challenge-devops/app/views/products/product.pdf b/challenge-devops/code/app/views/products/product.pdf
similarity index 100%
rename from challenge-devops/app/views/products/product.pdf
rename to challenge-devops/code/app/views/products/product.pdf
diff --git a/challenge-devops/app/views/thank_you/index.html.erb b/challenge-devops/code/app/views/thank_you/index.html.erb
similarity index 100%
rename from challenge-devops/app/views/thank_you/index.html.erb
rename to challenge-devops/code/app/views/thank_you/index.html.erb
diff --git a/challenge-devops/app/views/users/_user.html.erb b/challenge-devops/code/app/views/users/_user.html.erb
similarity index 100%
rename from challenge-devops/app/views/users/_user.html.erb
rename to challenge-devops/code/app/views/users/_user.html.erb
diff --git a/challenge-devops/app/views/users/index.html.erb b/challenge-devops/code/app/views/users/index.html.erb
similarity index 100%
rename from challenge-devops/app/views/users/index.html.erb
rename to challenge-devops/code/app/views/users/index.html.erb
diff --git a/challenge-devops/app/views/users/show.html.erb b/challenge-devops/code/app/views/users/show.html.erb
similarity index 100%
rename from challenge-devops/app/views/users/show.html.erb
rename to challenge-devops/code/app/views/users/show.html.erb
diff --git a/challenge-devops/app/views/visitors/index.html.erb b/challenge-devops/code/app/views/visitors/index.html.erb
similarity index 100%
rename from challenge-devops/app/views/visitors/index.html.erb
rename to challenge-devops/code/app/views/visitors/index.html.erb
diff --git a/challenge-devops/bin/bundle b/challenge-devops/code/bin/bundle
similarity index 100%
rename from challenge-devops/bin/bundle
rename to challenge-devops/code/bin/bundle
diff --git a/challenge-devops/bin/rails b/challenge-devops/code/bin/rails
similarity index 100%
rename from challenge-devops/bin/rails
rename to challenge-devops/code/bin/rails
diff --git a/challenge-devops/bin/rake b/challenge-devops/code/bin/rake
similarity index 100%
rename from challenge-devops/bin/rake
rename to challenge-devops/code/bin/rake
diff --git a/challenge-devops/bin/setup b/challenge-devops/code/bin/setup
similarity index 100%
rename from challenge-devops/bin/setup
rename to challenge-devops/code/bin/setup
diff --git a/challenge-devops/bin/update b/challenge-devops/code/bin/update
similarity index 100%
rename from challenge-devops/bin/update
rename to challenge-devops/code/bin/update
diff --git a/challenge-devops/bin/yarn b/challenge-devops/code/bin/yarn
similarity index 100%
rename from challenge-devops/bin/yarn
rename to challenge-devops/code/bin/yarn
diff --git a/challenge-devops/config.ru b/challenge-devops/code/config.ru
similarity index 100%
rename from challenge-devops/config.ru
rename to challenge-devops/code/config.ru
diff --git a/challenge-devops/config/application.rb b/challenge-devops/code/config/application.rb
similarity index 100%
rename from challenge-devops/config/application.rb
rename to challenge-devops/code/config/application.rb
diff --git a/challenge-devops/config/boot.rb b/challenge-devops/code/config/boot.rb
similarity index 100%
rename from challenge-devops/config/boot.rb
rename to challenge-devops/code/config/boot.rb
diff --git a/challenge-devops/config/cable.yml b/challenge-devops/code/config/cable.yml
similarity index 100%
rename from challenge-devops/config/cable.yml
rename to challenge-devops/code/config/cable.yml
diff --git a/challenge-devops/config/credentials.yml.enc b/challenge-devops/code/config/credentials.yml.enc
similarity index 100%
rename from challenge-devops/config/credentials.yml.enc
rename to challenge-devops/code/config/credentials.yml.enc
diff --git a/challenge-devops/config/database.yml b/challenge-devops/code/config/database.yml
similarity index 69%
rename from challenge-devops/config/database.yml
rename to challenge-devops/code/config/database.yml
index 0d02f24..232bbbb 100644
--- a/challenge-devops/config/database.yml
+++ b/challenge-devops/code/config/database.yml
@@ -5,21 +5,24 @@
 #   gem 'sqlite3'
 #
 default: &default
-  adapter: sqlite3
-  pool: <%= ENV.fetch("RAILS_MAX_THREADS") { 5 } %>
-  timeout: 5000
+  adapter: postgresql
+  encoding: unicode
+  host: patroni
+  username: admin
+  password: cola
+  pool: 5
 
 development:
   <<: *default
-  database: db/development.sqlite3
+  database: development
 
 # Warning: The database defined as "test" will be erased and
 # re-generated from your development database when you run "rake".
 # Do not set this db to the same as development or production.
 test:
   <<: *default
-  database: db/test.sqlite3
+  database: test
 
 production:
   <<: *default
-  database: db/production.sqlite3
+  database: production
diff --git a/challenge-devops/config/environment.rb b/challenge-devops/code/config/environment.rb
similarity index 100%
rename from challenge-devops/config/environment.rb
rename to challenge-devops/code/config/environment.rb
diff --git a/challenge-devops/config/environments/development.rb b/challenge-devops/code/config/environments/development.rb
similarity index 100%
rename from challenge-devops/config/environments/development.rb
rename to challenge-devops/code/config/environments/development.rb
diff --git a/challenge-devops/config/environments/production.rb b/challenge-devops/code/config/environments/production.rb
similarity index 100%
rename from challenge-devops/config/environments/production.rb
rename to challenge-devops/code/config/environments/production.rb
diff --git a/challenge-devops/config/environments/test.rb b/challenge-devops/code/config/environments/test.rb
similarity index 100%
rename from challenge-devops/config/environments/test.rb
rename to challenge-devops/code/config/environments/test.rb
diff --git a/challenge-devops/config/initializers/application_controller_renderer.rb b/challenge-devops/code/config/initializers/application_controller_renderer.rb
similarity index 100%
rename from challenge-devops/config/initializers/application_controller_renderer.rb
rename to challenge-devops/code/config/initializers/application_controller_renderer.rb
diff --git a/challenge-devops/config/initializers/assets.rb b/challenge-devops/code/config/initializers/assets.rb
similarity index 100%
rename from challenge-devops/config/initializers/assets.rb
rename to challenge-devops/code/config/initializers/assets.rb
diff --git a/challenge-devops/config/initializers/backtrace_silencers.rb b/challenge-devops/code/config/initializers/backtrace_silencers.rb
similarity index 100%
rename from challenge-devops/config/initializers/backtrace_silencers.rb
rename to challenge-devops/code/config/initializers/backtrace_silencers.rb
diff --git a/challenge-devops/config/initializers/content_security_policy.rb b/challenge-devops/code/config/initializers/content_security_policy.rb
similarity index 100%
rename from challenge-devops/config/initializers/content_security_policy.rb
rename to challenge-devops/code/config/initializers/content_security_policy.rb
diff --git a/challenge-devops/config/initializers/cookies_serializer.rb b/challenge-devops/code/config/initializers/cookies_serializer.rb
similarity index 100%
rename from challenge-devops/config/initializers/cookies_serializer.rb
rename to challenge-devops/code/config/initializers/cookies_serializer.rb
diff --git a/challenge-devops/config/initializers/devise.rb b/challenge-devops/code/config/initializers/devise.rb
similarity index 100%
rename from challenge-devops/config/initializers/devise.rb
rename to challenge-devops/code/config/initializers/devise.rb
diff --git a/challenge-devops/config/initializers/filter_parameter_logging.rb b/challenge-devops/code/config/initializers/filter_parameter_logging.rb
similarity index 100%
rename from challenge-devops/config/initializers/filter_parameter_logging.rb
rename to challenge-devops/code/config/initializers/filter_parameter_logging.rb
diff --git a/challenge-devops/config/initializers/inflections.rb b/challenge-devops/code/config/initializers/inflections.rb
similarity index 100%
rename from challenge-devops/config/initializers/inflections.rb
rename to challenge-devops/code/config/initializers/inflections.rb
diff --git a/challenge-devops/config/initializers/mime_types.rb b/challenge-devops/code/config/initializers/mime_types.rb
similarity index 100%
rename from challenge-devops/config/initializers/mime_types.rb
rename to challenge-devops/code/config/initializers/mime_types.rb
diff --git a/challenge-devops/config/initializers/wrap_parameters.rb b/challenge-devops/code/config/initializers/wrap_parameters.rb
similarity index 100%
rename from challenge-devops/config/initializers/wrap_parameters.rb
rename to challenge-devops/code/config/initializers/wrap_parameters.rb
diff --git a/challenge-devops/config/locales/devise.en.yml b/challenge-devops/code/config/locales/devise.en.yml
similarity index 100%
rename from challenge-devops/config/locales/devise.en.yml
rename to challenge-devops/code/config/locales/devise.en.yml
diff --git a/challenge-devops/config/locales/en.yml b/challenge-devops/code/config/locales/en.yml
similarity index 100%
rename from challenge-devops/config/locales/en.yml
rename to challenge-devops/code/config/locales/en.yml
diff --git a/challenge-devops/config/master.key b/challenge-devops/code/config/master.key
similarity index 100%
rename from challenge-devops/config/master.key
rename to challenge-devops/code/config/master.key
diff --git a/challenge-devops/config/puma.rb b/challenge-devops/code/config/puma.rb
similarity index 100%
rename from challenge-devops/config/puma.rb
rename to challenge-devops/code/config/puma.rb
diff --git a/challenge-devops/config/routes.rb b/challenge-devops/code/config/routes.rb
similarity index 100%
rename from challenge-devops/config/routes.rb
rename to challenge-devops/code/config/routes.rb
diff --git a/challenge-devops/config/secrets.yml b/challenge-devops/code/config/secrets.yml
similarity index 100%
rename from challenge-devops/config/secrets.yml
rename to challenge-devops/code/config/secrets.yml
diff --git a/challenge-devops/config/storage.yml b/challenge-devops/code/config/storage.yml
similarity index 100%
rename from challenge-devops/config/storage.yml
rename to challenge-devops/code/config/storage.yml
diff --git a/challenge-devops/db/migrate/20180514085158_devise_create_users.rb b/challenge-devops/code/db/migrate/20180514085158_devise_create_users.rb
similarity index 100%
rename from challenge-devops/db/migrate/20180514085158_devise_create_users.rb
rename to challenge-devops/code/db/migrate/20180514085158_devise_create_users.rb
diff --git a/challenge-devops/db/migrate/20180514085200_add_name_to_users.rb b/challenge-devops/code/db/migrate/20180514085200_add_name_to_users.rb
similarity index 100%
rename from challenge-devops/db/migrate/20180514085200_add_name_to_users.rb
rename to challenge-devops/code/db/migrate/20180514085200_add_name_to_users.rb
diff --git a/challenge-devops/db/migrate/20180514085203_add_role_to_users.rb b/challenge-devops/code/db/migrate/20180514085203_add_role_to_users.rb
similarity index 100%
rename from challenge-devops/db/migrate/20180514085203_add_role_to_users.rb
rename to challenge-devops/code/db/migrate/20180514085203_add_role_to_users.rb
diff --git a/challenge-devops/db/schema.rb b/challenge-devops/code/db/schema.rb
similarity index 100%
rename from challenge-devops/db/schema.rb
rename to challenge-devops/code/db/schema.rb
diff --git a/challenge-devops/db/seeds.rb b/challenge-devops/code/db/seeds.rb
similarity index 100%
rename from challenge-devops/db/seeds.rb
rename to challenge-devops/code/db/seeds.rb
diff --git a/challenge-devops/lib/assets/.keep b/challenge-devops/code/lib/assets/.keep
similarity index 100%
rename from challenge-devops/lib/assets/.keep
rename to challenge-devops/code/lib/assets/.keep
diff --git a/challenge-devops/lib/tasks/.keep b/challenge-devops/code/lib/tasks/.keep
similarity index 100%
rename from challenge-devops/lib/tasks/.keep
rename to challenge-devops/code/lib/tasks/.keep
diff --git a/challenge-devops/package.json b/challenge-devops/code/package.json
similarity index 100%
rename from challenge-devops/package.json
rename to challenge-devops/code/package.json
diff --git a/challenge-devops/public/404.html b/challenge-devops/code/public/404.html
similarity index 100%
rename from challenge-devops/public/404.html
rename to challenge-devops/code/public/404.html
diff --git a/challenge-devops/public/422.html b/challenge-devops/code/public/422.html
similarity index 100%
rename from challenge-devops/public/422.html
rename to challenge-devops/code/public/422.html
diff --git a/challenge-devops/public/500.html b/challenge-devops/code/public/500.html
similarity index 100%
rename from challenge-devops/public/500.html
rename to challenge-devops/code/public/500.html
diff --git a/challenge-devops/public/apple-touch-icon-precomposed.png b/challenge-devops/code/public/apple-touch-icon-precomposed.png
similarity index 100%
rename from challenge-devops/public/apple-touch-icon-precomposed.png
rename to challenge-devops/code/public/apple-touch-icon-precomposed.png
diff --git a/challenge-devops/public/apple-touch-icon.png b/challenge-devops/code/public/apple-touch-icon.png
similarity index 100%
rename from challenge-devops/public/apple-touch-icon.png
rename to challenge-devops/code/public/apple-touch-icon.png
diff --git a/challenge-devops/public/favicon.ico b/challenge-devops/code/public/favicon.ico
similarity index 100%
rename from challenge-devops/public/favicon.ico
rename to challenge-devops/code/public/favicon.ico
diff --git a/challenge-devops/public/humans.txt b/challenge-devops/code/public/humans.txt
similarity index 100%
rename from challenge-devops/public/humans.txt
rename to challenge-devops/code/public/humans.txt
diff --git a/challenge-devops/public/robots.txt b/challenge-devops/code/public/robots.txt
similarity index 100%
rename from challenge-devops/public/robots.txt
rename to challenge-devops/code/public/robots.txt
diff --git a/challenge-devops/spec/controllers/products_controller_spec.rb b/challenge-devops/code/spec/controllers/products_controller_spec.rb
similarity index 100%
rename from challenge-devops/spec/controllers/products_controller_spec.rb
rename to challenge-devops/code/spec/controllers/products_controller_spec.rb
diff --git a/challenge-devops/spec/features/users/product_acquisition_spec.rb b/challenge-devops/code/spec/features/users/product_acquisition_spec.rb
similarity index 100%
rename from challenge-devops/spec/features/users/product_acquisition_spec.rb
rename to challenge-devops/code/spec/features/users/product_acquisition_spec.rb
diff --git a/challenge-devops/storage/.keep b/challenge-devops/code/storage/.keep
similarity index 100%
rename from challenge-devops/storage/.keep
rename to challenge-devops/code/storage/.keep
diff --git a/challenge-devops/test/application_system_test_case.rb b/challenge-devops/code/test/application_system_test_case.rb
similarity index 100%
rename from challenge-devops/test/application_system_test_case.rb
rename to challenge-devops/code/test/application_system_test_case.rb
diff --git a/challenge-devops/test/controllers/.keep b/challenge-devops/code/test/controllers/.keep
similarity index 100%
rename from challenge-devops/test/controllers/.keep
rename to challenge-devops/code/test/controllers/.keep
diff --git a/challenge-devops/test/fixtures/.keep b/challenge-devops/code/test/fixtures/.keep
similarity index 100%
rename from challenge-devops/test/fixtures/.keep
rename to challenge-devops/code/test/fixtures/.keep
diff --git a/challenge-devops/test/fixtures/files/.keep b/challenge-devops/code/test/fixtures/files/.keep
similarity index 100%
rename from challenge-devops/test/fixtures/files/.keep
rename to challenge-devops/code/test/fixtures/files/.keep
diff --git a/challenge-devops/test/fixtures/users.yml b/challenge-devops/code/test/fixtures/users.yml
similarity index 100%
rename from challenge-devops/test/fixtures/users.yml
rename to challenge-devops/code/test/fixtures/users.yml
diff --git a/challenge-devops/test/helpers/.keep b/challenge-devops/code/test/helpers/.keep
similarity index 100%
rename from challenge-devops/test/helpers/.keep
rename to challenge-devops/code/test/helpers/.keep
diff --git a/challenge-devops/test/integration/.keep b/challenge-devops/code/test/integration/.keep
similarity index 100%
rename from challenge-devops/test/integration/.keep
rename to challenge-devops/code/test/integration/.keep
diff --git a/challenge-devops/test/mailers/.keep b/challenge-devops/code/test/mailers/.keep
similarity index 100%
rename from challenge-devops/test/mailers/.keep
rename to challenge-devops/code/test/mailers/.keep
diff --git a/challenge-devops/test/models/.keep b/challenge-devops/code/test/models/.keep
similarity index 100%
rename from challenge-devops/test/models/.keep
rename to challenge-devops/code/test/models/.keep
diff --git a/challenge-devops/test/models/user_test.rb b/challenge-devops/code/test/models/user_test.rb
similarity index 100%
rename from challenge-devops/test/models/user_test.rb
rename to challenge-devops/code/test/models/user_test.rb
diff --git a/challenge-devops/test/system/.keep b/challenge-devops/code/test/system/.keep
similarity index 100%
rename from challenge-devops/test/system/.keep
rename to challenge-devops/code/test/system/.keep
diff --git a/challenge-devops/test/test_helper.rb b/challenge-devops/code/test/test_helper.rb
similarity index 100%
rename from challenge-devops/test/test_helper.rb
rename to challenge-devops/code/test/test_helper.rb
diff --git a/challenge-devops/vendor/.keep b/challenge-devops/code/vendor/.keep
similarity index 100%
rename from challenge-devops/vendor/.keep
rename to challenge-devops/code/vendor/.keep
diff --git a/challenge-devops/deploy.sh b/challenge-devops/deploy.sh
new file mode 100755
index 0000000..7af1d77
--- /dev/null
+++ b/challenge-devops/deploy.sh
@@ -0,0 +1,54 @@
+#!/bin/bash
+export BASE_PATH=`pwd`
+export HELM_PATH=$BASE_PATH/helm
+if [ -z "$1" ]
+        then
+        echo "#################################################"        
+        echo "# please add environment argument !!            #"
+        echo "# =================================             #"
+        echo "# sh scriptname.sh dev or sh scriptname.sh prod #"
+        echo "#################################################"
+else
+        export ENV=$1
+        define_env()
+        {
+                export NAMESPACE=opn-$ENV
+                #please specify your kubeconfig file in my case i use as below file
+                export KUBECONFIG=~/.kube/config_$ENV
+        }
+
+        deploy_db()
+        {
+                cd $HELM_PATH/patroni
+                if [ "$ENV" = "prod" ]; then
+                       helm upgrade --install --atomic --timeout 30s --values values-$ENV.yaml --namespace  $NAMESPACE patroni . 
+
+                else
+                        helm upgrade --install --atomic --timeout 30s --values values-$ENV.yaml --namespace  $NAMESPACE patroni .
+                fi
+        }
+
+                deploy_apps()
+        {
+                cd $HELM_PATH/opn-devops
+                if [ "$ENV" = "prod" ]; then
+                         helm upgrade --install --atomic --timeout 30s --values values-$ENV.yaml $NAMESPACE opn-devops . 
+
+                else
+                        helm upgrade --install --atomic --timeout 30s --values values-$ENV.yaml --namespace  $NAMESPACE opn-devops . 
+                fi
+        }
+
+        echo "define Environment Variables:"
+        echo =================================
+        define_env
+        echo ""
+        echo "Deploy DB Using Helm Chart"
+        echo =================================
+        #deploy_db
+        echo ""
+        echo "Deploy Apps Using Helm Chart"
+        echo =================================
+        deploy_apps
+fi
+
diff --git a/challenge-devops/docker-compose.yml b/challenge-devops/docker-compose.yml
new file mode 100644
index 0000000..160fbdb
--- /dev/null
+++ b/challenge-devops/docker-compose.yml
@@ -0,0 +1,16 @@
+version: '3'
+services:
+        #  db:
+        #    image: postgres:9.6-alpine
+        #    volumes:
+        #      - ./tmp/db:/var/lib/postgresql/data
+        #   environment:
+        #      POSTGRES_PASSWORD: dr2711hp
+        #      POSTGRES_DB: development
+        #    ports:
+        #      - "5432:5432"
+  web:
+    build: .
+    image: haryopramudito/opn-devops:patroni
+    ports:
+      - "3000:3000"
diff --git a/challenge-devops/helm/opn-devops/.helmignore b/challenge-devops/helm/opn-devops/.helmignore
new file mode 100644
index 0000000..0e8a0eb
--- /dev/null
+++ b/challenge-devops/helm/opn-devops/.helmignore
@@ -0,0 +1,23 @@
+# Patterns to ignore when building packages.
+# This supports shell glob matching, relative path matching, and
+# negation (prefixed with !). Only one pattern per line.
+.DS_Store
+# Common VCS dirs
+.git/
+.gitignore
+.bzr/
+.bzrignore
+.hg/
+.hgignore
+.svn/
+# Common backup files
+*.swp
+*.bak
+*.tmp
+*.orig
+*~
+# Various IDEs
+.project
+.idea/
+*.tmproj
+.vscode/
diff --git a/challenge-devops/helm/opn-devops/Chart.yaml b/challenge-devops/helm/opn-devops/Chart.yaml
new file mode 100644
index 0000000..b9c35fa
--- /dev/null
+++ b/challenge-devops/helm/opn-devops/Chart.yaml
@@ -0,0 +1,24 @@
+apiVersion: v2
+name: opn-devops
+description: A Helm chart for Kubernetes
+
+# A chart can be either an 'application' or a 'library' chart.
+#
+# Application charts are a collection of templates that can be packaged into versioned archives
+# to be deployed.
+#
+# Library charts provide useful utilities or functions for the chart developer. They're included as
+# a dependency of application charts to inject those utilities and functions into the rendering
+# pipeline. Library charts do not define any templates and therefore cannot be deployed.
+type: application
+
+# This is the chart version. This version number should be incremented each time you make changes
+# to the chart and its templates, including the app version.
+# Versions are expected to follow Semantic Versioning (https://semver.org/)
+version: 0.1.0
+
+# This is the version number of the application being deployed. This version number should be
+# incremented each time you make changes to the application. Versions are not expected to
+# follow Semantic Versioning. They should reflect the version the application is using.
+# It is recommended to use it with quotes.
+appVersion: "1.16.0"
diff --git a/challenge-devops/helm/opn-devops/templates/NOTES.txt b/challenge-devops/helm/opn-devops/templates/NOTES.txt
new file mode 100644
index 0000000..d78d573
--- /dev/null
+++ b/challenge-devops/helm/opn-devops/templates/NOTES.txt
@@ -0,0 +1,22 @@
+1. Get the application URL by running these commands:
+{{- if .Values.ingress.enabled }}
+{{- range $host := .Values.ingress.hosts }}
+  {{- range .paths }}
+  http{{ if $.Values.ingress.tls }}s{{ end }}://{{ $host.host }}{{ .path }}
+  {{- end }}
+{{- end }}
+{{- else if contains "NodePort" .Values.service.type }}
+  export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[0].nodePort}" services {{ include "opn-devops.fullname" . }})
+  export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
+  echo http://$NODE_IP:$NODE_PORT
+{{- else if contains "LoadBalancer" .Values.service.type }}
+     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
+           You can watch the status of by running 'kubectl get --namespace {{ .Release.Namespace }} svc -w {{ include "opn-devops.fullname" . }}'
+  export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ include "opn-devops.fullname" . }} --template "{{"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}"}}")
+  echo http://$SERVICE_IP:{{ .Values.service.port }}
+{{- else if contains "ClusterIP" .Values.service.type }}
+  export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l "app.kubernetes.io/name={{ include "opn-devops.name" . }},app.kubernetes.io/instance={{ .Release.Name }}" -o jsonpath="{.items[0].metadata.name}")
+  export CONTAINER_PORT=$(kubectl get pod --namespace {{ .Release.Namespace }} $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
+  echo "Visit http://127.0.0.1:8080 to use your application"
+  kubectl --namespace {{ .Release.Namespace }} port-forward $POD_NAME 8080:$CONTAINER_PORT
+{{- end }}
diff --git a/challenge-devops/helm/opn-devops/templates/_helpers.tpl b/challenge-devops/helm/opn-devops/templates/_helpers.tpl
new file mode 100644
index 0000000..6839b51
--- /dev/null
+++ b/challenge-devops/helm/opn-devops/templates/_helpers.tpl
@@ -0,0 +1,62 @@
+{{/*
+Expand the name of the chart.
+*/}}
+{{- define "opn-devops.name" -}}
+{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" }}
+{{- end }}
+
+{{/*
+Create a default fully qualified app name.
+We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).
+If release name contains chart name it will be used as a full name.
+*/}}
+{{- define "opn-devops.fullname" -}}
+{{- if .Values.fullnameOverride }}
+{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" }}
+{{- else }}
+{{- $name := default .Chart.Name .Values.nameOverride }}
+{{- if contains $name .Release.Name }}
+{{- .Release.Name | trunc 63 | trimSuffix "-" }}
+{{- else }}
+{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" }}
+{{- end }}
+{{- end }}
+{{- end }}
+
+{{/*
+Create chart name and version as used by the chart label.
+*/}}
+{{- define "opn-devops.chart" -}}
+{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" }}
+{{- end }}
+
+{{/*
+Common labels
+*/}}
+{{- define "opn-devops.labels" -}}
+helm.sh/chart: {{ include "opn-devops.chart" . }}
+{{ include "opn-devops.selectorLabels" . }}
+{{- if .Chart.AppVersion }}
+app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
+{{- end }}
+app.kubernetes.io/managed-by: {{ .Release.Service }}
+{{- end }}
+
+{{/*
+Selector labels
+*/}}
+{{- define "opn-devops.selectorLabels" -}}
+app.kubernetes.io/name: {{ include "opn-devops.name" . }}
+app.kubernetes.io/instance: {{ .Release.Name }}
+{{- end }}
+
+{{/*
+Create the name of the service account to use
+*/}}
+{{- define "opn-devops.serviceAccountName" -}}
+{{- if .Values.serviceAccount.create }}
+{{- default (include "opn-devops.fullname" .) .Values.serviceAccount.name }}
+{{- else }}
+{{- default "default" .Values.serviceAccount.name }}
+{{- end }}
+{{- end }}
diff --git a/challenge-devops/helm/opn-devops/templates/deployment.yaml b/challenge-devops/helm/opn-devops/templates/deployment.yaml
new file mode 100644
index 0000000..7773454
--- /dev/null
+++ b/challenge-devops/helm/opn-devops/templates/deployment.yaml
@@ -0,0 +1,77 @@
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: {{ include "opn-devops.fullname" . }}
+  labels:
+    {{- include "opn-devops.labels" . | nindent 4 }}
+spec:
+  {{- if not .Values.autoscaling.enabled }}
+  replicas: {{ .Values.replicaCount }}
+  {{- end }}
+  selector:
+    matchLabels:
+      {{- include "opn-devops.selectorLabels" . | nindent 6 }}
+  template:
+    metadata:
+      {{- with .Values.podAnnotations }}
+      annotations:
+        {{- toYaml . | nindent 8 }}
+      {{- end }}
+      labels:
+        {{- include "opn-devops.selectorLabels" . | nindent 8 }}
+    spec:
+      {{- with .Values.imagePullSecrets }}
+      imagePullSecrets:
+        {{- toYaml . | nindent 8 }}
+      {{- end }}
+      serviceAccountName: {{ include "opn-devops.serviceAccountName" . }}
+      securityContext:
+        {{- toYaml .Values.podSecurityContext | nindent 8 }}
+      containers:
+        - name: db-migration
+          securityContext:
+            {{- toYaml .Values.securityContext | nindent 12 }}
+          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
+          command:
+            - /app/bin/rails
+          args:
+            - db:migrate
+          imagePullPolicy: {{ .Values.image.pullPolicy }}
+          env:
+            - name: {{ .Values.envvar.name }}
+              value: {{ .Values.envvar.RAILS_ENV }}
+          ports:
+            - name: http
+              containerPort: 3000
+              protocol: TCP
+        - name: {{ .Chart.Name }}
+          securityContext:
+            {{- toYaml .Values.securityContext | nindent 12 }}
+          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
+          imagePullPolicy: {{ .Values.image.pullPolicy }}
+          ports:
+            - name: http
+              containerPort: 3000
+              protocol: TCP
+          livenessProbe:
+            httpGet:
+              path: /
+              port: http
+          readinessProbe:
+            httpGet:
+              path: /
+              port: http
+          resources:
+            {{- toYaml .Values.resources | nindent 12 }}
+      {{- with .Values.nodeSelector }}
+      nodeSelector:
+        {{- toYaml . | nindent 8 }}
+      {{- end }}
+      {{- with .Values.affinity }}
+      affinity:
+        {{- toYaml . | nindent 8 }}
+      {{- end }}
+      {{- with .Values.tolerations }}
+      tolerations:
+        {{- toYaml . | nindent 8 }}
+      {{- end }}
diff --git a/challenge-devops/helm/opn-devops/templates/hpa.yaml b/challenge-devops/helm/opn-devops/templates/hpa.yaml
new file mode 100644
index 0000000..caf16c9
--- /dev/null
+++ b/challenge-devops/helm/opn-devops/templates/hpa.yaml
@@ -0,0 +1,28 @@
+{{- if .Values.autoscaling.enabled }}
+apiVersion: autoscaling/v2beta1
+kind: HorizontalPodAutoscaler
+metadata:
+  name: {{ include "opn-devops.fullname" . }}
+  labels:
+    {{- include "opn-devops.labels" . | nindent 4 }}
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: {{ include "opn-devops.fullname" . }}
+  minReplicas: {{ .Values.autoscaling.minReplicas }}
+  maxReplicas: {{ .Values.autoscaling.maxReplicas }}
+  metrics:
+    {{- if .Values.autoscaling.targetCPUUtilizationPercentage }}
+    - type: Resource
+      resource:
+        name: cpu
+        targetAverageUtilization: {{ .Values.autoscaling.targetCPUUtilizationPercentage }}
+    {{- end }}
+    {{- if .Values.autoscaling.targetMemoryUtilizationPercentage }}
+    - type: Resource
+      resource:
+        name: memory
+        targetAverageUtilization: {{ .Values.autoscaling.targetMemoryUtilizationPercentage }}
+    {{- end }}
+{{- end }}
diff --git a/challenge-devops/helm/opn-devops/templates/ingress.yaml b/challenge-devops/helm/opn-devops/templates/ingress.yaml
new file mode 100644
index 0000000..611da0c
--- /dev/null
+++ b/challenge-devops/helm/opn-devops/templates/ingress.yaml
@@ -0,0 +1,61 @@
+{{- if .Values.ingress.enabled -}}
+{{- $fullName := include "opn-devops.fullname" . -}}
+{{- $svcPort := .Values.service.port -}}
+{{- if and .Values.ingress.className (not (semverCompare ">=1.18-0" .Capabilities.KubeVersion.GitVersion)) }}
+  {{- if not (hasKey .Values.ingress.annotations "kubernetes.io/ingress.class") }}
+  {{- $_ := set .Values.ingress.annotations "kubernetes.io/ingress.class" .Values.ingress.className}}
+  {{- end }}
+{{- end }}
+{{- if semverCompare ">=1.19-0" .Capabilities.KubeVersion.GitVersion -}}
+apiVersion: networking.k8s.io/v1
+{{- else if semverCompare ">=1.14-0" .Capabilities.KubeVersion.GitVersion -}}
+apiVersion: networking.k8s.io/v1beta1
+{{- else -}}
+apiVersion: extensions/v1beta1
+{{- end }}
+kind: Ingress
+metadata:
+  name: {{ $fullName }}
+  labels:
+    {{- include "opn-devops.labels" . | nindent 4 }}
+  {{- with .Values.ingress.annotations }}
+  annotations:
+    {{- toYaml . | nindent 4 }}
+  {{- end }}
+spec:
+  {{- if and .Values.ingress.className (semverCompare ">=1.18-0" .Capabilities.KubeVersion.GitVersion) }}
+  ingressClassName: {{ .Values.ingress.className }}
+  {{- end }}
+  {{- if .Values.ingress.tls }}
+  tls:
+    {{- range .Values.ingress.tls }}
+    - hosts:
+        {{- range .hosts }}
+        - {{ . | quote }}
+        {{- end }}
+      secretName: {{ .secretName }}
+    {{- end }}
+  {{- end }}
+  rules:
+    {{- range .Values.ingress.hosts }}
+    - host: {{ .host | quote }}
+      http:
+        paths:
+          {{- range .paths }}
+          - path: {{ .path }}
+            {{- if and .pathType (semverCompare ">=1.18-0" $.Capabilities.KubeVersion.GitVersion) }}
+            pathType: {{ .pathType }}
+            {{- end }}
+            backend:
+              {{- if semverCompare ">=1.19-0" $.Capabilities.KubeVersion.GitVersion }}
+              service:
+                name: {{ $fullName }}
+                port:
+                  number: {{ $svcPort }}
+              {{- else }}
+              serviceName: {{ $fullName }}
+              servicePort: {{ $svcPort }}
+              {{- end }}
+          {{- end }}
+    {{- end }}
+{{- end }}
diff --git a/challenge-devops/helm/opn-devops/templates/service.yaml b/challenge-devops/helm/opn-devops/templates/service.yaml
new file mode 100644
index 0000000..c07b8af
--- /dev/null
+++ b/challenge-devops/helm/opn-devops/templates/service.yaml
@@ -0,0 +1,15 @@
+apiVersion: v1
+kind: Service
+metadata:
+  name: {{ include "opn-devops.fullname" . }}
+  labels:
+    {{- include "opn-devops.labels" . | nindent 4 }}
+spec:
+  type: {{ .Values.service.type }}
+  ports:
+    - port: {{ .Values.service.port }}
+      targetPort: http
+      protocol: TCP
+      name: http
+  selector:
+    {{- include "opn-devops.selectorLabels" . | nindent 4 }}
diff --git a/challenge-devops/helm/opn-devops/templates/serviceaccount.yaml b/challenge-devops/helm/opn-devops/templates/serviceaccount.yaml
new file mode 100644
index 0000000..0735344
--- /dev/null
+++ b/challenge-devops/helm/opn-devops/templates/serviceaccount.yaml
@@ -0,0 +1,12 @@
+{{- if .Values.serviceAccount.create -}}
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: {{ include "opn-devops.serviceAccountName" . }}
+  labels:
+    {{- include "opn-devops.labels" . | nindent 4 }}
+  {{- with .Values.serviceAccount.annotations }}
+  annotations:
+    {{- toYaml . | nindent 4 }}
+  {{- end }}
+{{- end }}
diff --git a/challenge-devops/helm/opn-devops/templates/tests/test-connection.yaml b/challenge-devops/helm/opn-devops/templates/tests/test-connection.yaml
new file mode 100644
index 0000000..3efd45b
--- /dev/null
+++ b/challenge-devops/helm/opn-devops/templates/tests/test-connection.yaml
@@ -0,0 +1,15 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  name: "{{ include "opn-devops.fullname" . }}-test-connection"
+  labels:
+    {{- include "opn-devops.labels" . | nindent 4 }}
+  annotations:
+    "helm.sh/hook": test
+spec:
+  containers:
+    - name: wget
+      image: busybox
+      command: ['wget']
+      args: ['{{ include "opn-devops.fullname" . }}:{{ .Values.service.port }}']
+  restartPolicy: Never
diff --git a/challenge-devops/helm/opn-devops/values-dev.yaml b/challenge-devops/helm/opn-devops/values-dev.yaml
new file mode 100644
index 0000000..ead4485
--- /dev/null
+++ b/challenge-devops/helm/opn-devops/values-dev.yaml
@@ -0,0 +1,86 @@
+# Default values for opn-devops.
+# This is a YAML-formatted file.
+# Declare variables to be passed into your templates.
+
+replicaCount: 1
+
+image:
+  repository: haryopramudito/opn-devops
+  pullPolicy: IfNotPresent
+  # Overrides the image tag whose default is the chart appVersion.
+  tag: "patroni"
+
+imagePullSecrets: []
+nameOverride: ""
+fullnameOverride: ""
+
+envvar:
+  name: RAILS_ENV
+  RAILS_ENV: "development"
+
+serviceAccount:
+  # Specifies whether a service account should be created
+  create: true
+  # Annotations to add to the service account
+  annotations: {}
+  # The name of the service account to use.
+  # If not set and create is true, a name is generated using the fullname template
+  name: ""
+
+podAnnotations: {}
+
+podSecurityContext: {}
+  # fsGroup: 2000
+
+securityContext: {}
+  # capabilities:
+  #   drop:
+  #   - ALL
+  # readOnlyRootFilesystem: true
+  # runAsNonRoot: true
+  # runAsUser: 1000
+
+service:
+  type: ClusterIP
+  port: 3000
+
+ingress:
+  enabled: false
+  className: ""
+  annotations: {}
+    # kubernetes.io/ingress.class: nginx
+    # kubernetes.io/tls-acme: "true"
+  hosts:
+    - host: chart-example.local
+      paths:
+        - path: /
+          pathType: ImplementationSpecific
+  tls: []
+  #  - secretName: chart-example-tls
+  #    hosts:
+  #      - chart-example.local
+
+resources: {}
+  # We usually recommend not to specify default resources and to leave this as a conscious
+  # choice for the user. This also increases chances charts run on environments with little
+  # resources, such as Minikube. If you do want to specify resources, uncomment the following
+  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
+  # limits:
+  #   cpu: 100m
+  #   memory: 128Mi
+  # requests:
+  #   cpu: 100m
+  #   memory: 128Mi
+
+autoscaling:
+  enabled: false
+  minReplicas: 1
+  maxReplicas: 100
+  targetCPUUtilizationPercentage: 80
+  # targetMemoryUtilizationPercentage: 80
+
+nodeSelector: {}
+
+tolerations: []
+
+affinity: {}
diff --git a/challenge-devops/helm/opn-devops/values-prod.yaml b/challenge-devops/helm/opn-devops/values-prod.yaml
new file mode 100644
index 0000000..ead4485
--- /dev/null
+++ b/challenge-devops/helm/opn-devops/values-prod.yaml
@@ -0,0 +1,86 @@
+# Default values for opn-devops.
+# This is a YAML-formatted file.
+# Declare variables to be passed into your templates.
+
+replicaCount: 1
+
+image:
+  repository: haryopramudito/opn-devops
+  pullPolicy: IfNotPresent
+  # Overrides the image tag whose default is the chart appVersion.
+  tag: "patroni"
+
+imagePullSecrets: []
+nameOverride: ""
+fullnameOverride: ""
+
+envvar:
+  name: RAILS_ENV
+  RAILS_ENV: "development"
+
+serviceAccount:
+  # Specifies whether a service account should be created
+  create: true
+  # Annotations to add to the service account
+  annotations: {}
+  # The name of the service account to use.
+  # If not set and create is true, a name is generated using the fullname template
+  name: ""
+
+podAnnotations: {}
+
+podSecurityContext: {}
+  # fsGroup: 2000
+
+securityContext: {}
+  # capabilities:
+  #   drop:
+  #   - ALL
+  # readOnlyRootFilesystem: true
+  # runAsNonRoot: true
+  # runAsUser: 1000
+
+service:
+  type: ClusterIP
+  port: 3000
+
+ingress:
+  enabled: false
+  className: ""
+  annotations: {}
+    # kubernetes.io/ingress.class: nginx
+    # kubernetes.io/tls-acme: "true"
+  hosts:
+    - host: chart-example.local
+      paths:
+        - path: /
+          pathType: ImplementationSpecific
+  tls: []
+  #  - secretName: chart-example-tls
+  #    hosts:
+  #      - chart-example.local
+
+resources: {}
+  # We usually recommend not to specify default resources and to leave this as a conscious
+  # choice for the user. This also increases chances charts run on environments with little
+  # resources, such as Minikube. If you do want to specify resources, uncomment the following
+  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
+  # limits:
+  #   cpu: 100m
+  #   memory: 128Mi
+  # requests:
+  #   cpu: 100m
+  #   memory: 128Mi
+
+autoscaling:
+  enabled: false
+  minReplicas: 1
+  maxReplicas: 100
+  targetCPUUtilizationPercentage: 80
+  # targetMemoryUtilizationPercentage: 80
+
+nodeSelector: {}
+
+tolerations: []
+
+affinity: {}
diff --git a/challenge-devops/helm/patroni/Chart.yaml b/challenge-devops/helm/patroni/Chart.yaml
new file mode 100755
index 0000000..cf574d6
--- /dev/null
+++ b/challenge-devops/helm/patroni/Chart.yaml
@@ -0,0 +1,10 @@
+apiVersion: v1
+appVersion: 1.5-p5
+deprecated: true
+description: 'DEPRECATED Highly available elephant herd: HA PostgreSQL cluster.'
+home: https://github.com/zalando/patroni
+name: patroni
+sources:
+- https://github.com/zalando/patroni
+- https://github.com/zalando/spilo
+version: 0.16.3
diff --git a/challenge-devops/helm/patroni/README.md b/challenge-devops/helm/patroni/README.md
new file mode 100755
index 0000000..08801e0
--- /dev/null
+++ b/challenge-devops/helm/patroni/README.md
@@ -0,0 +1,156 @@
+# ⚠️ Repo Archive Notice
+
+As of Nov 13, 2020, charts in this repo will no longer be updated.
+For more information, see the Helm Charts [Deprecation and Archive Notice](https://github.com/helm/charts#%EF%B8%8F-deprecation-and-archive-notice), and [Update](https://helm.sh/blog/charts-repo-deprecation/).
+
+# Patroni Helm Chart
+
+This directory contains a Kubernetes chart to deploy a five node [Patroni](https://github.com/zalando/patroni/) cluster using a [Spilo](https://github.com/zalando/spilo) and a StatefulSet.
+
+## DEPRECATION NOTICE
+
+This chart is deprecated and no longer supported.
+
+## Prerequisites Details
+* Kubernetes 1.9+
+* PV support on the underlying infrastructure
+
+## StatefulSet Details
+* https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/
+
+## StatefulSet Caveats
+* https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#limitations
+
+## Todo
+* Make namespace configurable
+
+## Chart Details
+This chart will do the following:
+
+* Implement a HA scalable PostgreSQL 10 cluster using a Kubernetes StatefulSet.
+
+## Installing the Chart
+
+To install the chart with the release name `my-release`:
+
+```console
+$ helm repo add incubator https://kubernetes-charts-incubator.storage.googleapis.com/
+$ helm dependency update
+$ helm install --name my-release incubator/patroni
+```
+
+To install the chart with randomly generated passwords:
+
+```console
+$ helm install --name my-release incubator/patroni \
+  --set credentials.superuser="$(< /dev/urandom tr -dc _A-Z-a-z-0-9 | head -c32)",credentials.admin="$(< /dev/urandom tr -dc _A-Z-a-z-0-9 | head -c32)",credentials.standby="$(< /dev/urandom tr -dc _A-Z-a-z-0-9 | head -c32)"
+```
+
+## Connecting to PostgreSQL
+
+Your access point is a cluster IP. In order to access it spin up another pod:
+
+```console
+$ kubectl run -i --tty --rm psql --image=postgres --restart=Never -- bash -il
+```
+
+Then, from inside the pod, connect to PostgreSQL:
+
+```console
+$ psql -U admin -h my-release-patroni.default.svc.cluster.local postgres
+<admin password from values.yaml>
+postgres=>
+```
+
+## Configuration
+
+The following table lists the configurable parameters of the patroni chart and their default values.
+
+|       Parameter                   |           Description                       |                         Default                     |
+|-----------------------------------|---------------------------------------------|-----------------------------------------------------|
+| `nameOverride`                    | Override the name of the chart              | `nil`                                               |
+| `fullnameOverride`                | Override the fullname of the chart          | `nil`                                               |
+| `replicaCount`                    | Amount of pods to spawn                     | `5`                                                 |
+| `image.repository`                | The image to pull                           | `registry.opensource.zalan.do/acid/spilo-10`        |
+| `image.tag`                       | The version of the image to pull            | `1.5-p5`                                            |
+| `image.pullPolicy`                | The pull policy                             | `IfNotPresent`                                      |
+| `credentials.superuser`           | Password of the superuser                   | `tea`                                               |
+| `credentials.admin`               | Password of the admin                       | `cola`                                              |
+| `credentials.standby`             | Password of the replication user            | `pinacolada`                                        |
+| `kubernetes.dcs.enable`           | Using Kubernetes as DCS                     | `true`                                              |
+| `kubernetes.configmaps.enable`    | Using Kubernetes configmaps instead of endpoints | `false`                                        |
+| `etcd.enable`                     | Using etcd as DCS                           | `false`                                             |
+| `etcd.deployChart`                | Deploy etcd chart                           | `false`                                             |
+| `etcd.host`                       | Host name of etcd cluster                   | `nil`                                               |
+| `etcd.discovery`                  | Domain name of etcd cluster                 | `nil`                                               |
+| `zookeeper.enable`                | Using ZooKeeper as DCS                      | `false`                                             |
+| `zookeeper.deployChart`           | Deploy ZooKeeper chart                      | `false`                                             |
+| `zookeeper.hosts`                 | List of ZooKeeper cluster members           | `host1:port1,host2:port,etc...`                     |
+| `consul.enable`                   | Using Consul as DCS                         | `false`                                             |
+| `consul.deployChart`              | Deploy Consul chart                         | `false`                                             |
+| `consul.host`                     | Host name of consul cluster                 | `nil`                                               |
+| `env`                             | Extra custom environment variables          | `{}`                                                |
+| `walE.enable`                     | Use of Wal-E tool for base backup/restore   | `false`                                             |
+| `walE.scheduleCronJob`            | Schedule of Wal-E backups                   | `00 01 * * *`                                       |
+| `walE.retainBackups`              | Number of base backups to retain            | `2`                                                 |
+| `walE.s3Bucket:`                  | Amazon S3 bucket used for wal-e backups     | `nil`                                               |
+| `walE.gcsBucket`                  | GCS storage used for Wal-E backups          | `nil`                                               |
+| `walE.kubernetesSecret`           | K8s secret name for provider bucket         | `nil`                                               |
+| `walE.backupThresholdMegabytes`   | Maximum size of the WAL segments accumulated after the base backup to consider WAL-E restore instead of pg_basebackup | `1024` |
+| `walE.backupThresholdPercentage`  | Maximum ratio (in percents) of the accumulated WAL files to the base backup to consider WAL-E restore instead of pg_basebackup | `30` |
+| `resources`                       | Any resources you wish to assign to the pod | `{}`                                                |
+| `nodeSelector`                    | Node label to use for scheduling            | `{}`                                                |
+| `tolerations`                     | List of node taints to tolerate             | `[]`                                                |
+| `affinityTemplate`                | A template string to use to generate the affinity settings | Anti-affinity preferred on hostname  |
+| `affinity`                        | Affinity settings. Overrides `affinityTemplate` if set. | `{}`                                    |
+| `schedulerName`                   | Alternate scheduler name                    | `nil`                                               |
+| `persistentVolume.accessModes`    | Persistent Volume access modes              | `[ReadWriteOnce]`                                   |
+| `persistentVolume.annotations`    | Annotations for Persistent Volume Claim`    | `{}`                                                |
+| `persistentVolume.mountPath`      | Persistent Volume mount root path           | `/home/postgres/pgdata`                             |
+| `persistentVolume.size`           | Persistent Volume size                      | `2Gi`                                               |
+| `persistentVolume.storageClass`   | Persistent Volume Storage Class             | `volume.alpha.kubernetes.io/storage-class: default` |
+| `persistentVolume.subPath`        | Subdirectory of Persistent Volume to mount  | `""`                                                |
+| `rbac.create`                     | Create required role and rolebindings       | `true`                                              |
+| `serviceAccount.create`           | If true, create a new service account	      | `true`                                              |
+| `serviceAccount.name`             | Service account to be used. If not set and `serviceAccount.create` is `true`, a name is generated using the fullname template | `nil` |
+
+Specify each parameter using the `--set key=value[,key=value]` argument to `helm install`.
+
+Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example,
+
+```console
+$ helm install --name my-release -f values.yaml incubator/patroni
+```
+
+> **Tip**: You can use the default [values.yaml](values.yaml)
+
+## Cleanup
+
+To remove the spawned pods you can run a simple `helm delete <release-name>`.
+
+Helm will however preserve created persistent volume claims,
+to also remove them execute the commands below.
+
+```console
+$ release=<release-name>
+$ helm delete $release
+$ kubectl delete pvc -l release=$release
+```
+
+## Internals
+
+Patroni is responsible for electing a PostgreSQL master pod by leveraging the
+DCS of your choice. After election it adds a `spilo-role=master` label to the
+elected master and set the label to `spilo-role=replica` for all replicas.
+Simultaneously it will update the `<release-name>-patroni` endpoint to let the
+service route traffic to the elected master.
+
+```console
+$ kubectl get pods -l spilo-role -L spilo-role
+NAME                   READY     STATUS    RESTARTS   AGE       SPILO-ROLE
+my-release-patroni-0   1/1       Running   0          9m        replica
+my-release-patroni-1   1/1       Running   0          9m        master
+my-release-patroni-2   1/1       Running   0          8m        replica
+my-release-patroni-3   1/1       Running   0          8m        replica
+my-release-patroni-4   1/1       Running   0          8m        replica
+```
diff --git a/challenge-devops/helm/patroni/charts/consul/Chart.yaml b/challenge-devops/helm/patroni/charts/consul/Chart.yaml
new file mode 100755
index 0000000..9c7109d
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/consul/Chart.yaml
@@ -0,0 +1,13 @@
+appVersion: 1.0.0
+description: Highly available and distributed service discovery and key-value store
+  designed with support for the modern data center to make distributed systems and
+  configuration easy.
+home: https://github.com/hashicorp/consul
+icon: https://raw.githubusercontent.com/hashicorp/consul/bce3809dfca37b883828c3715b84143dd71c0f85/website/source/assets/images/favicons/android-chrome-512x512.png
+maintainers:
+- email: lachlan.evenson@microsoft.com
+  name: lachie83
+name: consul
+sources:
+- https://github.com/kelseyhightower/consul-on-kubernetes
+version: 3.6.1
diff --git a/challenge-devops/helm/patroni/charts/consul/README.md b/challenge-devops/helm/patroni/charts/consul/README.md
new file mode 100755
index 0000000..5ed72bb
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/consul/README.md
@@ -0,0 +1,289 @@
+# Consul Helm Chart
+
+## Prerequisites Details
+* Kubernetes 1.6+
+* PV support on underlying infrastructure
+
+## StatefulSet Details
+* http://kubernetes.io/docs/concepts/abstractions/controllers/statefulsets/
+
+## Chart Details
+This chart will do the following:
+
+* Implemented a dynamically scalable consul cluster using Kubernetes StatefulSet
+
+## Installing the Chart
+
+To install the chart with the release name `my-release`:
+
+```bash
+$ helm install --name my-release stable/consul
+```
+
+## Configuration
+
+The following table lists the configurable parameters of the consul chart and their default values.
+
+| Parameter               | Description                           | Default                                                    |
+| ----------------------- | ----------------------------------    | ---------------------------------------------------------- |
+| `Name`                  | Consul statefulset name               | `consul`                                                   |
+| `Image`                 | Container image name                  | `consul`                                                   |
+| `ImageTag`              | Container image tag                   | `1.0.0`                                                    |
+| `ImagePullPolicy`       | Container pull policy                 | `Always`                                                   |
+| `Replicas`              | k8s statefulset replicas              | `3`                                                        |
+| `Component`             | k8s selector key                      | `consul`                                                   |
+| `ConsulConfig`          | List of secrets and configMaps containing consul configuration | []                                |
+| `Cpu`                   | container requested cpu               | `100m`                                                     |
+| `DatacenterName`        | Consul Datacenter Name                | `dc1` (The consul default)                                 |
+| `DisableHostNodeId`     | Disable Node Id creation (uses random)| `false`                                                    |
+| `EncryptGossip`         | Whether or not gossip is encrypted    | `true`                                                     |
+| `GossipKey`             | Gossip-key to use by all members      | `nil`                                                      |
+| `Storage`               | Persistent volume size                | `1Gi`                                                      |
+| `StorageClass`          | Persistent volume storage class       | `nil`                                                      |
+| `HttpPort`              | Consul http listening port            | `8500`                                                     |
+| `Resources`             | Container resource requests and limits| `{}`                                                       |
+| `priorityClassName`     | priorityClassName                     | `nil`                                                      |
+| `RpcPort`               | Consul rpc listening port             | `8400`                                                     |
+| `SerflanPort`           | Container serf lan listening port     | `8301`                                                     |
+| `SerflanUdpPort`        | Container serf lan UDP listening port | `8301`                                                     |
+| `SerfwanPort`           | Container serf wan listening port     | `8302`                                                     |
+| `SerfwanUdpPort`        | Container serf wan UDP listening port | `8302`                                                     |
+| `ServerPort`            | Container server listening port       | `8300`                                                     |
+| `ConsulDnsPort`         | Container dns listening port          | `8600`                                                     |
+| `affinity`              | Consul affinity settings              | `see values.yaml`                                          |
+| `nodeSelector`          | Node labels for pod assignment        | `{}`                                                       |
+| `tolerations`           | Tolerations for pod assignment        | `[]`                                                       |
+| `maxUnavailable`        | Pod disruption Budget maxUnavailable  | `1`                                                        |
+| `ui.enabled`            | Enable Consul Web UI                  | `true`                                                     |
+| `uiIngress.enabled`     | Create Ingress for Consul Web UI      | `false`                                                    |
+| `uiIngress.annotations` | Associate annotations to the Ingress  | `{}`                                                       |
+| `uiIngress.labels`      | Associate labels to the Ingress       | `{}`                                                       |
+| `uiIngress.hosts`       | Associate hosts with the Ingress      | `[]`                                                       |
+| `uiIngress.tls`         | Associate TLS with the Ingress        | `{}`                                                       |
+| `uiService.enabled`     | Create dedicated Consul Web UI svc    | `true`                                                     |
+| `uiService.type`        | Dedicate Consul Web UI svc type       | `NodePort`                                                 |
+| `uiService.annotations` | Extra annotations for UI service      | `{}`                                                       |
+| `acl.enabled`           | Enable basic ACL configuration        | `false`                                                    |
+| `acl.masterToken`       | Master token that was provided in consul ACL config file | `""`                                    |
+| `acl.agentToken`        | Agent token that was provided in consul ACL config file | `""`                                     |
+| `test.image`            | Test container image requires kubectl + bash (used for helm test)   | `lachlanevenson/k8s-kubectl` |
+| `test.imageTag`         | Test container image tag  (used for helm test)     | `v1.4.8-bash`                                 |
+| `test.rbac.create`                      | Create rbac for test container                 | `false`                           |
+| `test.rbac.serviceAccountName`          | Name of existed service account for test container    | ``                         |
+| `additionalLabels`      | Add labels to Pod and StatefulSet     | `{}`                                                       |
+
+Specify each parameter using the `--set key=value[,key=value]` argument to `helm install`.
+
+Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example,
+
+```bash
+$ helm install --name my-release -f values.yaml stable/consul
+```
+> **Tip**: `ConsulConfig` is impossible to set using --set as it's not possible to set list of hashes with it at the moment, use a YAML file instead.
+
+> **Tip**: You can use the default [values.yaml](values.yaml)
+
+## Further consul configuration
+
+To support passing in more detailed/complex configuration options using `secret`s or `configMap`s. As an example, here is what a `values.yaml` could look like:
+```yaml
+ConsulConfig:
+  - type: configMap
+    name: consul-defaults
+  - type: secret
+    name: consul-secrets
+```
+
+> These are both mounted as files in the consul pods, including the secrets. When they are changed, the cluster may need to be restarted.
+
+> **Important**: Kubernetes does not allow the volumes to be changed for a StatefulSet. If a new item needs to be added to this list, the StatefulSet needs to be deleted and re-created. The contents of each item can change and will be respected when the containers would read configuration (reload/restart).
+
+This would require the `consul-defaults` `configMap` and `consul-secrets` `secret` in the same `namespace`. There is no difference from the consul perspective, one could use only `secret`s, or only `configMap`s, or neither. They can each contain multiple consul configuration files (every `JSON` file contained in them will be interpreted as one). The order in which the configuration will be loaded is the same order as they are specified in the `ConsulConfig` setting (later overrides earlier). In case they contain multiple files, the order between those files is decided by consul (as per the [--config-dir](https://www.consul.io/docs/agent/options.html#_config_dir) argument in consul agent), but the order in `ConsulConfig` is still respected. The configuration generated by helm (this chart) is loaded last, and therefore overrides the configuration set here.
+
+## Cleanup orphaned Persistent Volumes
+
+Deleting a StateFul will not delete associated Persistent Volumes.
+
+Do the following after deleting the chart release to clean up orphaned Persistent Volumes.
+
+```bash
+$ kubectl delete pvc -l component=${RELEASE-NAME}-consul
+```
+
+## Pitfalls
+
+* When ACLs are enabled and `acl_default_policy` is set to `deny`, it is necessary to set the `acl_token` to a token that can perform at least the `consul members`, otherwise the kubernetes liveness probe will keep failing and the containers will be killed every 5 minutes.
+  * Basic ACLs configuration can be done by setting `acl.enabled` to `true`, and setting values for `acl.masterToken` and `acl.agentToken`.
+
+## Testing
+
+Helm tests are included and they confirm the first three cluster members have quorum.
+
+```bash
+helm test <RELEASE_NAME>
+RUNNING: inky-marsupial-ui-test-nn6lv
+PASSED: inky-marsupial-ui-test-nn6lv
+```
+
+It will confirm that there are at least 3 consul servers present.
+
+## Cluster Health
+
+```
+$ for i in <0..n>; do kubectl exec <consul-$i> -- sh -c 'consul members'; done
+```
+eg.
+```
+for i in {0..2}; do kubectl exec consul-$i --namespace=consul -- sh -c 'consul members'; done
+Node      Address          Status  Type    Build  Protocol  DC
+consul-0  10.244.2.6:8301  alive   server  0.6.4  2         dc1
+consul-1  10.244.3.8:8301  alive   server  0.6.4  2         dc1
+consul-2  10.244.1.7:8301  alive   server  0.6.4  2         dc1
+Node      Address          Status  Type    Build  Protocol  DC
+consul-0  10.244.2.6:8301  alive   server  0.6.4  2         dc1
+consul-1  10.244.3.8:8301  alive   server  0.6.4  2         dc1
+consul-2  10.244.1.7:8301  alive   server  0.6.4  2         dc1
+Node      Address          Status  Type    Build  Protocol  DC
+consul-0  10.244.2.6:8301  alive   server  0.6.4  2         dc1
+consul-1  10.244.3.8:8301  alive   server  0.6.4  2         dc1
+consul-2  10.244.1.7:8301  alive   server  0.6.4  2         dc1
+cluster is healthy
+```
+
+## Failover
+
+If any consul member fails it gets re-joined eventually.
+You can test the scenario by killing process of one of the pods:
+
+```
+shell
+$ ps aux | grep consul
+$ kill CONSUL_PID
+```
+
+```
+kubectl logs consul-0 --namespace=consul
+Waiting for consul-0.consul to come up
+Waiting for consul-1.consul to come up
+Waiting for consul-2.consul to come up
+==> WARNING: Expect Mode enabled, expecting 3 servers
+==> Starting Consul agent...
+==> Starting Consul agent RPC...
+==> Consul agent running!
+         Node name: 'consul-0'
+        Datacenter: 'dc1'
+            Server: true (bootstrap: false)
+       Client Addr: 0.0.0.0 (HTTP: 8500, HTTPS: -1, DNS: 8600, RPC: 8400)
+      Cluster Addr: 10.244.2.6 (LAN: 8301, WAN: 8302)
+    Gossip encrypt: false, RPC-TLS: false, TLS-Incoming: false
+             Atlas: <disabled>
+
+==> Log data will now stream in as it occurs:
+
+    2016/08/18 19:20:35 [INFO] serf: EventMemberJoin: consul-0 10.244.2.6
+    2016/08/18 19:20:35 [INFO] serf: EventMemberJoin: consul-0.dc1 10.244.2.6
+    2016/08/18 19:20:35 [INFO] raft: Node at 10.244.2.6:8300 [Follower] entering Follower state
+    2016/08/18 19:20:35 [INFO] serf: Attempting re-join to previously known node: consul-1: 10.244.3.8:8301
+    2016/08/18 19:20:35 [INFO] consul: adding LAN server consul-0 (Addr: 10.244.2.6:8300) (DC: dc1)
+    2016/08/18 19:20:35 [WARN] serf: Failed to re-join any previously known node
+    2016/08/18 19:20:35 [INFO] consul: adding WAN server consul-0.dc1 (Addr: 10.244.2.6:8300) (DC: dc1)
+    2016/08/18 19:20:35 [ERR] agent: failed to sync remote state: No cluster leader
+    2016/08/18 19:20:35 [INFO] agent: Joining cluster...
+    2016/08/18 19:20:35 [INFO] agent: (LAN) joining: [10.244.2.6 10.244.3.8 10.244.1.7]
+    2016/08/18 19:20:35 [INFO] serf: EventMemberJoin: consul-1 10.244.3.8
+    2016/08/18 19:20:35 [WARN] memberlist: Refuting an alive message
+    2016/08/18 19:20:35 [INFO] serf: EventMemberJoin: consul-2 10.244.1.7
+    2016/08/18 19:20:35 [INFO] serf: Re-joined to previously known node: consul-1: 10.244.3.8:8301
+    2016/08/18 19:20:35 [INFO] consul: adding LAN server consul-1 (Addr: 10.244.3.8:8300) (DC: dc1)
+    2016/08/18 19:20:35 [INFO] consul: adding LAN server consul-2 (Addr: 10.244.1.7:8300) (DC: dc1)
+    2016/08/18 19:20:35 [INFO] agent: (LAN) joined: 3 Err: <nil>
+    2016/08/18 19:20:35 [INFO] agent: Join completed. Synced with 3 initial agents
+    2016/08/18 19:20:51 [INFO] agent.rpc: Accepted client: 127.0.0.1:36302
+    2016/08/18 19:20:59 [INFO] agent.rpc: Accepted client: 127.0.0.1:36313
+    2016/08/18 19:21:01 [INFO] agent: Synced node info
+```
+
+## Scaling using kubectl
+
+The consul cluster can be scaled up by running ``kubectl patch`` or ``kubectl edit``. For example,
+
+```
+kubectl get pods -l "component=${RELEASE-NAME}-consul" --namespace=consul
+NAME       READY     STATUS    RESTARTS   AGE
+consul-0   1/1       Running   1          4h
+consul-1   1/1       Running   0          4h
+consul-2   1/1       Running   0          4h
+
+$ kubectl patch statefulset/consul -p '{"spec":{"replicas": 5}}'
+"consul" patched
+
+kubectl get pods -l "component=${RELEASE-NAME}-consul" --namespace=consul
+NAME       READY     STATUS    RESTARTS   AGE
+consul-0   1/1       Running   1          4h
+consul-1   1/1       Running   0          4h
+consul-2   1/1       Running   0          4h
+consul-3   1/1       Running   0          41s
+consul-4   1/1       Running   0          23s
+
+lachlanevenson@faux$ for i in {0..4}; do kubectl exec consul-$i --namespace=consul -- sh -c 'consul members'; done
+Node      Address          Status  Type    Build  Protocol  DC
+consul-0  10.244.2.6:8301  alive   server  0.6.4  2         dc1
+consul-1  10.244.3.8:8301  alive   server  0.6.4  2         dc1
+consul-2  10.244.1.7:8301  alive   server  0.6.4  2         dc1
+consul-3  10.244.2.7:8301  alive   server  0.6.4  2         dc1
+consul-4  10.244.2.8:8301  alive   server  0.6.4  2         dc1
+Node      Address          Status  Type    Build  Protocol  DC
+consul-0  10.244.2.6:8301  alive   server  0.6.4  2         dc1
+consul-1  10.244.3.8:8301  alive   server  0.6.4  2         dc1
+consul-2  10.244.1.7:8301  alive   server  0.6.4  2         dc1
+consul-3  10.244.2.7:8301  alive   server  0.6.4  2         dc1
+consul-4  10.244.2.8:8301  alive   server  0.6.4  2         dc1
+Node      Address          Status  Type    Build  Protocol  DC
+consul-0  10.244.2.6:8301  alive   server  0.6.4  2         dc1
+consul-1  10.244.3.8:8301  alive   server  0.6.4  2         dc1
+consul-2  10.244.1.7:8301  alive   server  0.6.4  2         dc1
+consul-3  10.244.2.7:8301  alive   server  0.6.4  2         dc1
+consul-4  10.244.2.8:8301  alive   server  0.6.4  2         dc1
+Node      Address          Status  Type    Build  Protocol  DC
+consul-0  10.244.2.6:8301  alive   server  0.6.4  2         dc1
+consul-1  10.244.3.8:8301  alive   server  0.6.4  2         dc1
+consul-2  10.244.1.7:8301  alive   server  0.6.4  2         dc1
+consul-3  10.244.2.7:8301  alive   server  0.6.4  2         dc1
+consul-4  10.244.2.8:8301  alive   server  0.6.4  2         dc1
+Node      Address          Status  Type    Build  Protocol  DC
+consul-0  10.244.2.6:8301  alive   server  0.6.4  2         dc1
+consul-1  10.244.3.8:8301  alive   server  0.6.4  2         dc1
+consul-2  10.244.1.7:8301  alive   server  0.6.4  2         dc1
+consul-3  10.244.2.7:8301  alive   server  0.6.4  2         dc1
+consul-4  10.244.2.8:8301  alive   server  0.6.4  2         dc1
+```
+
+Scale down
+```
+kubectl patch statefulset/consul -p '{"spec":{"replicas": 3}}' --namespace=consul
+"consul" patched
+lachlanevenson@faux$ kubectl get pods -l "component=${RELEASE-NAME}-consul" --namespace=consul
+NAME       READY     STATUS    RESTARTS   AGE
+consul-0   1/1       Running   1          4h
+consul-1   1/1       Running   0          4h
+consul-2   1/1       Running   0          4h
+lachlanevenson@faux$ for i in {0..2}; do kubectl exec consul-$i --namespace=consul -- sh -c 'consul members'; done
+Node      Address          Status  Type    Build  Protocol  DC
+consul-0  10.244.2.6:8301  alive   server  0.6.4  2         dc1
+consul-1  10.244.3.8:8301  alive   server  0.6.4  2         dc1
+consul-2  10.244.1.7:8301  alive   server  0.6.4  2         dc1
+consul-3  10.244.2.7:8301  failed  server  0.6.4  2         dc1
+consul-4  10.244.2.8:8301  failed  server  0.6.4  2         dc1
+Node      Address          Status  Type    Build  Protocol  DC
+consul-0  10.244.2.6:8301  alive   server  0.6.4  2         dc1
+consul-1  10.244.3.8:8301  alive   server  0.6.4  2         dc1
+consul-2  10.244.1.7:8301  alive   server  0.6.4  2         dc1
+consul-3  10.244.2.7:8301  failed  server  0.6.4  2         dc1
+consul-4  10.244.2.8:8301  failed  server  0.6.4  2         dc1
+Node      Address          Status  Type    Build  Protocol  DC
+consul-0  10.244.2.6:8301  alive   server  0.6.4  2         dc1
+consul-1  10.244.3.8:8301  alive   server  0.6.4  2         dc1
+consul-2  10.244.1.7:8301  alive   server  0.6.4  2         dc1
+consul-3  10.244.2.7:8301  failed  server  0.6.4  2         dc1
+consul-4  10.244.2.8:8301  failed  server  0.6.4  2         dc1
+```
diff --git a/challenge-devops/helm/patroni/charts/consul/templates/NOTES.txt b/challenge-devops/helm/patroni/charts/consul/templates/NOTES.txt
new file mode 100755
index 0000000..2db1aca
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/consul/templates/NOTES.txt
@@ -0,0 +1,7 @@
+1. Watch all cluster members come up.
+  $ kubectl get pods --namespace={{ .Release.Namespace }} -w
+2. Test cluster health using Helm test.
+  $ helm test {{ template "consul.fullname" . }}
+3. (Optional) Manually confirm consul cluster is healthy.
+  $ CONSUL_POD=$(kubectl get pods -l='release={{ template "consul.fullname" . }}' --output=jsonpath={.items[0].metadata.name})
+  $ kubectl exec $CONSUL_POD consul members --namespace={{ .Release.Namespace }} | grep server
diff --git a/challenge-devops/helm/patroni/charts/consul/templates/_helpers.tpl b/challenge-devops/helm/patroni/charts/consul/templates/_helpers.tpl
new file mode 100755
index 0000000..61af072
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/consul/templates/_helpers.tpl
@@ -0,0 +1,32 @@
+{{/* vim: set filetype=mustache: */}}
+{{/*
+Expand the name of the chart.
+*/}}
+{{- define "consul.name" -}}
+{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" -}}
+{{- end -}}
+
+{{/*
+Create a default fully qualified app name.
+We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).
+If release name contains chart name it will be used as a full name.
+*/}}
+{{- define "consul.fullname" -}}
+{{- if .Values.fullnameOverride -}}
+{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" -}}
+{{- else -}}
+{{- $name := default .Chart.Name .Values.nameOverride -}}
+{{- if contains $name .Release.Name -}}
+{{- .Release.Name | trunc 63 | trimSuffix "-" -}}
+{{- else -}}
+{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" -}}
+{{- end -}}
+{{- end -}}
+{{- end -}}
+
+{{/*
+Create chart name and version as used by the chart label.
+*/}}
+{{- define "consul.chart" -}}
+{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" -}}
+{{- end -}}
diff --git a/challenge-devops/helm/patroni/charts/consul/templates/basic-acls.yaml b/challenge-devops/helm/patroni/charts/consul/templates/basic-acls.yaml
new file mode 100755
index 0000000..e74723b
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/consul/templates/basic-acls.yaml
@@ -0,0 +1,46 @@
+{{- if .Values.acl.enabled}}
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: "configure-basic-acls"
+  annotations:
+    "helm.sh/hook": post-install
+    "helm.sh/hook-delete-policy": hook-succeeded
+  labels:
+    heritage: {{ .Release.Service | quote }}
+    release: {{ .Release.Name | quote }}
+    chart: {{ template "consul.chart" . }}
+    component: "{{ .Release.Name }}-{{ .Values.Component }}"
+spec:
+  template:
+    metadata:
+      name: "configure-basic-acls"
+      labels:
+        heritage: {{ .Release.Service | quote }}
+        release: {{ .Release.Name | quote }}
+        chart: {{ template "consul.chart" . }}
+        component: "{{ .Release.Name }}-{{ .Values.Component }}"
+    spec:
+      restartPolicy: Never
+      containers:
+      - name: "add-agent-acl"
+        image: appropriate/curl:latest
+        args:
+        - -X 
+        - PUT
+        - --header
+        - 'X-Consul-Token: {{ .Values.acl.masterToken }}'
+        - --data
+        - '{ "Name": "Agent Token", "Type": "client", "Rules": "node \"\" { policy = \"write\" } service \"\" { policy = \"read\" } key \"_rexec\" { policy = \"write\" }", "ID": "{{ .Values.acl.agentToken }}"}'
+        - 'http://{{ .Release.Name }}.{{ .Release.Namespace }}.svc.cluster.local:{{ .Values.HttpPort }}/v1/acl/create'
+      - name: "modify-anonymous-acl"
+        image: appropriate/curl:latest
+        args:
+        - -X 
+        - PUT
+        - --header
+        - 'X-Consul-Token: {{ .Values.acl.masterToken }}'
+        - --data
+        - '{ "Name": "Anonymous Token", "Type": "client", "Rules": "node \"\" { policy = \"read\" }", "ID": "anonymous"}'
+        - 'http://{{ .Release.Name }}.{{ .Release.Namespace }}.svc.cluster.local:{{ .Values.HttpPort }}/v1/acl/create'
+{{- end }}
\ No newline at end of file
diff --git a/challenge-devops/helm/patroni/charts/consul/templates/consul-ingress.yaml b/challenge-devops/helm/patroni/charts/consul/templates/consul-ingress.yaml
new file mode 100755
index 0000000..776fc07
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/consul/templates/consul-ingress.yaml
@@ -0,0 +1,35 @@
+{{- if .Values.uiIngress.enabled -}}
+{{- $releaseName := .Release.Name -}}
+{{- $servicePort := .Values.HttpPort -}}
+{{- $serviceName := printf "%s-%s" (include "consul.fullname" .) "ui" -}}
+apiVersion: extensions/v1beta1
+kind: Ingress
+metadata:
+  annotations:
+  {{- range $key, $value := .Values.uiIngress.annotations }}
+    {{ $key }}: {{ $value | quote }}
+  {{- end }}
+  labels:
+    heritage: {{ .Release.Service | quote }}
+    release: {{ .Release.Name | quote }}
+    chart: {{ template "consul.chart" . }}
+    component: "{{ .Release.Name }}-{{ .Values.Component }}"
+    {{- range $key, $value := .Values.uiIngress.labels }}
+    {{ $key }}: {{ $value | quote }}
+    {{- end }}
+  name: "{{ template "consul.fullname" . }}-ui"
+spec:
+  rules:
+  {{- range .Values.uiIngress.hosts }}
+    - host: {{ . }}
+      http:
+        paths:
+          - backend:
+              serviceName: {{ $serviceName }}
+              servicePort: {{ $servicePort }}
+  {{- end -}}
+  {{- if .Values.uiIngress.tls }}
+  tls:
+{{ toYaml .Values.uiIngress.tls | indent 4 }}
+  {{- end -}}
+{{- end }}
diff --git a/challenge-devops/helm/patroni/charts/consul/templates/consul-service.yaml b/challenge-devops/helm/patroni/charts/consul/templates/consul-service.yaml
new file mode 100755
index 0000000..75afd75
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/consul/templates/consul-service.yaml
@@ -0,0 +1,39 @@
+apiVersion: v1
+kind: Service
+metadata:
+  name: "{{ template "consul.fullname" . }}"
+  labels:
+    heritage: {{ .Release.Service | quote }}
+    release: {{ .Release.Name | quote }}
+    chart: {{ template "consul.chart" . }}
+    component: "{{ .Release.Name }}-{{ .Values.Component }}"
+  annotations:
+    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
+spec:
+  ports:
+  - name: http
+    port: {{ .Values.HttpPort }}
+  - name: rpc
+    port: {{ .Values.RpcPort }}
+  - name: serflan-tcp
+    protocol: "TCP"
+    port: {{ .Values.SerflanPort }}
+  - name: serflan-udp
+    protocol: "UDP"
+    port: {{ .Values.SerflanUdpPort }}
+  - name: serfwan-tcp
+    protocol: "TCP"
+    port: {{ .Values.SerfwanPort }}
+  - name: serfwan-udp
+    protocol: "UDP"
+    port: {{ .Values.SerfwanUdpPort }}
+  - name: server
+    port: {{.Values.ServerPort}}
+  - name: consuldns-tcp
+    port: {{.Values.ConsulDnsPort}}
+  - name: consuldns-udp
+    protocol: "UDP"
+    port: {{.Values.ConsulDnsPort}}
+  clusterIP: None
+  selector:
+    component: "{{ .Release.Name }}-{{ .Values.Component }}"
diff --git a/challenge-devops/helm/patroni/charts/consul/templates/consul-statefulset.yaml b/challenge-devops/helm/patroni/charts/consul/templates/consul-statefulset.yaml
new file mode 100755
index 0000000..8a78c4c
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/consul/templates/consul-statefulset.yaml
@@ -0,0 +1,202 @@
+apiVersion: apps/v1beta1
+kind: StatefulSet
+metadata:
+  name: "{{ template "consul.fullname" . }}"
+  labels:
+    heritage: {{ .Release.Service | quote }}
+    release: {{ .Release.Name | quote }}
+    chart: {{ template "consul.chart" . }}
+    component: "{{ .Release.Name }}-{{ .Values.Component }}"
+{{- if .Values.additionalLabels }}
+{{ toYaml .Values.additionalLabels | indent 4 }}
+{{- end }}
+spec:
+  serviceName: "{{ template "consul.fullname" . }}"
+  replicas: {{ default 3 .Values.Replicas }}
+  updateStrategy:
+    type: RollingUpdate
+  selector:
+    matchLabels:
+      release: {{ .Release.Name | quote }}
+      component: "{{ .Release.Name }}-{{ .Values.Component }}"
+  template:
+    metadata:
+      name: "{{ template "consul.fullname" . }}"
+      labels:
+        heritage: {{ .Release.Service | quote }}
+        release: {{ .Release.Name | quote }}
+        chart: {{ template "consul.chart" . }}
+        component: "{{ .Release.Name }}-{{ .Values.Component }}"
+{{- if .Values.additionalLabels }}
+{{ toYaml .Values.additionalLabels | indent 8 }}
+{{- end }}
+    spec:
+      securityContext:
+        fsGroup: 1000
+{{- if .Values.priorityClassName }}
+      priorityClassName: "{{ .Values.priorityClassName }}"
+{{- end }}
+    {{- if .Values.affinity }}
+      affinity:
+{{ tpl .Values.affinity . | indent 8 }}
+    {{- end }}
+    {{- if .Values.nodeSelector }}
+      nodeSelector:
+{{ toYaml .Values.nodeSelector | indent 8 }}
+    {{- end }}
+    {{- if .Values.tolerations }}
+      tolerations:
+{{ toYaml .Values.tolerations | indent 8 }}
+    {{- end }}
+      containers:
+      - name: "{{ template "consul.fullname" . }}"
+        image: "{{ .Values.Image }}:{{ .Values.ImageTag }}"
+        imagePullPolicy: "{{ .Values.ImagePullPolicy }}"
+        ports:
+        - name: http
+          containerPort: {{ .Values.HttpPort }}
+        - name: rpc
+          containerPort: {{ .Values.RpcPort }}
+        - name: serflan-tcp
+          protocol: "TCP"
+          containerPort: {{ .Values.SerflanPort }}
+        - name: serflan-udp
+          protocol: "UDP"
+          containerPort: {{ .Values.SerflanUdpPort }}
+        - name: serfwan-tcp
+          protocol: "TCP"
+          containerPort: {{ .Values.SerfwanPort }}
+        - name: serfwan-udp
+          protocol: "UDP"
+          containerPort: {{ .Values.SerfwanUdpPort }}
+        - name: server
+          containerPort: {{.Values.ServerPort}}
+        - name: consuldns-tcp
+          containerPort: {{.Values.ConsulDnsPort}}
+        - name: consuldns-udp
+          protocol: "UDP"
+          containerPort: {{.Values.ConsulDnsPort}}
+        resources:
+{{ toYaml .Values.Resources | indent 10 }}
+        env:
+        - name: INITIAL_CLUSTER_SIZE
+          value: {{ default 3 .Values.Replicas | quote }}
+        - name: STATEFULSET_NAME
+          value: "{{ template "consul.fullname" . }}"
+        - name: POD_IP
+          valueFrom:
+            fieldRef:
+              fieldPath: status.podIP
+        - name: STATEFULSET_NAMESPACE
+          valueFrom:
+            fieldRef:
+              fieldPath: metadata.namespace
+        - name: DNSPORT
+          value: "{{ .Values.ConsulDnsPort }}"
+        volumeMounts:
+        - name: datadir
+          mountPath: /var/lib/consul
+        - name: gossip-key
+          mountPath: /etc/consul/secrets
+          readOnly: true
+        {{ range .Values.ConsulConfig }}
+        - name: userconfig-{{ .name }}
+          readOnly: true
+          mountPath: /etc/consul/userconfig/{{ .name }}
+        {{ end }}
+        livenessProbe:
+          exec:
+            command:
+            - consul
+            - members
+            - -http-addr=http://127.0.0.1:{{ .Values.HttpPort }}
+          initialDelaySeconds: 300
+          timeoutSeconds: 5
+        command:
+          - "/bin/sh"
+          - "-ec"
+          - |
+            IP=$(hostname -i)
+
+            {{- if .Values.Gossip.Encrypt }}
+            if [ -e /etc/consul/secrets/gossip-key ]; then
+              echo "{\"encrypt\": \"$(base64 /etc/consul/secrets/gossip-key)\"}" > /etc/consul/encrypt.json
+              GOSSIP_KEY="-config-file /etc/consul/encrypt.json"
+            fi
+            {{- end }}
+
+            for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
+                while true; do
+                    echo "Waiting for ${STATEFULSET_NAME}-${i}.${STATEFULSET_NAME} to come up"
+                    ping -W 1 -c 1 ${STATEFULSET_NAME}-${i}.${STATEFULSET_NAME}.${STATEFULSET_NAMESPACE}.svc > /dev/null && break
+                    sleep 1s
+                done
+            done
+
+            PEERS=""
+            for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
+              NEXT_PEER="$(ping -c 1 ${STATEFULSET_NAME}-${i}.${STATEFULSET_NAME}.${STATEFULSET_NAMESPACE}.svc | awk -F'[()]' '/PING/{print $2}')"
+              if [ "${NEXT_PEER}" != "${POD_IP}" ]; then
+                PEERS="${PEERS}${PEERS:+ } -retry-join ${STATEFULSET_NAME}-${i}.${STATEFULSET_NAME}.${STATEFULSET_NAMESPACE}.svc"
+              fi
+            done
+
+            exec /bin/consul agent \
+            {{- range .Values.ConsulConfig }}
+              -config-dir /etc/consul/userconfig/{{ .name }} \
+            {{- end}}
+            {{- if .Values.ui.enabled }}
+              -ui \
+            {{- end }}
+            {{- if .Values.DisableHostNodeId }}
+              -disable-host-node-id \
+            {{- end }}
+            {{- if .Values.DatacenterName }}
+              -datacenter {{ .Values.DatacenterName }} \
+            {{- end }}
+            {{- if .Values.Domain }}
+              -domain={{ .Values.Domain }} \
+            {{- end }}
+              -data-dir=/var/lib/consul \
+              -server \
+              -bootstrap-expect=${INITIAL_CLUSTER_SIZE} \
+              -disable-keyring-file \
+              -bind=0.0.0.0 \
+              -advertise=${IP} \
+              ${PEERS} \
+            {{- if .Values.Gossip.Encrypt }}
+              ${GOSSIP_KEY} \
+            {{- end }}
+              -client=0.0.0.0 \
+              -dns-port=${DNSPORT} \
+              -http-port={{ .Values.HttpPort }}
+      volumes:
+      - name: gossip-key
+        secret:
+          secretName: {{ template "consul.fullname" . }}-gossip-key
+      {{ range .Values.ConsulConfig }}
+      - name: userconfig-{{ .name }}
+        {{ .type }}:
+          {{- if (eq .type "configMap") }}
+          name: {{ .name }}
+          {{- else if (eq .type "secret") }}
+          secretName: {{ .name }}
+          {{- end}}
+      {{ end }}
+  volumeClaimTemplates:
+  - metadata:
+      name: datadir
+    spec:
+      accessModes:
+        - "ReadWriteOnce"
+      resources:
+        requests:
+          # upstream recommended max is 700M
+          storage: "{{ .Values.Storage }}"
+    {{- if .Values.StorageClass }}
+    {{- if (eq "-" .Values.StorageClass) }}
+      storageClassName: ""
+    {{- else }}
+      storageClassName: "{{ .Values.StorageClass }}"
+    {{- end }}
+    {{- end }}
diff --git a/challenge-devops/helm/patroni/charts/consul/templates/consul-test-clusterrole.yaml b/challenge-devops/helm/patroni/charts/consul/templates/consul-test-clusterrole.yaml
new file mode 100755
index 0000000..d073b0a
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/consul/templates/consul-test-clusterrole.yaml
@@ -0,0 +1,18 @@
+{{- if .Values.test.rbac.create }}
+apiVersion: rbac.authorization.k8s.io/v1beta1
+kind: ClusterRole
+metadata:
+  labels:
+    app: {{ template "consul.name" . }}
+    chart: {{ template "consul.chart" . }}
+    heritage: {{ .Release.Service }}
+    release: {{ .Release.Name }}
+  name: {{ template "consul.fullname" . }}-test
+rules:
+- apiGroups: [""]
+  resources: ["pods", "pods/log"]
+  verbs: ["get", "list"]
+- apiGroups: [""]
+  resources: ["pods/exec"]
+  verbs: ["create"]
+{{- end }}
diff --git a/challenge-devops/helm/patroni/charts/consul/templates/consul-test-clusterrolebinding.yaml b/challenge-devops/helm/patroni/charts/consul/templates/consul-test-clusterrolebinding.yaml
new file mode 100755
index 0000000..9b39349
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/consul/templates/consul-test-clusterrolebinding.yaml
@@ -0,0 +1,19 @@
+{{- if .Values.test.rbac.create }}
+apiVersion: rbac.authorization.k8s.io/v1beta1
+kind: ClusterRoleBinding
+metadata:
+  labels:
+    app: {{ template "consul.name" . }}
+    chart: {{ template "consul.chart" . }}
+    heritage: {{ .Release.Service }}
+    release: {{ .Release.Name }}
+  name: {{ template "consul.fullname" . }}-test
+subjects:
+  - kind: ServiceAccount
+    name: {{ template "consul.fullname" . }}-test
+    namespace: {{ .Release.Namespace }}
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: ClusterRole
+  name: {{ template "consul.fullname" . }}-test
+{{- end }}
diff --git a/challenge-devops/helm/patroni/charts/consul/templates/consul-test-serviceaccount.yaml b/challenge-devops/helm/patroni/charts/consul/templates/consul-test-serviceaccount.yaml
new file mode 100755
index 0000000..3ce1cbf
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/consul/templates/consul-test-serviceaccount.yaml
@@ -0,0 +1,11 @@
+{{- if .Values.test.rbac.create }}
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  labels:
+    app: {{ template "consul.name" . }}
+    chart: {{ template "consul.chart" . }}
+    heritage: {{ .Release.Service }}
+    release: {{ .Release.Name }}
+  name: {{ template "consul.fullname" . }}-test
+{{- end }}
diff --git a/challenge-devops/helm/patroni/charts/consul/templates/consul-test.yaml b/challenge-devops/helm/patroni/charts/consul/templates/consul-test.yaml
new file mode 100755
index 0000000..8fd1833
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/consul/templates/consul-test.yaml
@@ -0,0 +1,40 @@
+apiVersion: v1
+kind: Pod
+metadata:
+  name: "{{ .Release.Name }}-ui-test-{{ randAlphaNum 5 | lower }}"
+  annotations:
+    "helm.sh/hook": test-success
+spec:
+  {{- if or .Values.test.rbac.create .Values.test.rbac.serviceAccountName }}
+  serviceAccountName: {{ if .Values.test.rbac.create }}{{ template "consul.fullname" . }}-test{{ else }}"{{ .Values.test.rbac.serviceAccountName }}"{{ end }}
+  {{- end }}
+  initContainers:
+    - name: test-framework
+      image: dduportal/bats:0.4.0
+      command:
+      - "bash"
+      - "-c"
+      - |
+        set -ex
+        # copy bats to tools dir
+        cp -R /usr/local/libexec/ /tools/bats/
+      volumeMounts:
+      - mountPath: /tools
+        name: tools
+  containers:
+    - name: {{ .Release.Name }}-ui-test
+      image: {{ .Values.test.image }}:{{ .Values.test.imageTag }}
+      command: ["/tools/bats/bats", "-t", "/tests/run.sh"]
+      volumeMounts:
+      - mountPath: /tests
+        name: tests
+        readOnly: true
+      - mountPath: /tools
+        name: tools
+  volumes:
+  - name: tests
+    configMap:
+      name: {{ template "consul.fullname" . }}-tests
+  - name: tools
+    emptyDir: {}
+  restartPolicy: Never
diff --git a/challenge-devops/helm/patroni/charts/consul/templates/gossip-secret.yaml b/challenge-devops/helm/patroni/charts/consul/templates/gossip-secret.yaml
new file mode 100755
index 0000000..19dc3c4
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/consul/templates/gossip-secret.yaml
@@ -0,0 +1,18 @@
+{{ if .Values.Gossip.Create }}
+apiVersion: v1
+kind: Secret
+metadata:
+  name: {{ template "consul.fullname" . }}-gossip-key
+  labels:
+    heritage: {{ .Release.Service | quote }}
+    release: {{ .Release.Name | quote }}
+    chart: {{ template "consul.chart" . }}
+    component: "{{ .Release.Name }}-{{ .Values.Component }}"
+type: Opaque
+data:
+  {{ if .Values.GossipKey }}
+  gossip-key: {{ .Values.GossipKey | b64enc }}
+  {{ else }}
+  gossip-key: {{ randAlphaNum 24 | b64enc }}
+  {{ end }}
+{{ end }}
diff --git a/challenge-devops/helm/patroni/charts/consul/templates/pod-dist-budget.yaml b/challenge-devops/helm/patroni/charts/consul/templates/pod-dist-budget.yaml
new file mode 100755
index 0000000..8806c50
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/consul/templates/pod-dist-budget.yaml
@@ -0,0 +1,16 @@
+{{- if .Values.maxUnavailable }}
+apiVersion: policy/v1beta1
+kind: PodDisruptionBudget
+metadata:
+  name: "{{ template "consul.fullname" . }}-pdb"
+  labels:
+    heritage: {{ .Release.Service | quote }}
+    release: {{ .Release.Name | quote }}
+    chart: {{ template "consul.chart" . }}
+    component: "{{ .Release.Name }}-{{ .Values.Component }}"
+spec:
+  maxUnavailable: {{ .Values.maxUnavailable }}
+  selector:
+    matchLabels:
+      component: "{{ .Release.Name }}-{{ .Values.Component }}"
+{{- end }}
diff --git a/challenge-devops/helm/patroni/charts/consul/templates/test-config.yaml b/challenge-devops/helm/patroni/charts/consul/templates/test-config.yaml
new file mode 100755
index 0000000..a5e2822
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/consul/templates/test-config.yaml
@@ -0,0 +1,24 @@
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: {{ template "consul.fullname" . }}-tests
+data:
+  run.sh: |-
+    @test "Testing Consul cluster has quorum" {
+      for i in {0..2}; do
+        for n in {1..30}; do
+          if [ `kubectl exec {{ template "consul.fullname" . }}-$i consul members --namespace={{ .Release.Namespace }} | grep server | wc -l` -ge "3" ]; then
+            echo "{{ template "consul.fullname" . }}-$i OK. consul members returning at least 3 records."
+            break
+          else
+            echo "{{ template "consul.fullname" . }}-$i ERROR. consul members returning less than 3 records."
+          fi
+
+          if [ "$n" -ge "30" ]; then
+            echo "Failed $n times to get members from {{ template "consul.fullname" . }}-$i"
+            exit 1
+          fi
+          sleep 10
+        done
+      done
+    }
diff --git a/challenge-devops/helm/patroni/charts/consul/templates/ui-service.yaml b/challenge-devops/helm/patroni/charts/consul/templates/ui-service.yaml
new file mode 100755
index 0000000..33cfd4d
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/consul/templates/ui-service.yaml
@@ -0,0 +1,22 @@
+{{- if .Values.uiService.enabled }}
+apiVersion: v1
+kind: Service
+metadata:
+  name: "{{ template "consul.fullname" . }}-ui"
+{{- with .Values.uiService.annotations }}
+  annotations:
+{{ toYaml . | indent 4 }}
+{{- end }}
+  labels:
+    heritage: {{ .Release.Service | quote }}
+    release: {{ .Release.Name | quote }}
+    chart: {{ template "consul.chart" . }}
+    component: "{{ .Release.Name }}-{{ .Values.Component }}"
+spec:
+  ports:
+  - name: http
+    port: {{ .Values.HttpPort }}
+  selector:
+    component: "{{ .Release.Name }}-{{ .Values.Component }}"
+  type: "{{ .Values.uiService.type }}"
+{{- end }}
diff --git a/challenge-devops/helm/patroni/charts/consul/values.yaml b/challenge-devops/helm/patroni/charts/consul/values.yaml
new file mode 100755
index 0000000..6e15bb4
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/consul/values.yaml
@@ -0,0 +1,139 @@
+# Default values for consul.
+# This is a YAML-formatted file.
+# Declare name/value pairs to be passed into your templates.
+# name: value
+
+## Consul service ports
+HttpPort: 8500
+RpcPort: 8400
+SerflanPort: 8301
+SerflanUdpPort: 8301
+SerfwanPort: 8302
+SerfwanUdpPort: 8302
+ServerPort: 8300
+ConsulDnsPort: 8600
+
+## Specify the domain with which consul should run with
+## This will be passed as a -domain parameter
+Domain: consul
+
+## Used as selector
+Component: "consul"
+Replicas: 3
+Image: "consul"
+ImageTag: "1.0.0"
+ImagePullPolicy: "Always"
+Resources: {}
+ # requests:
+ #   cpu: "100m"
+ #   memory: "256Mi"
+ # limits:
+ #   cpu: "500m"
+ #   memory: "512Mi"
+## Persistent volume size
+
+priorityClassName: ""
+
+Storage: "1Gi"
+
+## Needed for 0.8.0 and later IF all consul containers are spun up
+## on the same machine. Without this they all generate the same
+## host id.
+DisableHostNodeId: false
+
+## Datacenter name for consul. If not supplied, will use the consul
+## default 'dc1'
+# DatacenterName: dc1
+
+## Encrypt Gossip Traffic
+Gossip:
+  Encrypt: true
+  Create: true
+
+## predefined value for gossip key.
+## Will use a generated random alpha numeric if not provided
+# GossipKey: key
+
+## consul data Persistent Volume Storage Class
+## If defined, StorageClassName: <storageClass>
+## If set to "-", StorageClassName: "", which disables dynamic provisioning
+## If undefined (the default) or set to null, no storageClassName spec is
+##   set, choosing the default provisioner.  (gp2 on AWS, standard on
+##   GKE, AWS & OpenStack)
+##
+# StorageClass: "-"
+
+## Setting maxUnavailable will create a pod disruption budget that will prevent
+## voluntarty cluster administration from taking down too many consul pods. If
+## you set maxUnavailable, you should set it to ceil((n/2) - 1), where
+## n = Replicas. For example, if you have 5 or 6 Replicas, you'll want to set
+## maxUnavailable = 2. If you are using the default of 3 Replicas, you'll want
+## to set maxUnavailable to 1.
+maxUnavailable: 1
+
+## nodeAffinity settings
+# nodeAffinity:
+#   requiredDuringSchedulingIgnoredDuringExecution:
+#     nodeSelectorTerms:
+#     - matchExpressions:
+#       - key: cloud.google.com/gke-preemptible
+#         operator: NotIn
+#         values:
+#         - true
+
+## Affinity settings
+affinity: |
+  podAntiAffinity:
+    preferredDuringSchedulingIgnoredDuringExecution:
+    - weight: 1
+      podAffinityTerm:
+        topologyKey: kubernetes.io/hostname
+        labelSelector:
+          matchExpressions:
+          - key: component
+            operator: In
+            values:
+            - "{{ .Release.Name }}-{{ .Values.Component }}"
+
+## Enable Consul Web UI
+##
+ui:
+  enabled: true
+## Create dedicated UI service
+##
+uiService:
+  enabled: true
+  type: "NodePort"
+  annotations: {}
+
+ConsulConfig: []
+#  - type: secret
+#    name: consul-defaults
+#  - type: configMap
+#    name: consul-defaults
+
+## Create an Ingress for the Web UI
+uiIngress:
+  enabled: false
+  annotations: {}
+  labels: {}
+  hosts: []
+  tls: {}
+
+## Useful when ACLs are enabled
+acl:
+  enabled: false
+  masterToken: ""
+  agentToken: ""
+
+## test container details
+test:
+  image: lachlanevenson/k8s-kubectl
+  imageTag: v1.4.8-bash
+  rbac:
+    create: false
+    serviceAccountName: ""
+
+nodeSelector: {}
+tolerations: []
+additionalLabels: {}
diff --git a/challenge-devops/helm/patroni/charts/etcd/.helmignore b/challenge-devops/helm/patroni/charts/etcd/.helmignore
new file mode 100755
index 0000000..f0c1319
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/etcd/.helmignore
@@ -0,0 +1,21 @@
+# Patterns to ignore when building packages.
+# This supports shell glob matching, relative path matching, and
+# negation (prefixed with !). Only one pattern per line.
+.DS_Store
+# Common VCS dirs
+.git/
+.gitignore
+.bzr/
+.bzrignore
+.hg/
+.hgignore
+.svn/
+# Common backup files
+*.swp
+*.bak
+*.tmp
+*~
+# Various IDEs
+.project
+.idea/
+*.tmproj
diff --git a/challenge-devops/helm/patroni/charts/etcd/Chart.yaml b/challenge-devops/helm/patroni/charts/etcd/Chart.yaml
new file mode 100755
index 0000000..9a21f49
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/etcd/Chart.yaml
@@ -0,0 +1,12 @@
+appVersion: 2.2.5
+description: Distributed reliable key-value store for the most critical data of a
+  distributed system.
+home: https://github.com/coreos/etcd
+icon: https://raw.githubusercontent.com/coreos/etcd/master/logos/etcd-horizontal-color.png
+maintainers:
+- email: lachlan@deis.com
+  name: lachie83
+name: etcd
+sources:
+- https://github.com/coreos/etcd
+version: 0.6.2
diff --git a/challenge-devops/helm/patroni/charts/etcd/README.md b/challenge-devops/helm/patroni/charts/etcd/README.md
new file mode 100755
index 0000000..fa92cb0
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/etcd/README.md
@@ -0,0 +1,172 @@
+# Etcd Helm Chart
+
+Credit to https://github.com/ingvagabund. This is an implementation of that work
+
+* https://github.com/kubernetes/contrib/pull/1295
+
+## Prerequisites Details
+* Kubernetes 1.5 (for `StatefulSets` support)
+* PV support on the underlying infrastructure
+
+## StatefulSet Details
+* https://kubernetes.io/docs/concepts/abstractions/controllers/statefulsets/
+
+## StatefulSet Caveats
+* https://kubernetes.io/docs/concepts/abstractions/controllers/statefulsets/#limitations
+
+## Todo
+* Implement SSL
+
+## Chart Details
+This chart will do the following:
+
+* Implemented a dynamically scalable etcd cluster using Kubernetes StatefulSets
+
+## Installing the Chart
+
+To install the chart with the release name `my-release`:
+
+```bash
+$ helm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator
+$ helm install --name my-release incubator/etcd
+```
+
+## Configuration
+
+The following table lists the configurable parameters of the etcd chart and their default values.
+
+| Parameter               | Description                          | Default                                            |
+| ----------------------- | ------------------------------------ | -------------------------------------------------- |
+| `image.repository`      | Container image repository           | `k8s.gcr.io/etcd-amd64`                            |
+| `image.tag`             | Container image tag                  | `2.2.5`                                            |
+| `image.pullPolicy`      | Container pull policy                | `IfNotPresent`                                     |
+| `replicas`              | k8s statefulset replicas             | `3`                                                |
+| `resources`             | container required resources         | `{}`                                             |                                            |
+| `clientPort`            | k8s service port                     | `2379`                                             |
+| `peerPorts`             | Container listening port             | `2380`                                             |
+| `storage`               | Persistent volume size               | `1Gi`                                              |
+| `storageClass`          | Persistent volume storage class      | `anything`                                         |
+| `affinity`              | affinity settings for pod assignment | `{}`                                               |
+| `nodeSelector`          | Node labels for pod assignment       | `{}`                                               |
+| `tolerations`           | Toleration labels for pod assignment | `[]`                                               |
+| `extraEnv`              | Optional environment variables       | `[]`                                               |
+| `memoryMode`            | Using memory as backend storage      | `false`                                            |
+
+Specify each parameter using the `--set key=value[,key=value]` argument to `helm install`.
+
+Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example,
+
+```bash
+$ helm install --name my-release -f values.yaml incubator/etcd
+```
+
+> **Tip**: You can use the default [values.yaml](values.yaml)
+
+# Deep dive
+
+## Cluster Health
+
+```
+$ for i in <0..n>; do kubectl exec <release-podname-$i> -- sh -c 'etcdctl cluster-health'; done
+```
+eg.
+```
+$ for i in {0..9}; do kubectl exec named-lynx-etcd-$i --namespace=etcd -- sh -c 'etcdctl cluster-health'; done
+member 7878c44dabe58db is healthy: got healthy result from http://named-lynx-etcd-7.named-lynx-etcd:2379
+member 19d2ab7b415341cc is healthy: got healthy result from http://named-lynx-etcd-4.named-lynx-etcd:2379
+member 6b627d1b92282322 is healthy: got healthy result from http://named-lynx-etcd-3.named-lynx-etcd:2379
+member 6bb377156d9e3fb3 is healthy: got healthy result from http://named-lynx-etcd-0.named-lynx-etcd:2379
+member 8ebbb00c312213d6 is healthy: got healthy result from http://named-lynx-etcd-8.named-lynx-etcd:2379
+member a32e3e8a520ff75f is healthy: got healthy result from http://named-lynx-etcd-5.named-lynx-etcd:2379
+member dc83003f0a226816 is healthy: got healthy result from http://named-lynx-etcd-2.named-lynx-etcd:2379
+member e3dc94686f60465d is healthy: got healthy result from http://named-lynx-etcd-6.named-lynx-etcd:2379
+member f5ee1ca177a88a58 is healthy: got healthy result from http://named-lynx-etcd-1.named-lynx-etcd:2379
+cluster is healthy
+```
+
+## Failover
+
+If any etcd member fails it gets re-joined eventually.
+You can test the scenario by killing process of one of the replicas:
+
+```shell
+$ ps aux | grep etcd-1
+$ kill -9 ETCD_1_PID
+```
+
+```shell
+$ kubectl get pods -l "release=${RELEASE-NAME},app=etcd"
+NAME                 READY     STATUS        RESTARTS   AGE
+etcd-0               1/1       Running       0          54s
+etcd-2               1/1       Running       0          51s
+```
+
+After a while:
+
+```shell
+$ kubectl get pods -l "release=${RELEASE-NAME},app=etcd"
+NAME                 READY     STATUS    RESTARTS   AGE
+etcd-0               1/1       Running   0          1m
+etcd-1               1/1       Running   0          20s
+etcd-2               1/1       Running   0          1m
+```
+
+You can check state of re-joining from ``etcd-1``'s logs:
+
+```shell
+$ kubectl logs etcd-1
+Waiting for etcd-0.etcd to come up
+Waiting for etcd-1.etcd to come up
+ping: bad address 'etcd-1.etcd'
+Waiting for etcd-1.etcd to come up
+Waiting for etcd-2.etcd to come up
+Re-joining etcd member
+Updated member with ID 7fd61f3f79d97779 in cluster
+2016-06-20 11:04:14.962169 I | etcdmain: etcd Version: 2.2.5
+2016-06-20 11:04:14.962287 I | etcdmain: Git SHA: bc9ddf2
+...
+```
+
+## Scaling using kubectl
+
+This is for reference. Scaling should be managed by `helm upgrade`
+
+The etcd cluster can be scale up by running ``kubectl patch`` or ``kubectl edit``. For instance,
+
+```sh
+$ kubectl get pods -l "release=${RELEASE-NAME},app=etcd"
+NAME      READY     STATUS    RESTARTS   AGE
+etcd-0    1/1       Running   0          7m
+etcd-1    1/1       Running   0          7m
+etcd-2    1/1       Running   0          6m
+
+$ kubectl patch statefulset/etcd -p '{"spec":{"replicas": 5}}'
+"etcd" patched
+
+$ kubectl get pods -l "release=${RELEASE-NAME},app=etcd"
+NAME      READY     STATUS    RESTARTS   AGE
+etcd-0    1/1       Running   0          8m
+etcd-1    1/1       Running   0          8m
+etcd-2    1/1       Running   0          8m
+etcd-3    1/1       Running   0          4s
+etcd-4    1/1       Running   0          1s
+```
+
+Scaling-down is similar. For instance, changing the number of replicas to ``4``:
+
+```sh
+$ kubectl edit statefulset/etcd
+statefulset "etcd" edited
+
+$ kubectl get pods -l "release=${RELEASE-NAME},app=etcd"
+NAME      READY     STATUS    RESTARTS   AGE
+etcd-0    1/1       Running   0          8m
+etcd-1    1/1       Running   0          8m
+etcd-2    1/1       Running   0          8m
+etcd-3    1/1       Running   0          4s
+```
+
+Once a replica is terminated (either by running ``kubectl delete pod etcd-ID`` or scaling down),
+content of ``/var/run/etcd/`` directory is cleaned up.
+If any of the etcd pods restarts (e.g. caused by etcd failure or any other),
+the directory is kept untouched so the pod can recover from the failure.
diff --git a/challenge-devops/helm/patroni/charts/etcd/templates/NOTES.txt b/challenge-devops/helm/patroni/charts/etcd/templates/NOTES.txt
new file mode 100755
index 0000000..7c94006
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/etcd/templates/NOTES.txt
@@ -0,0 +1 @@
+etcd has been installed.
\ No newline at end of file
diff --git a/challenge-devops/helm/patroni/charts/etcd/templates/_helpers.tpl b/challenge-devops/helm/patroni/charts/etcd/templates/_helpers.tpl
new file mode 100755
index 0000000..fe486ea
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/etcd/templates/_helpers.tpl
@@ -0,0 +1,32 @@
+{{/* vim: set filetype=mustache: */}}
+{{/*
+Expand the name of the chart.
+*/}}
+{{- define "etcd.name" -}}
+{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" -}}
+{{- end -}}
+
+{{/*
+Create a default fully qualified app name.
+We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).
+If release name contains chart name it will be used as a full name.
+*/}}
+{{- define "etcd.fullname" -}}
+{{- if .Values.fullnameOverride -}}
+{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" -}}
+{{- else -}}
+{{- $name := default .Chart.Name .Values.nameOverride -}}
+{{- if contains $name .Release.Name -}}
+{{- .Release.Name | trunc 63 | trimSuffix "-" -}}
+{{- else -}}
+{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" -}}
+{{- end -}}
+{{- end -}}
+{{- end -}}
+
+{{/*
+Create chart name and version as used by the chart label.
+*/}}
+{{- define "etcd.chart" -}}
+{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" -}}
+{{- end -}}
diff --git a/challenge-devops/helm/patroni/charts/etcd/templates/service.yaml b/challenge-devops/helm/patroni/charts/etcd/templates/service.yaml
new file mode 100755
index 0000000..174efe8
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/etcd/templates/service.yaml
@@ -0,0 +1,22 @@
+apiVersion: v1
+kind: Service
+metadata:
+  annotations:
+    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
+metadata:
+  name: {{ template "etcd.fullname" . }}
+  labels:
+    heritage: {{ .Release.Service | quote }}
+    release: {{ .Release.Name | quote }}
+    chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
+    app: {{ template "etcd.name" . }}
+spec:
+  ports:
+  - port: {{ .Values.peerPort }}
+    name: etcd-server
+  - port: {{ .Values.clientPort }}
+    name: etcd-client
+  clusterIP: None
+  selector:
+    app: {{ template "etcd.name" . }}
+    release: {{ .Release.Name | quote }}
\ No newline at end of file
diff --git a/challenge-devops/helm/patroni/charts/etcd/templates/statefulset.yaml b/challenge-devops/helm/patroni/charts/etcd/templates/statefulset.yaml
new file mode 100755
index 0000000..bc45ae4
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/etcd/templates/statefulset.yaml
@@ -0,0 +1,217 @@
+apiVersion: apps/v1beta1
+kind: StatefulSet
+metadata:
+  name: {{ template "etcd.fullname" . }}
+  labels:
+    heritage: {{ .Release.Service | quote }}
+    release: {{ .Release.Name | quote }}
+    chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
+    app: {{ template "etcd.name" . }}
+spec:
+  serviceName: {{ template "etcd.fullname" . }}
+  replicas: {{ .Values.replicas }}
+  template:
+    metadata:
+      name: {{ template "etcd.fullname" . }}
+      labels:
+        heritage: {{ .Release.Service | quote }}
+        release: {{ .Release.Name | quote }}
+        chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
+        app: {{ template "etcd.name" . }}
+    spec:
+{{- if .Values.affinity }}
+      affinity:
+{{ toYaml .Values.affinity | indent 8 }}
+{{- end }}
+{{- if .Values.nodeSelector }}
+      nodeSelector:
+{{ toYaml .Values.nodeSelector | indent 8 }}
+{{- end }}
+{{- if .Values.tolerations }}
+      tolerations:
+{{ toYaml .Values.tolerations | indent 8 }}
+{{- end }}
+      containers:
+      - name: {{ template "etcd.fullname" . }}
+        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
+        imagePullPolicy: "{{ .Values.image.pullPolicy }}"
+        ports:
+        - containerPort: {{ .Values.peerPort }}
+          name: peer
+        - containerPort: {{ .Values.clientPort }}
+          name: client
+        resources:
+{{ toYaml .Values.resources | indent 10 }}          
+        env:
+        - name: INITIAL_CLUSTER_SIZE
+          value: {{ .Values.replicas | quote }}
+        - name: SET_NAME
+          value: {{ template "etcd.fullname" . }}
+{{- if .Values.extraEnv }}
+{{ toYaml .Values.extraEnv | indent 8 }}
+{{- end }}
+        volumeMounts:
+        - name: datadir
+          mountPath: /var/run/etcd
+        lifecycle:
+          preStop:
+            exec:
+              command:
+                - "/bin/sh"
+                - "-ec"
+                - |
+                  EPS=""
+                  for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
+                      EPS="${EPS}${EPS:+,}http://${SET_NAME}-${i}.${SET_NAME}:2379"
+                  done
+
+                  HOSTNAME=$(hostname)
+
+                  member_hash() {
+                      etcdctl member list | grep http://${HOSTNAME}.${SET_NAME}:2380 | cut -d':' -f1 | cut -d'[' -f1
+                  }
+
+                  SET_ID=${HOSTNAME##*[^0-9]}
+
+                  if [ "${SET_ID}" -ge ${INITIAL_CLUSTER_SIZE} ]; then
+                      echo "Removing ${HOSTNAME} from etcd cluster"
+                      ETCDCTL_ENDPOINT=${EPS} etcdctl member remove $(member_hash)
+                      if [ $? -eq 0 ]; then
+                          # Remove everything otherwise the cluster will no longer scale-up
+                          rm -rf /var/run/etcd/*
+                      fi
+                  fi
+        command:
+          - "/bin/sh"
+          - "-ec"
+          - |
+            HOSTNAME=$(hostname)
+
+            # store member id into PVC for later member replacement
+            collect_member() {
+                while ! etcdctl member list &>/dev/null; do sleep 1; done
+                etcdctl member list | grep http://${HOSTNAME}.${SET_NAME}:2380 | cut -d':' -f1 | cut -d'[' -f1 > /var/run/etcd/member_id
+                exit 0
+            }
+
+            eps() {
+                EPS=""
+                for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
+                    EPS="${EPS}${EPS:+,}http://${SET_NAME}-${i}.${SET_NAME}:2379"
+                done
+                echo ${EPS}
+            }
+
+            member_hash() {
+                etcdctl member list | grep http://${HOSTNAME}.${SET_NAME}:2380 | cut -d':' -f1 | cut -d'[' -f1
+            }
+
+            # we should wait for other pods to be up before trying to join
+            # otherwise we got "no such host" errors when trying to resolve other members
+            for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
+                while true; do
+                    echo "Waiting for ${SET_NAME}-${i}.${SET_NAME} to come up"
+                    ping -W 1 -c 1 ${SET_NAME}-${i}.${SET_NAME} > /dev/null && break
+                    sleep 1s
+                done
+            done
+
+            # re-joining after failure?
+            if [ -e /var/run/etcd/default.etcd ]; then
+                echo "Re-joining etcd member"
+                member_id=$(cat /var/run/etcd/member_id)
+
+                # re-join member
+                ETCDCTL_ENDPOINT=$(eps) etcdctl member update ${member_id} http://${HOSTNAME}.${SET_NAME}:2380 | true
+                exec etcd --name ${HOSTNAME} \
+                    --listen-peer-urls http://0.0.0.0:2380 \
+                    --listen-client-urls http://0.0.0.0:2379\
+                    --advertise-client-urls http://${HOSTNAME}.${SET_NAME}:2379 \
+                    --data-dir /var/run/etcd/default.etcd
+            fi
+
+            # etcd-SET_ID
+            SET_ID=${HOSTNAME##*[^0-9]}
+
+            # adding a new member to existing cluster (assuming all initial pods are available)
+            if [ "${SET_ID}" -ge ${INITIAL_CLUSTER_SIZE} ]; then
+                export ETCDCTL_ENDPOINT=$(eps)
+
+                # member already added?
+                MEMBER_HASH=$(member_hash)
+                if [ -n "${MEMBER_HASH}" ]; then
+                    # the member hash exists but for some reason etcd failed
+                    # as the datadir has not be created, we can remove the member
+                    # and retrieve new hash
+                    etcdctl member remove ${MEMBER_HASH}
+                fi
+
+                echo "Adding new member"
+                etcdctl member add ${HOSTNAME} http://${HOSTNAME}.${SET_NAME}:2380 | grep "^ETCD_" > /var/run/etcd/new_member_envs
+
+                if [ $? -ne 0 ]; then
+                    echo "Exiting"
+                    rm -f /var/run/etcd/new_member_envs
+                    exit 1
+                fi
+
+                cat /var/run/etcd/new_member_envs
+                source /var/run/etcd/new_member_envs
+
+                collect_member &
+
+                exec etcd --name ${HOSTNAME} \
+                    --listen-peer-urls http://0.0.0.0:2380 \
+                    --listen-client-urls http://0.0.0.0:2379 \
+                    --advertise-client-urls http://${HOSTNAME}.${SET_NAME}:2379 \
+                    --data-dir /var/run/etcd/default.etcd \
+                    --initial-advertise-peer-urls http://${HOSTNAME}.${SET_NAME}:2380 \
+                    --initial-cluster ${ETCD_INITIAL_CLUSTER} \
+                    --initial-cluster-state ${ETCD_INITIAL_CLUSTER_STATE}
+            fi
+
+            PEERS=""
+            for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
+                PEERS="${PEERS}${PEERS:+,}${SET_NAME}-${i}=http://${SET_NAME}-${i}.${SET_NAME}:2380"
+            done
+
+            collect_member &
+
+            # join member
+            exec etcd --name ${HOSTNAME} \
+                --initial-advertise-peer-urls http://${HOSTNAME}.${SET_NAME}:2380 \
+                --listen-peer-urls http://0.0.0.0:2380 \
+                --listen-client-urls http://0.0.0.0:2379 \
+                --advertise-client-urls http://${HOSTNAME}.${SET_NAME}:2379 \
+                --initial-cluster-token etcd-cluster-1 \
+                --initial-cluster ${PEERS} \
+                --initial-cluster-state new \
+                --data-dir /var/run/etcd/default.etcd
+  {{- if .Values.persistentVolume.enabled }}
+  volumeClaimTemplates:
+  - metadata:
+      name: datadir
+    spec:
+      accessModes:
+        - "ReadWriteOnce"
+      resources:
+        requests:
+          # upstream recommended max is 700M
+          storage: "{{ .Values.persistentVolume.storage }}"
+    {{- if .Values.persistentVolume.storageClass }}
+    {{- if (eq "-" .Values.persistentVolume.storageClass) }}
+      storageClassName: ""
+    {{- else }}
+      storageClassName: "{{ .Values.persistentVolume.storageClass }}"
+    {{- end }}
+    {{- end }}
+  {{- else }}
+      volumes:
+      - name: datadir
+      {{- if .Values.memoryMode }}
+        emptyDir:
+          medium: Memory
+      {{- else }}
+        emptyDir: {}
+      {{- end }}
+  {{- end }}
diff --git a/challenge-devops/helm/patroni/charts/etcd/values.yaml b/challenge-devops/helm/patroni/charts/etcd/values.yaml
new file mode 100755
index 0000000..6e8be12
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/etcd/values.yaml
@@ -0,0 +1,49 @@
+# Default values for etcd.
+# This is a YAML-formatted file.
+# Declare name/value pairs to be passed into your templates.
+# name: value
+
+peerPort: 2380
+clientPort: 2379
+component: "etcd"
+replicas: 3
+image:
+  repository: "k8s.gcr.io/etcd-amd64"
+  tag: "2.2.5"
+  pullPolicy: "IfNotPresent"
+resources: {}
+# We usually recommend not to specify default resources and to leave this as a conscious
+# choice for the user. This also increases chances charts run on environments with little
+# resources, such as Minikube. If you do want to specify resources, uncomment the following
+# lines, adjust them as necessary, and remove the curly braces after 'resources:'.
+# limits:
+#  cpu: 100m
+#  memory: 128Mi
+# requests:
+#  cpu: 100m
+#  memory: 128Mi
+
+persistentVolume:
+  enabled: false
+  # storage: "1Gi"
+  ## etcd data Persistent Volume Storage Class
+  ## If defined, storageClassName: <storageClass>
+  ## If set to "-", storageClassName: "", which disables dynamic provisioning
+  ## If undefined (the default) or set to null, no storageClassName spec is
+  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
+  ##   GKE, AWS & OpenStack)
+  ##
+  # storageClass: "-"
+
+## This is only available when persistentVolume is false:
+## If persistentVolume is not enabled, one can choose to use memory mode for ETCD by setting memoryMode to "true".
+## The system will create a volume with "medium: Memory"
+memoryMode: false
+
+## Node labels and tolerations for pod assignment
+## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
+## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#taints-and-tolerations-beta-feature
+nodeSelector: {}
+tolerations: []
+affinity: {}
+extraEnv: []
diff --git a/challenge-devops/helm/patroni/charts/zookeeper/.helmignore b/challenge-devops/helm/patroni/charts/zookeeper/.helmignore
new file mode 100755
index 0000000..f0c1319
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/zookeeper/.helmignore
@@ -0,0 +1,21 @@
+# Patterns to ignore when building packages.
+# This supports shell glob matching, relative path matching, and
+# negation (prefixed with !). Only one pattern per line.
+.DS_Store
+# Common VCS dirs
+.git/
+.gitignore
+.bzr/
+.bzrignore
+.hg/
+.hgignore
+.svn/
+# Common backup files
+*.swp
+*.bak
+*.tmp
+*~
+# Various IDEs
+.project
+.idea/
+*.tmproj
diff --git a/challenge-devops/helm/patroni/charts/zookeeper/Chart.yaml b/challenge-devops/helm/patroni/charts/zookeeper/Chart.yaml
new file mode 100755
index 0000000..2d49473
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/zookeeper/Chart.yaml
@@ -0,0 +1,15 @@
+appVersion: 3.4.10
+description: Centralized service for maintaining configuration information, naming,
+  providing distributed synchronization, and providing group services.
+home: https://zookeeper.apache.org/
+icon: https://zookeeper.apache.org/images/zookeeper_small.gif
+maintainers:
+- email: lachlan.evenson@microsoft.com
+  name: lachie83
+- email: owensk@google.com
+  name: kow3ns
+name: zookeeper
+sources:
+- https://github.com/apache/zookeeper
+- https://github.com/kubernetes/contrib/tree/master/statefulsets/zookeeper
+version: 1.0.0
diff --git a/challenge-devops/helm/patroni/charts/zookeeper/OWNERS b/challenge-devops/helm/patroni/charts/zookeeper/OWNERS
new file mode 100755
index 0000000..dd9facd
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/zookeeper/OWNERS
@@ -0,0 +1,6 @@
+approvers:
+- lachie83
+- kow3ns
+reviewers:
+- lachie83
+- kow3ns
diff --git a/challenge-devops/helm/patroni/charts/zookeeper/README.md b/challenge-devops/helm/patroni/charts/zookeeper/README.md
new file mode 100755
index 0000000..8cbe4bb
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/zookeeper/README.md
@@ -0,0 +1,140 @@
+# incubator/zookeeper
+
+This helm chart provides an implementation of the ZooKeeper [StatefulSet](http://kubernetes.io/docs/concepts/abstractions/controllers/statefulsets/) found in Kubernetes Contrib [Zookeeper StatefulSet](https://github.com/kubernetes/contrib/tree/master/statefulsets/zookeeper).
+
+## Prerequisites
+* Kubernetes 1.6+
+* PersistentVolume support on the underlying infrastructure
+* A dynamic provisioner for the PersistentVolumes
+* A familiarity with [Apache ZooKeeper 3.4.x](https://zookeeper.apache.org/doc/current/)
+
+## Chart Components
+This chart will do the following:
+
+* Create a fixed size ZooKeeper ensemble using a [StatefulSet](http://kubernetes.io/docs/concepts/abstractions/controllers/statefulsets/).
+* Create a [PodDisruptionBudget](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-disruption-budget/) so kubectl drain will respect the Quorum size of the ensemble.
+* Create a [Headless Service](https://kubernetes.io/docs/concepts/services-networking/service/) to control the domain of the ZooKeeper ensemble.
+* Create a Service configured to connect to the available ZooKeeper instance on the configured client port.
+* Optionally apply a [Pod Anti-Affinity](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-feature) to spread the ZooKeeper ensemble across nodes.
+* Optionally start JMX Exporter and Zookeeper Exporter containers inside Zookeeper pods.
+* Optionally create a job which creates Zookeeper chroots (e.g. `/kafka1`).
+
+## Installing the Chart
+You can install the chart with the release name `zookeeper` as below.
+
+```console
+$ helm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator
+$ helm install --name zookeeper incubator/zookeeper
+```
+
+If you do not specify a name, helm will select a name for you.
+
+### Installed Components
+You can use `kubectl get` to view all of the installed components.
+
+```console{%raw}
+$ kubectl get all -l app=zookeeper
+NAME:   zookeeper
+LAST DEPLOYED: Wed Apr 11 17:09:48 2018
+NAMESPACE: default
+STATUS: DEPLOYED
+
+RESOURCES:
+==> v1beta1/PodDisruptionBudget
+NAME       MIN AVAILABLE  MAX UNAVAILABLE  ALLOWED DISRUPTIONS  AGE
+zookeeper  N/A            1                1                    2m
+
+==> v1/Service
+NAME                TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)                     AGE
+zookeeper-headless  ClusterIP  None           <none>       2181/TCP,3888/TCP,2888/TCP  2m
+zookeeper           ClusterIP  10.98.179.165  <none>       2181/TCP                    2m
+
+==> v1beta1/StatefulSet
+NAME       DESIRED  CURRENT  AGE
+zookeeper  3        3        2m
+```
+
+1. `statefulsets/zookeeper` is the StatefulSet created by the chart.
+1. `po/zookeeper-<0|1|2>` are the Pods created by the StatefulSet. Each Pod has a single container running a ZooKeeper server.
+1. `svc/zookeeper-headless` is the Headless Service used to control the network domain of the ZooKeeper ensemble.
+1. `svc/zookeeper` is a Service that can be used by clients to connect to an available ZooKeeper server.
+
+## Configuration
+You can specify each parameter using the `--set key=value[,key=value]` argument to `helm install`.
+
+Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example,
+
+```console
+$ helm install --name my-release -f values.yaml incubator/zookeeper
+```
+
+## Default Values
+
+- You can find all user-configurable settings, their defaults and commentary about them in [values.yaml](values.yaml).
+
+## Deep Dive
+
+## Image Details
+The image used for this chart is based on Ubuntu 16.04 LTS. This image is larger than Alpine or BusyBox, but it provides glibc, rather than ulibc or mucl, and a JVM release that is built against it. You can easily convert this chart to run against a smaller image with a JVM that is build against that images libc. However, as far as we know, no Hadoop vendor supports, or has verified, ZooKeeper running on such a JVM.
+
+## JVM Details
+The Java Virtual Machine used for this chart is the OpenJDK JVM 8u111 JRE (headless).
+
+## ZooKeeper Details
+The ZooKeeper version is the latest stable version (3.4.10). The distribution is installed into /opt/zookeeper-3.4.10. This directory is symbolically linked to /opt/zookeeper. Symlinks are created to simulate a rpm installation into /usr.
+
+## Failover
+You can test failover by killing the leader. Insert a key:
+```console
+$ kubectl exec zookeeper-0 -- /opt/zookeeper/bin/zkCli.sh create /foo bar;
+$ kubectl exec zookeeper-2 -- /opt/zookeeper/bin/zkCli.sh get /foo;
+```
+
+Watch existing members:
+```console
+$ kubectl run --attach bbox --image=busybox --restart=Never -- sh -c 'while true; do for i in 0 1 2; do echo zk-${i} $(echo stats | nc <pod-name>-${i}.<headless-service-name>:2181 | grep Mode); sleep 1; done; done';
+
+zk-2 Mode: follower
+zk-0 Mode: follower
+zk-1 Mode: leader
+zk-2 Mode: follower
+```
+
+Delete Pods and wait for the StatefulSet controller to bring them back up:
+```console
+$ kubectl delete po -l app=zookeeper
+$ kubectl get po --watch-only
+NAME          READY     STATUS    RESTARTS   AGE
+zookeeper-0   0/1       Running   0          35s
+zookeeper-0   1/1       Running   0         50s
+zookeeper-1   0/1       Pending   0         0s
+zookeeper-1   0/1       Pending   0         0s
+zookeeper-1   0/1       ContainerCreating   0         0s
+zookeeper-1   0/1       Running   0         19s
+zookeeper-1   1/1       Running   0         40s
+zookeeper-2   0/1       Pending   0         0s
+zookeeper-2   0/1       Pending   0         0s
+zookeeper-2   0/1       ContainerCreating   0         0s
+zookeeper-2   0/1       Running   0         19s
+zookeeper-2   1/1       Running   0         41s
+```
+
+Check the previously inserted key:
+```console
+$ kubectl exec zookeeper-1 -- /opt/zookeeper/bin/zkCli.sh get /foo
+ionid = 0x354887858e80035, negotiated timeout = 30000
+
+WATCHER::
+
+WatchedEvent state:SyncConnected type:None path:null
+bar
+```
+
+## Scaling
+ZooKeeper can not be safely scaled in versions prior to 3.5.x. This chart currently uses 3.4.x. There are manual procedures for scaling a 3.4.x ensemble, but as noted in the [ZooKeeper 3.5.2 documentation](https://zookeeper.apache.org/doc/r3.5.2-alpha/zookeeperReconfig.html) these procedures require a rolling restart, are known to be error prone, and often result in a data loss.
+
+While ZooKeeper 3.5.x does allow for dynamic ensemble reconfiguration (including scaling membership), the current status of the release is still alpha, and 3.5.x is therefore not recommended for production use.
+
+## Limitations
+* StatefulSet and PodDisruptionBudget are beta resources.
+* Only supports storage options that have backends for persistent volume claims.
diff --git a/challenge-devops/helm/patroni/charts/zookeeper/templates/NOTES.txt b/challenge-devops/helm/patroni/charts/zookeeper/templates/NOTES.txt
new file mode 100755
index 0000000..6c5da85
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/zookeeper/templates/NOTES.txt
@@ -0,0 +1,7 @@
+Thank you for installing ZooKeeper on your Kubernetes cluster. More information
+about ZooKeeper can be found at https://zookeeper.apache.org/doc/current/
+
+Your connection string should look like:
+  {{ template "zookeeper.fullname" . }}-0.{{ template "zookeeper.fullname" . }}-headless:{{ .Values.service.ports.client.port }},{{ template "zookeeper.fullname" . }}-1.{{ template "zookeeper.fullname" . }}-headless:{{ .Values.service.ports.client.port }},...
+
+You can also use the client service {{ template "zookeeper.fullname" . }}:{{ .Values.service.ports.client.port }} to connect to an available ZooKeeper server.
diff --git a/challenge-devops/helm/patroni/charts/zookeeper/templates/_helpers.tpl b/challenge-devops/helm/patroni/charts/zookeeper/templates/_helpers.tpl
new file mode 100755
index 0000000..ae36115
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/zookeeper/templates/_helpers.tpl
@@ -0,0 +1,32 @@
+{{/* vim: set filetype=mustache: */}}
+{{/*
+Expand the name of the chart.
+*/}}
+{{- define "zookeeper.name" -}}
+{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" -}}
+{{- end -}}
+
+{{/*
+Create a default fully qualified app name.
+We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).
+If release name contains chart name it will be used as a full name.
+*/}}
+{{- define "zookeeper.fullname" -}}
+{{- if .Values.fullnameOverride -}}
+{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" -}}
+{{- else -}}
+{{- $name := default .Chart.Name .Values.nameOverride -}}
+{{- if contains $name .Release.Name -}}
+{{- .Release.Name | trunc 63 | trimSuffix "-" -}}
+{{- else -}}
+{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" -}}
+{{- end -}}
+{{- end -}}
+{{- end -}}
+
+{{/*
+Create chart name and version as used by the chart label.
+*/}}
+{{- define "zookeeper.chart" -}}
+{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" -}}
+{{- end -}}
diff --git a/challenge-devops/helm/patroni/charts/zookeeper/templates/config-jmx-exporter.yaml b/challenge-devops/helm/patroni/charts/zookeeper/templates/config-jmx-exporter.yaml
new file mode 100755
index 0000000..79905e5
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/zookeeper/templates/config-jmx-exporter.yaml
@@ -0,0 +1,19 @@
+{{- if .Values.exporters.jmx.enabled }}
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: {{ .Release.Name }}-jmx-exporter
+  labels:
+    app: {{ template "zookeeper.name" . }}
+    chart: {{ template "zookeeper.chart" . }}
+    release: {{ .Release.Name }}
+    heritage: {{ .Release.Service }}
+data:
+  config.yml: |-
+    hostPort: 127.0.0.1:{{ .Values.env.JMXPORT }}
+    lowercaseOutputName: {{ .Values.exporters.jmx.config.lowercaseOutputName }}
+    rules:
+{{ .Values.exporters.jmx.config.rules | toYaml | indent 6 }}
+    ssl: false
+    startDelaySeconds: {{ .Values.exporters.jmx.config.startDelaySeconds }}
+{{- end }}
diff --git a/challenge-devops/helm/patroni/charts/zookeeper/templates/job-chroots.yaml b/challenge-devops/helm/patroni/charts/zookeeper/templates/job-chroots.yaml
new file mode 100755
index 0000000..6663ddb
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/zookeeper/templates/job-chroots.yaml
@@ -0,0 +1,62 @@
+{{- if .Values.jobs.chroots.enabled }}
+{{- $root := . }}
+{{- $job := .Values.jobs.chroots }}
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: {{ template "zookeeper.fullname" . }}-chroots
+  annotations:
+    "helm.sh/hook": post-install,post-upgrade
+    "helm.sh/hook-weight": "-5"
+    "helm.sh/hook-delete-policy": hook-succeeded
+  labels:
+    app: {{ template "zookeeper.name" . }}
+    chart: {{ template "zookeeper.chart" . }}
+    release: {{ .Release.Name }}
+    heritage: {{ .Release.Service }}
+    component: jobs
+    job: chroots
+spec:
+  activeDeadlineSeconds: {{ $job.activeDeadlineSeconds }}
+  backoffLimit: {{ $job.backoffLimit }}
+  completions: {{ $job.completions }}
+  parallelism: {{ $job.parallelism }}
+  template:
+    metadata:
+      labels:
+        app: {{ template "zookeeper.name" . }}
+        release: {{ .Release.Name }}
+        component: jobs
+        job: chroots
+    spec:
+      restartPolicy: {{ $job.restartPolicy }}
+      containers:
+        - name: main
+          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
+          imagePullPolicy: {{ .Values.image.pullPolicy }}
+          command:
+            - /bin/bash
+            - -o
+            - pipefail
+            - -euc
+  {{- $port := .Values.service.ports.client.port }}
+            - >
+              sleep 15;
+              export SERVER={{ template "zookeeper.fullname" $root }}:{{ $port }};
+  {{- range $job.config.create }}
+              echo '==> {{ . }}';
+              echo '====> Create chroot if does not exist.';
+              zkCli.sh -server {{ template "zookeeper.fullname" $root }}:{{ $port }} get {{ . }} 2>&1 >/dev/null | grep 'cZxid'
+              || zkCli.sh -server {{ template "zookeeper.fullname" $root }}:{{ $port }} create {{ . }} "";
+              echo '====> Confirm chroot exists.';
+              zkCli.sh -server {{ template "zookeeper.fullname" $root }}:{{ $port }} get {{ . }} 2>&1 >/dev/null | grep 'cZxid';
+              echo '====> Chroot exists.';
+  {{- end }}
+          env:
+          {{- range $key, $value := $job.env }}
+            - name: {{ $key | upper | replace "." "_" }}
+              value: {{ $value | quote }}
+          {{- end }}
+          resources:
+{{ toYaml $job.resources | indent 12 }}
+{{- end -}}
diff --git a/challenge-devops/helm/patroni/charts/zookeeper/templates/poddisruptionbudget.yaml b/challenge-devops/helm/patroni/charts/zookeeper/templates/poddisruptionbudget.yaml
new file mode 100755
index 0000000..15ee008
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/zookeeper/templates/poddisruptionbudget.yaml
@@ -0,0 +1,17 @@
+apiVersion: policy/v1beta1
+kind: PodDisruptionBudget
+metadata:
+  name: {{ template "zookeeper.fullname" . }}
+  labels:
+    app: {{ template "zookeeper.name" . }}
+    chart: {{ template "zookeeper.chart" . }}
+    release: {{ .Release.Name }}
+    heritage: {{ .Release.Service }}
+    component: server
+spec:
+  selector:
+    matchLabels:
+      app: {{ template "zookeeper.name" . }}
+      release: {{ .Release.Name }}
+      component: server
+{{ toYaml .Values.podDisruptionBudget | indent 2 }}
diff --git a/challenge-devops/helm/patroni/charts/zookeeper/templates/service-headless.yaml b/challenge-devops/helm/patroni/charts/zookeeper/templates/service-headless.yaml
new file mode 100755
index 0000000..8822867
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/zookeeper/templates/service-headless.yaml
@@ -0,0 +1,21 @@
+apiVersion: v1
+kind: Service
+metadata:
+  name: {{ template "zookeeper.fullname" . }}-headless
+  labels:
+    app: {{ template "zookeeper.name" . }}
+    chart: {{ template "zookeeper.chart" . }}
+    release: {{ .Release.Name }}
+    heritage: {{ .Release.Service }}
+spec:
+  clusterIP: None
+  ports:
+{{- range $key, $port := .Values.ports }}
+    - name: {{ $key }}
+      port: {{ $port.containerPort }}
+      targetPort: {{ $port.name }}
+      protocol: {{ $port.protocol }}
+{{- end }}
+  selector:
+    app: {{ template "zookeeper.name" . }}
+    release: {{ .Release.Name }}
diff --git a/challenge-devops/helm/patroni/charts/zookeeper/templates/service.yaml b/challenge-devops/helm/patroni/charts/zookeeper/templates/service.yaml
new file mode 100755
index 0000000..5f10861
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/zookeeper/templates/service.yaml
@@ -0,0 +1,23 @@
+apiVersion: v1
+kind: Service
+metadata:
+  name: {{ template "zookeeper.fullname" . }}
+  labels:
+    app: {{ template "zookeeper.name" . }}
+    chart: {{ template "zookeeper.chart" . }}
+    release: {{ .Release.Name }}
+    heritage: {{ .Release.Service }}
+  annotations:
+{{- with .Values.service.annotations }}
+{{ toYaml . | indent 4 }}
+{{- end }}
+spec:
+  type: {{ .Values.service.type }}
+  ports:
+  {{- range $key, $value := .Values.service.ports }}
+    - name: {{ $key }}
+{{ toYaml $value | indent 6 }}
+  {{- end }}
+  selector:
+    app: {{ template "zookeeper.name" . }}
+    release: {{ .Release.Name }}
diff --git a/challenge-devops/helm/patroni/charts/zookeeper/templates/statefulset.yaml b/challenge-devops/helm/patroni/charts/zookeeper/templates/statefulset.yaml
new file mode 100755
index 0000000..057a454
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/zookeeper/templates/statefulset.yaml
@@ -0,0 +1,183 @@
+apiVersion: apps/v1beta1
+kind: StatefulSet
+metadata:
+  name: {{ template "zookeeper.fullname" . }}
+  labels:
+    app: {{ template "zookeeper.name" . }}
+    chart: {{ template "zookeeper.chart" . }}
+    release: {{ .Release.Name }}
+    heritage: {{ .Release.Service }}
+    component: server
+spec:
+  serviceName: {{ template "zookeeper.fullname" . }}-headless
+  replicas: {{ .Values.replicaCount }}
+  terminationGracePeriodSeconds: {{ .Values.terminationGracePeriodSeconds }}
+  selector:
+    matchLabels:
+      app: {{ template "zookeeper.name" . }}
+      release: {{ .Release.Name }}
+      component: server
+  updateStrategy:
+{{ toYaml .Values.updateStrategy | indent 4 }}
+  template:
+    metadata:
+      labels:
+        app: {{ template "zookeeper.name" . }}
+        release: {{ .Release.Name }}
+        component: server
+      {{- if .Values.podLabels }}
+        ## Custom pod labels
+        {{- range $key, $value := .Values.podLabels }}
+        {{ $key }}: {{ $value | quote }}
+        {{- end }}
+      {{- end }}
+      annotations:
+      {{- if .Values.podAnnotations }}
+        ## Custom pod annotations
+        {{- range $key, $value := .Values.podAnnotations }}
+        {{ $key }}: {{ $value | quote }}
+        {{- end }}
+      {{- end }}
+    spec:
+{{- if .Values.schedulerName }}
+      schedulerName: "{{ .Values.schedulerName }}"
+{{- end }}
+      securityContext:
+{{ toYaml .Values.securityContext | indent 8 }}
+      containers:
+
+        - name: zookeeper
+          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
+          imagePullPolicy: {{ .Values.image.pullPolicy }}
+          command:
+            - /bin/bash
+            - -xec
+            - zkGenConfig.sh && exec zkServer.sh start-foreground
+          ports:
+{{- range $key, $port := .Values.ports }}
+            - name: {{ $key }}
+{{ toYaml $port | indent 14 }}
+{{- end }}
+          livenessProbe:
+{{ toYaml .Values.livenessProbe | indent 12 }}
+          readinessProbe:
+{{ toYaml .Values.readinessProbe | indent 12 }}
+          env:
+            - name: ZK_REPLICAS
+              value: {{ .Values.replicaCount | quote }}
+          {{- range $key, $value := .Values.env }}
+            - name: {{ $key | upper | replace "." "_" }}
+              value: {{ $value | quote }}
+          {{- end }}
+          resources:
+{{ toYaml .Values.resources | indent 12 }}
+          volumeMounts:
+            - name: data
+              mountPath: /var/lib/zookeeper
+
+{{- if .Values.exporters.jmx.enabled }}
+        - name: jmx-exporter
+          image: "{{ .Values.exporters.jmx.image.repository }}:{{ .Values.exporters.jmx.image.tag }}"
+          imagePullPolicy: {{ .Values.exporters.jmx.image.pullPolicy }}
+          ports:
+  {{- range $key, $port := .Values.exporters.jmx.ports }}
+            - name: {{ $key }}
+{{ toYaml $port | indent 14 }}
+  {{- end }}
+          livenessProbe:
+{{ toYaml .Values.exporters.jmx.livenessProbe | indent 12 }}
+          readinessProbe:
+{{ toYaml .Values.exporters.jmx.readinessProbe | indent 12 }}
+          env:
+            - name: SERVICE_PORT
+              value: {{ .Values.exporters.jmx.ports.jmxxp.containerPort | quote }}
+          {{- with .Values.exporters.jmx.env }}
+            {{- range $key, $value := . }}
+            - name: {{ $key | upper | replace "." "_" }}
+              value: {{ $value | quote }}
+            {{- end }}
+          {{- end }}
+          resources:
+{{ toYaml .Values.exporters.jmx.resources | indent 12 }}
+          volumeMounts:
+            - name: config-jmx-exporter
+              mountPath: /opt/jmx_exporter/config.yml
+              subPath: config.yml
+{{- end }}
+
+{{- if .Values.exporters.zookeeper.enabled }}
+        - name: zookeeper-exporter
+          image: "{{ .Values.exporters.zookeeper.image.repository }}:{{ .Values.exporters.zookeeper.image.tag }}"
+          imagePullPolicy: {{ .Values.exporters.zookeeper.image.pullPolicy }}
+          args:
+            - -bind-addr=:{{ .Values.exporters.zookeeper.ports.zookeeperxp.containerPort }}
+            - -metrics-path={{ .Values.exporters.zookeeper.path }}
+            - -zookeeper=localhost:{{ .Values.ports.client.containerPort }}
+            - -log-level={{ .Values.exporters.zookeeper.config.logLevel }}
+            - -reset-on-scrape={{ .Values.exporters.zookeeper.config.resetOnScrape }}
+          ports:
+  {{- range $key, $port := .Values.exporters.zookeeper.ports }}
+            - name: {{ $key }}
+{{ toYaml $port | indent 14 }}
+  {{- end }}
+          livenessProbe:
+{{ toYaml .Values.exporters.zookeeper.livenessProbe | indent 12 }}
+          readinessProbe:
+{{ toYaml .Values.exporters.zookeeper.readinessProbe | indent 12 }}
+          env:
+          {{- range $key, $value := .Values.exporters.zookeeper.env }}
+            - name: {{ $key | upper | replace "." "_" }}
+              value: {{ $value | quote }}
+          {{- end }}
+          resources:
+{{ toYaml .Values.exporters.zookeeper.resources | indent 12 }}
+{{- end }}
+
+    {{- with .Values.nodeSelector }}
+      nodeSelector:
+{{ toYaml . | indent 8 }}
+    {{- end }}
+    {{- with .Values.affinity }}
+      affinity:
+{{ toYaml . | indent 8 }}
+    {{- end }}
+    {{- with .Values.tolerations }}
+      tolerations:
+{{ toYaml . | indent 8 }}
+    {{- end }}
+{{- if .Values.exporters.jmx.enabled }}
+      volumes:
+        - name: config-jmx-exporter
+          configMap:
+            name: {{ .Release.Name }}-jmx-exporter
+{{- end }}
+
+    {{- with .Values.nodeSelector }}
+      nodeSelector:
+{{ toYaml . | indent 8 }}
+    {{- end }}
+    {{- with .Values.affinity }}
+      affinity:
+{{ toYaml . | indent 8 }}
+    {{- end }}
+    {{- with .Values.tolerations }}
+      tolerations:
+{{ toYaml . | indent 8 }}
+    {{- end }}
+
+  volumeClaimTemplates:
+    - metadata:
+        name: data
+      spec:
+        accessModes:
+          - {{ .Values.persistence.accessMode | quote }}
+        resources:
+          requests:
+            storage: {{ .Values.persistence.size | quote }}
+      {{- if .Values.persistence.storageClass }}
+        {{- if (eq "-" .Values.persistence.storageClass) }}
+        storageClassName: ""
+        {{- else }}
+        storageClassName: "{{ .Values.persistence.storageClass }}"
+        {{- end }}
+      {{- end }}
diff --git a/challenge-devops/helm/patroni/charts/zookeeper/values.yaml b/challenge-devops/helm/patroni/charts/zookeeper/values.yaml
new file mode 100755
index 0000000..35f5e98
--- /dev/null
+++ b/challenge-devops/helm/patroni/charts/zookeeper/values.yaml
@@ -0,0 +1,294 @@
+## As weighted quorums are not supported, it is imperative that an odd number of replicas
+## be chosen. Moreover, the number of replicas should be either 1, 3, 5, or 7.
+##
+## ref: https://github.com/kubernetes/contrib/tree/master/statefulsets/zookeeper#stateful-set
+replicaCount: 3  # Desired quantity of ZooKeeper pods. This should always be (1,3,5, or 7)
+
+podDisruptionBudget:
+  maxUnavailable: 1  # Limits how many Zokeeper pods may be unavailable due to voluntary disruptions.
+
+terminationGracePeriodSeconds: 1800  # Duration in seconds a Zokeeper pod needs to terminate gracefully.
+
+## OnDelete requires you to manually delete each pod when making updates.
+## This approach is at the moment safer than RollingUpdate because replication
+## may be incomplete when replication source pod is killed.
+##
+## ref: http://blog.kubernetes.io/2017/09/kubernetes-statefulsets-daemonsets.html
+updateStrategy:
+  type: OnDelete  # Pods will only be created when you manually delete old pods.
+
+## refs:
+## - https://github.com/kubernetes/contrib/tree/master/statefulsets/zookeeper
+## - https://github.com/kubernetes/contrib/blob/master/statefulsets/zookeeper/Makefile#L1
+image:
+  repository: gcr.io/google_samples/k8szk  # Container image repository for zookeeper container.
+  tag: v3  # Container image tag for zookeeper container.
+  pullPolicy: IfNotPresent  # Image pull criteria for zookeeper container.
+
+service:
+  type: ClusterIP  # Exposes zookeeper on a cluster-internal IP.
+  annotations: {}  # Arbitrary non-identifying metadata for zookeeper service.
+    ## AWS example for use with LoadBalancer service type.
+    # external-dns.alpha.kubernetes.io/hostname: zookeeper.cluster.local
+    # service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
+    # service.beta.kubernetes.io/aws-load-balancer-internal: "true"
+  ports:
+    client:
+      port: 2181  # Service port number for client port.
+      targetPort: client  # Service target port for client port.
+      protocol: TCP  # Service port protocol for client port.
+
+
+ports:
+  client:
+    containerPort: 2181  # Port number for zookeeper container client port.
+    protocol: TCP  # Protocol for zookeeper container client port.
+  election:
+    containerPort: 3888  # Port number for zookeeper container election port.
+    protocol: TCP  # Protocol for zookeeper container election port.
+  server:
+    containerPort: 2888  # Port number for zookeeper container server port.
+    protocol: TCP  # Protocol for zookeeper container server port.
+
+resources: {}  # Optionally specify how much CPU and memory (RAM) each zookeeper container needs.
+  # We usually recommend not to specify default resources and to leave this as a conscious
+  # choice for the user. This also increases chances charts run on environments with little
+  # resources, such as Minikube. If you do want to specify resources, uncomment the following
+  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
+  # limits:
+  #  cpu: 100m
+  #  memory: 128Mi
+  # requests:
+  #  cpu: 100m
+  #  memory: 128Mi
+
+nodeSelector: {}  # Node label-values required to run zookeeper pods.
+
+tolerations: []  # Node taint overrides for zookeeper pods.
+
+affinity: {}  # Criteria by which pod label-values influence scheduling for zookeeper pods.
+  # podAntiAffinity:
+  #   requiredDuringSchedulingIgnoredDuringExecution:
+  #     - topologyKey: "kubernetes.io/hostname"
+  #       labelSelector:
+  #         matchLabels:
+  #           release: zookeeper
+
+podAnnotations: {}  # Arbitrary non-identifying metadata for zookeeper pods.
+  # prometheus.io/scrape: "true"
+  # prometheus.io/path: "/metrics"
+  # prometheus.io/port: "9141"
+
+podLabels: {}  # Key/value pairs that are attached to zookeeper pods.
+  # team: "developers"
+  # service: "zookeeper"
+
+livenessProbe:
+  exec:
+    command:
+      - zkOk.sh
+  initialDelaySeconds: 20
+  # periodSeconds: 30
+  # timeoutSeconds: 30
+  # failureThreshold: 6
+  # successThreshold: 1
+
+readinessProbe:
+  exec:
+    command:
+      - zkOk.sh
+  initialDelaySeconds: 20
+  # periodSeconds: 30
+  # timeoutSeconds: 30
+  # failureThreshold: 6
+  # successThreshold: 1
+
+securityContext:
+  fsGroup: 1000
+  runAsUser: 1000
+
+persistence:
+  enabled: true
+  ## zookeeper data Persistent Volume Storage Class
+  ## If defined, storageClassName: <storageClass>
+  ## If set to "-", storageClassName: "", which disables dynamic provisioning
+  ## If undefined (the default) or set to null, no storageClassName spec is
+  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
+  ##   GKE, AWS & OpenStack)
+  ##
+  # storageClass: "-"
+  accessMode: ReadWriteOnce
+  size: 5Gi
+
+## Exporters query apps for metrics and make those metrics available for
+## Prometheus to scrape.
+exporters:
+
+  jmx:
+    enabled: false
+    image:
+      repository: sscaling/jmx-prometheus-exporter
+      tag: 0.3.0
+      pullPolicy: IfNotPresent
+    config:
+      lowercaseOutputName: false
+      ## ref: https://github.com/prometheus/jmx_exporter/blob/master/example_configs/zookeeper.yaml
+      rules:
+        - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d+)><>(\\w+)"
+          name: "zookeeper_$2"
+        - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d+), name1=replica.(\\d+)><>(\\w+)"
+          name: "zookeeper_$3"
+          labels:
+            replicaId: "$2"
+        - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d+), name1=replica.(\\d+), name2=(\\w+)><>(\\w+)"
+          name: "zookeeper_$4"
+          labels:
+            replicaId: "$2"
+            memberType: "$3"
+        - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d+), name1=replica.(\\d+), name2=(\\w+), name3=(\\w+)><>(\\w+)"
+          name: "zookeeper_$4_$5"
+          labels:
+            replicaId: "$2"
+            memberType: "$3"
+      startDelaySeconds: 30
+    env: {}
+    resources: {}
+    path: /metrics
+    ports:
+      jmxxp:
+        containerPort: 9404
+        protocol: TCP
+    livenessProbe:
+      httpGet:
+        path: /metrics
+        port: jmxxp
+      initialDelaySeconds: 30
+      periodSeconds: 15
+      timeoutSeconds: 60
+      failureThreshold: 8
+      successThreshold: 1
+    readinessProbe:
+      httpGet:
+        path: /metrics
+        port: jmxxp
+      initialDelaySeconds: 30
+      periodSeconds: 15
+      timeoutSeconds: 60
+      failureThreshold: 8
+      successThreshold: 1
+
+  zookeeper:
+  ## refs:
+  ## - https://github.com/carlpett/zookeeper_exporter
+  ## - https://hub.docker.com/r/josdotso/zookeeper-exporter/
+  ## - https://www.datadoghq.com/blog/monitoring-kafka-performance-metrics/#zookeeper-metrics
+    enabled: false
+    image:
+      repository: josdotso/zookeeper-exporter
+      tag: v1.1.2
+      pullPolicy: IfNotPresent
+    config:
+      logLevel: info
+      resetOnScrape: "true"
+    env: {}
+    resources: {}
+    path: /metrics
+    ports:
+      zookeeperxp:
+        containerPort: 9141
+        protocol: TCP
+    livenessProbe:
+      httpGet:
+        path: /metrics
+        port: zookeeperxp
+      initialDelaySeconds: 30
+      periodSeconds: 15
+      timeoutSeconds: 60
+      failureThreshold: 8
+      successThreshold: 1
+    readinessProbe:
+      httpGet:
+        path: /metrics
+        port: zookeeperxp
+      initialDelaySeconds: 30
+      periodSeconds: 15
+      timeoutSeconds: 60
+      failureThreshold: 8
+      successThreshold: 1
+
+## Use an alternate scheduler, e.g. "stork".
+## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
+##
+# schedulerName:
+
+## ref: https://github.com/kubernetes/contrib/tree/master/statefulsets/zookeeper
+env:
+
+  ## Options related to JMX exporter.
+  ## ref: https://github.com/apache/zookeeper/blob/master/bin/zkServer.sh#L36
+  JMXAUTH: "false"
+  JMXDISABLE: "false"
+  JMXPORT: 1099
+  JMXSSL: "false"
+
+  ## The port on which the server will accept client requests.
+  ZK_CLIENT_PORT: 2181
+
+  ## The port on which the ensemble performs leader election.
+  ZK_ELECTION_PORT: 3888
+
+  ## The JVM heap size.
+  ZK_HEAP_SIZE: 2G
+
+  ## The number of Ticks that an ensemble member is allowed to perform leader
+  ## election.
+  ZK_INIT_LIMIT: 5
+
+  ## The Log Level that for the ZooKeeper processes logger.
+  ## Choices are `TRACE,DEBUG,INFO,WARN,ERROR,FATAL`.
+  ZK_LOG_LEVEL: INFO
+
+  ## The maximum number of concurrent client connections that
+  ## a server in the ensemble will accept.
+  ZK_MAX_CLIENT_CNXNS: 60
+
+  ## The maximum session timeout that the ensemble will allow a client to request.
+  ## Upstream default is `20 * ZK_TICK_TIME`
+  ZK_MAX_SESSION_TIMEOUT: 40000
+
+  ## The minimum session timeout that the ensemble will allow a client to request.
+  ## Upstream default is `2 * ZK_TICK_TIME`.
+  ZK_MIN_SESSION_TIMEOUT: 4000
+
+  ## The delay, in hours, between ZooKeeper log and snapshot cleanups.
+  ZK_PURGE_INTERVAL: 0
+
+  ## The port on which the leader will send events to followers.
+  ZK_SERVER_PORT: 2888
+
+  ## The number of snapshots that the ZooKeeper process will retain if
+  ## `ZK_PURGE_INTERVAL` is set to a value greater than `0`.
+  ZK_SNAP_RETAIN_COUNT: 3
+
+  ## The number of Tick by which a follower may lag behind the ensembles leader.
+  ZK_SYNC_LIMIT: 10
+
+  ## The number of wall clock ms that corresponds to a Tick for the ensembles
+  ## internal time.
+  ZK_TICK_TIME: 2000
+
+jobs:
+  ## ref: http://zookeeper.apache.org/doc/r3.4.10/zookeeperProgrammers.html#ch_zkSessions
+  chroots:
+    enabled: false
+    activeDeadlineSeconds: 300
+    backoffLimit: 5
+    completions: 1
+    config:
+      create: []
+        # - /kafka
+        # - /ureplicator
+    env: []
+    parallelism: 1
+    resources: {}
+    restartPolicy: Never
diff --git a/challenge-devops/helm/patroni/requirements.lock b/challenge-devops/helm/patroni/requirements.lock
new file mode 100755
index 0000000..4522797
--- /dev/null
+++ b/challenge-devops/helm/patroni/requirements.lock
@@ -0,0 +1,12 @@
+dependencies:
+- name: etcd
+  repository: https://charts.helm.sh/incubator
+  version: 0.6.2
+- name: zookeeper
+  repository: https://charts.helm.sh/incubator
+  version: 1.0.0
+- name: consul
+  repository: https://charts.helm.sh/stable
+  version: 3.6.1
+digest: sha256:1d1ed086586703e7cdc528c6d44e5c03f68f3f4fddfc713e50898eff18dc5acf
+generated: "2020-10-30T00:42:58.035153-04:00"
diff --git a/challenge-devops/helm/patroni/requirements.yaml b/challenge-devops/helm/patroni/requirements.yaml
new file mode 100755
index 0000000..1cc75ec
--- /dev/null
+++ b/challenge-devops/helm/patroni/requirements.yaml
@@ -0,0 +1,13 @@
+dependencies:
+- name: etcd
+  version: 0.6.2
+  repository: https://charts.helm.sh/incubator
+  condition: etcd.deployChart
+- name: zookeeper
+  version: 1.0.0
+  repository: https://charts.helm.sh/incubator
+  condition: zookeeper.deployChart
+- name: consul
+  version: 3.6.1
+  repository: https://charts.helm.sh/stable
+  condition: consul.deployChart
diff --git a/challenge-devops/helm/patroni/templates/NOTES.txt b/challenge-devops/helm/patroni/templates/NOTES.txt
new file mode 100755
index 0000000..22a4f2d
--- /dev/null
+++ b/challenge-devops/helm/patroni/templates/NOTES.txt
@@ -0,0 +1,25 @@
+Patroni can be accessed via port 5432 on the following DNS name from within your cluster:
+{{ template "patroni.fullname" . }}.{{ .Release.Namespace }}.svc.cluster.local
+
+To get your password for superuser run:
+
+    # superuser password
+    PGPASSWORD_SUPERUSER=$(kubectl get secret --namespace {{ .Release.Namespace }} {{ template "patroni.fullname" . }} -o jsonpath="{.data.password-superuser}" | base64 --decode)
+
+    # admin password
+    PGPASSWORD_ADMIN=$(kubectl get secret --namespace {{ .Release.Namespace }} {{ template "patroni.fullname" . }} -o jsonpath="{.data.password-admin}" | base64 --decode)
+
+To connect to your database:
+
+1. Run a postgres pod and connect using the psql cli:
+    # login as superuser
+    kubectl run -i --tty --rm psql --image=postgres \
+      --env "PGPASSWORD=$PGPASSWORD_SUPERUSER" \
+      --command -- psql -U postgres \
+      -h {{ template "patroni.fullname" . }}.{{ .Release.Namespace }}.svc.cluster.local postgres
+
+    # login as admin
+    kubectl run -i -tty --rm psql --image=postgres \
+      --env "PGPASSWORD=$PGPASSWORD_ADMIN" \
+      --command -- psql -U admin \
+      -h {{ template "patroni.fullname" . }}.{{ .Release.Namespace }}.svc.cluster.local postgres
diff --git a/challenge-devops/helm/patroni/templates/_helpers.tpl b/challenge-devops/helm/patroni/templates/_helpers.tpl
new file mode 100755
index 0000000..2b1b6d4
--- /dev/null
+++ b/challenge-devops/helm/patroni/templates/_helpers.tpl
@@ -0,0 +1,43 @@
+{{/* vim: set filetype=mustache: */}}
+{{/*
+Expand the name of the chart.
+*/}}
+{{- define "patroni.name" -}}
+{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" -}}
+{{- end -}}
+
+{{/*
+Create a default fully qualified app name.
+We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).
+If release name contains chart name it will be used as a full name.
+*/}}
+{{- define "patroni.fullname" -}}
+{{- if .Values.fullnameOverride -}}
+{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" -}}
+{{- else -}}
+{{- $name := default .Chart.Name .Values.nameOverride -}}
+{{- if contains $name .Release.Name -}}
+{{- .Release.Name | trunc 63 | trimSuffix "-" -}}
+{{- else -}}
+{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" -}}
+{{- end -}}
+{{- end -}}
+{{- end -}}
+
+{{/*
+Create chart name and version as used by the chart label.
+*/}}
+{{- define "patroni.chart" -}}
+{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" -}}
+{{- end -}}
+
+{{/*
+Create the name of the service account to use.
+*/}}
+{{- define "patroni.serviceAccountName" -}}
+{{- if .Values.serviceAccount.create -}}
+    {{ default (include "patroni.fullname" .) .Values.serviceAccount.name }}
+{{- else -}}
+    {{ default "default" .Values.serviceAccount.name }}
+{{- end -}}
+{{- end -}}
diff --git a/challenge-devops/helm/patroni/templates/ep-patroni.yaml b/challenge-devops/helm/patroni/templates/ep-patroni.yaml
new file mode 100755
index 0000000..a218f53
--- /dev/null
+++ b/challenge-devops/helm/patroni/templates/ep-patroni.yaml
@@ -0,0 +1,10 @@
+apiVersion: v1
+kind: Endpoints
+metadata:
+  name: {{ template "patroni.fullname" . }}
+  labels:
+    app: {{ template "patroni.fullname" . }}
+    chart: {{ template "patroni.chart" . }}
+    release: {{ .Release.Name }}
+    heritage: {{ .Release.Service }}
+subsets: []
diff --git a/challenge-devops/helm/patroni/templates/role-patroni.yaml b/challenge-devops/helm/patroni/templates/role-patroni.yaml
new file mode 100755
index 0000000..1402171
--- /dev/null
+++ b/challenge-devops/helm/patroni/templates/role-patroni.yaml
@@ -0,0 +1,48 @@
+{{- if .Values.rbac.create }}
+apiVersion: rbac.authorization.k8s.io/v1beta1
+kind: Role
+metadata:
+  name: {{ template "patroni.fullname" . }}
+  labels:
+    app: {{ template "patroni.fullname" . }}
+    chart: {{ template "patroni.chart" . }}
+    release: {{ .Release.Name }}
+    heritage: {{ .Release.Service }}
+rules:
+- apiGroups: [""]
+  resources: ["configmaps"]
+  verbs:
+  - create
+  - get
+  - list
+  - patch
+  - update
+  - watch
+  # delete is required only for 'patronictl remove'
+  - delete
+- apiGroups: [""]
+  resources: ["services"]
+  verbs:
+  - create
+- apiGroups: [""]
+  resources: ["endpoints"]
+  verbs:
+  - create
+  - get
+  - patch
+  - update
+  # the following three privileges are necessary only when using endpoints
+  - list
+  - watch
+  # delete is required only for for 'patronictl remove'
+  - delete
+  - deletecollection
+- apiGroups: [""]
+  resources: ["pods"]
+  verbs:
+  - get
+  - list
+  - patch
+  - update
+  - watch
+{{- end }}
diff --git a/challenge-devops/helm/patroni/templates/rolebinding-patroni.yaml b/challenge-devops/helm/patroni/templates/rolebinding-patroni.yaml
new file mode 100755
index 0000000..163fa00
--- /dev/null
+++ b/challenge-devops/helm/patroni/templates/rolebinding-patroni.yaml
@@ -0,0 +1,18 @@
+{{- if .Values.rbac.create }}
+apiVersion: rbac.authorization.k8s.io/v1beta1
+kind: RoleBinding
+metadata:
+  name: {{ template "patroni.fullname" . }}
+  labels:
+    app: {{ template "patroni.fullname" . }}
+    chart: {{ template "patroni.chart" . }}
+    release: {{ .Release.Name }}
+    heritage: {{ .Release.Service }}
+subjects:
+  - kind: ServiceAccount
+    name: {{ template "patroni.serviceAccountName" . }}
+roleRef:
+  apiGroup: rbac.authorization.k8s.io
+  kind: Role
+  name: {{ template "patroni.fullname" . }}
+{{- end }}
diff --git a/challenge-devops/helm/patroni/templates/sec-patroni.yaml b/challenge-devops/helm/patroni/templates/sec-patroni.yaml
new file mode 100755
index 0000000..63950ab
--- /dev/null
+++ b/challenge-devops/helm/patroni/templates/sec-patroni.yaml
@@ -0,0 +1,14 @@
+apiVersion: v1
+kind: Secret
+metadata:
+  name: {{ template "patroni.fullname" . }}
+  labels:
+    app: {{ template "patroni.fullname" . }}
+    chart: {{ template "patroni.chart" . }}
+    release: {{ .Release.Name }}
+    heritage: {{ .Release.Service }}
+type: Opaque
+data:
+  password-superuser: {{ .Values.credentials.superuser | b64enc }}
+  password-admin: {{ .Values.credentials.admin | b64enc }}
+  password-standby: {{ .Values.credentials.standby | b64enc }}
diff --git a/challenge-devops/helm/patroni/templates/serviceaccount-patroni.yaml b/challenge-devops/helm/patroni/templates/serviceaccount-patroni.yaml
new file mode 100755
index 0000000..e88f6c6
--- /dev/null
+++ b/challenge-devops/helm/patroni/templates/serviceaccount-patroni.yaml
@@ -0,0 +1,11 @@
+{{- if .Values.serviceAccount.create }}
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: {{ template "patroni.serviceAccountName" . }}
+  labels:
+    app: {{ template "patroni.fullname" . }}
+    chart: {{ template "patroni.chart" . }}
+    release: {{ .Release.Name }}
+    heritage: {{ .Release.Service }}
+{{- end }}
diff --git a/challenge-devops/helm/patroni/templates/statefulset-patroni.yaml b/challenge-devops/helm/patroni/templates/statefulset-patroni.yaml
new file mode 100755
index 0000000..8018fe2
--- /dev/null
+++ b/challenge-devops/helm/patroni/templates/statefulset-patroni.yaml
@@ -0,0 +1,208 @@
+apiVersion: apps/v1
+kind: StatefulSet
+metadata:
+  name: {{ template "patroni.fullname" . }}
+  labels:
+    app: {{ template "patroni.fullname" . }}
+    chart: {{ template "patroni.chart" . }}
+    release: {{ .Release.Name }}
+    heritage: {{ .Release.Service }}
+spec:
+  serviceName: {{ template "patroni.fullname" . }}
+  replicas: {{ .Values.replicaCount }}
+  selector:
+    matchLabels:
+      app: {{ template "patroni.fullname" . }}
+      release: {{ .Release.Name }}
+  template:
+    metadata:
+      name: {{ template "patroni.fullname" . }}
+      labels:
+        app: {{ template "patroni.fullname" . }}
+        release: {{ .Release.Name }}
+    spec:
+      serviceAccountName: {{ template "patroni.serviceAccountName" . }}
+      containers:
+      - name: {{ .Chart.Name }}
+        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
+        imagePullPolicy: {{ .Values.image.pullPolicy }}
+        env:
+        - name: PGPASSWORD_SUPERUSER
+          valueFrom:
+            secretKeyRef:
+              name: {{ template "patroni.fullname" . }}
+              key: password-superuser
+        - name: PGPASSWORD_ADMIN
+          valueFrom:
+            secretKeyRef:
+              name: {{ template "patroni.fullname" . }}
+              key: password-admin
+        - name: PGPASSWORD_STANDBY
+          valueFrom:
+            secretKeyRef:
+              name: {{ template "patroni.fullname" . }}
+              key: password-standby
+        {{- if .Values.kubernetes.dcs.enable }}
+        - name: DCS_ENABLE_KUBERNETES_API
+          value: "true"
+        - name: KUBERNETES_LABELS
+          value: {{ (printf "{ \"app\": \"%s\", \"release\": \"%s\" }" (include "patroni.fullname" .) .Release.Name) | quote }}
+        - name: KUBERNETES_SCOPE_LABEL
+          value: "app"
+        {{- if .Values.kubernetes.configmaps.enable }}
+        - name: KUBERNETES_USE_CONFIGMAPS
+          value: "true"
+        {{- end }}
+        {{- end }}
+        {{- if .Values.etcd.enable }}
+        {{- if .Values.etcd.deployChart }}
+        - name: ETCD_DISCOVERY_DOMAIN
+          value: {{default (printf "%s-etcd" .Release.Name | trunc 63) .Values.etcd.discovery }}
+        {{- else }}
+        - name: ETCD_HOST
+          value: {{ .Values.etcd.host | quote }}
+        {{- end }}
+        {{- else if .Values.zookeeper.enable }}
+        {{- if .Values.zookeeper.deployChart }}
+        - name: ZOOKEEPER_HOSTS
+          value: {{(printf "'%s-zookeeper-headless:2181'" .Release.Name | trunc 63)}}
+        {{- else }}
+        - name: ZOOKEEPER_HOSTS
+          value: {{ .Values.zookeeper.hosts | quote }}
+        {{- end }}
+        {{- else if .Values.consul.enable }}
+        {{- if .Values.consul.deployChart }}
+        - name: PATRONI_CONSUL_HOST
+          value: {{(printf "'%s-consul'" .Release.Name | trunc 63)}}
+        {{- else }}
+        - name: PATRONI_CONSUL_HOST
+          value: {{ .Values.consul.host | quote }}
+        {{- end }}
+        {{- end }}
+        - name: SCOPE
+          value: {{ template "patroni.fullname" . }}
+        {{- if .Values.walE.enable }}
+        - name: USE_WALE
+          value: {{ .Values.walE.enable | quote }}
+        {{- if .Values.walE.scheduleCronJob }}
+        - name: BACKUP_SCHEDULE
+          value: {{ .Values.walE.scheduleCronJob | quote}}
+        {{- end }}
+        {{- if .Values.walE.retainBackups }}
+        - name: BACKUP_NUM_TO_RETAIN
+          value: {{ .Values.walE.retainBackups | quote}}
+        {{- end }}
+        {{- if .Values.walE.s3Bucket }}
+        - name: WAL_S3_BUCKET
+          value: {{ .Values.walE.s3Bucket | quote }}
+        {{else if .Values.walE.gcsBucket }}
+        - name: WAL_GCS_BUCKET
+          value: {{ .Values.walE.gcsBucket | quote }}
+        {{- if .Values.walE.kubernetesSecret }}
+        - name: GOOGLE_APPLICATION_CREDENTIALS
+          value: "/etc/credentials/{{.Values.walE.kubernetesSecret}}.json"
+        {{- end }}
+        {{- end }}
+        {{- if .Values.walE.backupThresholdMegabytes }}
+        - name: WALE_BACKUP_THRESHOLD_MEGABYTES
+          value: {{ .Values.walE.backupThresholdMegabytes | quote }}
+        {{- end }}
+        {{- if .Values.walE.backupThresholdPercentage }}
+        - name: WALE_BACKUP_THRESHOLD_PERCENTAGE
+          value: {{ .Values.walE.backupThresholdPercentage | quote }}
+        {{- end }}
+        {{- else }}
+        - name: USE_WALE
+          value: ""
+        {{- end }}
+        - name: PGROOT
+          value: "{{ .Values.persistentVolume.mountPath }}/pgroot"
+        - name: POD_NAMESPACE
+          valueFrom:
+            fieldRef:
+              apiVersion: v1
+              fieldPath: metadata.namespace
+        {{- if .Values.env }}
+        {{- range $key, $val := .Values.env }}
+        - name: {{  $key | quote | upper }}
+          value: {{ $val | quote }}
+        {{- end }}
+        {{- end }}
+        ports:
+        - containerPort: 8008
+        - containerPort: 5432
+        volumeMounts:
+        - name: storage-volume
+          mountPath: "{{ .Values.persistentVolume.mountPath }}"
+          subPath: "{{ .Values.persistentVolume.subPath }}"
+        - mountPath: /etc/patroni
+          name: patroni-config
+          readOnly: true
+        {{- if .Values.walE.enable }}
+        {{- if .Values.walE.kubernetesSecret }}
+        - name: {{ .Values.walE.kubernetesSecret }}
+          mountPath: /etc/credentials
+          readOnly: true
+        {{- end }}
+        {{- end }}
+        resources:
+{{ toYaml .Values.resources | indent 10 }}
+    {{- with .Values.nodeSelector }}
+      nodeSelector:
+{{ toYaml . | indent 8 }}
+    {{- end }}
+    {{- with .Values.tolerations }}
+      tolerations:
+{{ toYaml . | indent 8 }}
+    {{- end }}
+    {{- if .Values.schedulerName }}
+      schedulerName: {{ .Values.schedulerName }}
+    {{- end }}
+    {{- if .Values.affinity }}
+      affinity:
+{{ .Values.affinity | toYaml | indent 8 }}
+    {{- else if .Values.affinityTemplate }}
+      affinity:
+{{ tpl .Values.affinityTemplate . | indent 8 }}
+    {{- end }}
+      volumes:
+      - name: patroni-config
+        secret:
+          secretName: {{ template "patroni.fullname" . }}
+      {{- if .Values.walE.enable }}
+      {{- if .Values.walE.kubernetesSecret }}
+      - name: {{ .Values.walE.kubernetesSecret }}
+        secret:
+          secretName: {{ .Values.walE.kubernetesSecret }}
+      {{- end }}
+      {{- end }}
+      {{- if not .Values.persistentVolume.enabled }}
+      - name: storage-volume
+        emptyDir: {}
+      {{- end }}
+  {{- if .Values.persistentVolume.enabled }}
+  volumeClaimTemplates:
+    - metadata:
+        name: storage-volume
+        annotations:
+        {{- if .Values.persistentVolume.annotations }}
+{{ toYaml .Values.persistentVolume.annotations | indent 8 }}
+        {{- end }}
+        labels:
+          app: {{ template "patroni.fullname" . }}
+          release: {{ .Release.Name }}
+          heritage: {{ .Release.Service }}
+      spec:
+        accessModes:
+{{ toYaml .Values.persistentVolume.accessModes | indent 8 }}
+        resources:
+          requests:
+            storage: "{{ .Values.persistentVolume.size }}"
+      {{- if .Values.persistentVolume.storageClass }}
+      {{- if (eq "-" .Values.persistentVolume.storageClass) }}
+        storageClassName: ""
+      {{- else }}
+        storageClassName: "{{ .Values.persistentVolume.storageClass }}"
+      {{- end }}
+      {{- end }}
+  {{- end }}
diff --git a/challenge-devops/helm/patroni/templates/svc-patroni.yaml b/challenge-devops/helm/patroni/templates/svc-patroni.yaml
new file mode 100755
index 0000000..27e01bb
--- /dev/null
+++ b/challenge-devops/helm/patroni/templates/svc-patroni.yaml
@@ -0,0 +1,16 @@
+apiVersion: v1
+kind: Service
+metadata:
+  name: {{ template "patroni.fullname" . }}
+  labels:
+    app: {{ template "patroni.fullname" . }}
+    chart: {{ template "patroni.chart" . }}
+    release: {{ .Release.Name }}
+    heritage: {{ .Release.Service }}
+spec:
+  type: ClusterIP
+  ports:
+  - name: postgresql
+    port: 5432
+    targetPort: postgresql
+    protocol: TCP
diff --git a/challenge-devops/helm/patroni/values-dev.yaml b/challenge-devops/helm/patroni/values-dev.yaml
new file mode 100755
index 0000000..26cca35
--- /dev/null
+++ b/challenge-devops/helm/patroni/values-dev.yaml
@@ -0,0 +1,127 @@
+replicaCount: 5
+
+image:
+  # Image was built from
+  # https://github.com/zalando/spilo/tree/master/postgres-appliance
+  repository: registry.opensource.zalan.do/acid/spilo-10
+  tag: 1.5-p5
+  pullPolicy: IfNotPresent
+
+# Credentials used by Patroni
+# https://github.com/zalando/patroni/blob/master/docs/SETTINGS.rst#postgresql
+# https://github.com/zalando/spilo/blob/master/ENVIRONMENT.rst
+credentials:
+  superuser: tea
+  admin: cola
+  standby: pinacolada
+
+# Distribution Configuration stores
+# Please note that only one of the following stores should be enabled.
+kubernetes:
+  dcs:
+    enable: true
+  configmaps:
+    enable: false
+etcd:
+  enable: false
+  deployChart: false
+  # If not deploying etcd chart, fill-in value for etcd service
+  # <service>.<namespace>.svc.cluster.local
+  host:
+  # Leave blank to use vendored etcd chart
+  discovery:
+zookeeper:
+  enable: false
+  deployChart: false
+  # If not deploying etcd chart, fill-in list of ZooKeeper members in format:
+  # 'host1:port1','host2:port2','etc...'
+  hosts:
+consul:
+  enable: false
+  deployChart: false
+  # Leave blank to use vendored consul chart
+  hosts:
+
+# Extra custom environment variables.
+env: {}
+
+walE:
+  # Specifies whether Wal-E should be enabled
+  enable: false
+  # Cron schedule for doing base backups
+  scheduleCronJob: 00 01 * * *
+  # Amount of base backups to retain
+  retainBackups: 2
+  # Path to the S3 or GCS bucket used for WAL-E base backups
+  s3Bucket:
+  gcsBucket:
+  # Name of the secret that holds the credentials to the bucket
+  kubernetesSecret:
+  # Maximum size of the WAL segments accumulated after the base backup to
+  # consider WAL-E restore instead of pg_basebackup
+  backupThresholdMegabytes: 1024
+  # Maximum ratio (in percents) of the accumulated WAL files to the base backup
+  # to consider WAL-E restore instead of pg_basebackup
+  backupThresholdPercentage: 30
+
+persistentVolume:
+  enabled: true
+  size: 1G
+  ## database data Persistent Volume Storage Class
+  ## If defined, storageClassName: <storageClass>
+  ## If set to "-", storageClassName: "", which disables dynamic provisioning
+  ## If undefined (the default) or set to null, no storageClassName spec is
+  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
+  ##   GKE, AWS & OpenStack)
+  ##
+  # storageClass: "-"
+  subPath: ""
+  mountPath: "/home/postgres/pgdata"
+  annotations: {}
+  accessModes:
+    - ReadWriteOnce
+
+resources: {}
+  # If you do want to specify resources, uncomment the following
+  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
+  # limits:
+  #   cpu: 100m
+  #   memory: 128Mi
+  # requests:
+  #   cpu: 100m
+  #   memory: 128Mi
+
+# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
+nodeSelector: {}
+
+# https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
+tolerations: []
+
+# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
+affinityTemplate: |
+  podAntiAffinity:
+    preferredDuringSchedulingIgnoredDuringExecution:
+    - weight: 100
+      podAffinityTerm:
+        topologyKey: "kubernetes.io/hostname"
+        labelSelector:
+          matchLabels:
+            app:  {{ template "patroni.name" . }}
+            release: {{ .Release.Name | quote }}
+affinity: {}
+
+## Use an alternate scheduler, e.g. "stork".
+## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
+##
+# schedulerName:
+
+rbac:
+  # Specifies whether RBAC resources should be created
+  create: true
+
+serviceAccount:
+  # Specifies whether a ServiceAccount should be created
+  create: true
+  # The name of the ServiceAccount to use.
+  # If not set and create is true, a name is generated using the fullname template
+  name:
diff --git a/challenge-devops/helm/patroni/values-prod.yaml b/challenge-devops/helm/patroni/values-prod.yaml
new file mode 100755
index 0000000..26cca35
--- /dev/null
+++ b/challenge-devops/helm/patroni/values-prod.yaml
@@ -0,0 +1,127 @@
+replicaCount: 5
+
+image:
+  # Image was built from
+  # https://github.com/zalando/spilo/tree/master/postgres-appliance
+  repository: registry.opensource.zalan.do/acid/spilo-10
+  tag: 1.5-p5
+  pullPolicy: IfNotPresent
+
+# Credentials used by Patroni
+# https://github.com/zalando/patroni/blob/master/docs/SETTINGS.rst#postgresql
+# https://github.com/zalando/spilo/blob/master/ENVIRONMENT.rst
+credentials:
+  superuser: tea
+  admin: cola
+  standby: pinacolada
+
+# Distribution Configuration stores
+# Please note that only one of the following stores should be enabled.
+kubernetes:
+  dcs:
+    enable: true
+  configmaps:
+    enable: false
+etcd:
+  enable: false
+  deployChart: false
+  # If not deploying etcd chart, fill-in value for etcd service
+  # <service>.<namespace>.svc.cluster.local
+  host:
+  # Leave blank to use vendored etcd chart
+  discovery:
+zookeeper:
+  enable: false
+  deployChart: false
+  # If not deploying etcd chart, fill-in list of ZooKeeper members in format:
+  # 'host1:port1','host2:port2','etc...'
+  hosts:
+consul:
+  enable: false
+  deployChart: false
+  # Leave blank to use vendored consul chart
+  hosts:
+
+# Extra custom environment variables.
+env: {}
+
+walE:
+  # Specifies whether Wal-E should be enabled
+  enable: false
+  # Cron schedule for doing base backups
+  scheduleCronJob: 00 01 * * *
+  # Amount of base backups to retain
+  retainBackups: 2
+  # Path to the S3 or GCS bucket used for WAL-E base backups
+  s3Bucket:
+  gcsBucket:
+  # Name of the secret that holds the credentials to the bucket
+  kubernetesSecret:
+  # Maximum size of the WAL segments accumulated after the base backup to
+  # consider WAL-E restore instead of pg_basebackup
+  backupThresholdMegabytes: 1024
+  # Maximum ratio (in percents) of the accumulated WAL files to the base backup
+  # to consider WAL-E restore instead of pg_basebackup
+  backupThresholdPercentage: 30
+
+persistentVolume:
+  enabled: true
+  size: 1G
+  ## database data Persistent Volume Storage Class
+  ## If defined, storageClassName: <storageClass>
+  ## If set to "-", storageClassName: "", which disables dynamic provisioning
+  ## If undefined (the default) or set to null, no storageClassName spec is
+  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
+  ##   GKE, AWS & OpenStack)
+  ##
+  # storageClass: "-"
+  subPath: ""
+  mountPath: "/home/postgres/pgdata"
+  annotations: {}
+  accessModes:
+    - ReadWriteOnce
+
+resources: {}
+  # If you do want to specify resources, uncomment the following
+  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
+  # limits:
+  #   cpu: 100m
+  #   memory: 128Mi
+  # requests:
+  #   cpu: 100m
+  #   memory: 128Mi
+
+# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
+nodeSelector: {}
+
+# https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
+tolerations: []
+
+# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
+affinityTemplate: |
+  podAntiAffinity:
+    preferredDuringSchedulingIgnoredDuringExecution:
+    - weight: 100
+      podAffinityTerm:
+        topologyKey: "kubernetes.io/hostname"
+        labelSelector:
+          matchLabels:
+            app:  {{ template "patroni.name" . }}
+            release: {{ .Release.Name | quote }}
+affinity: {}
+
+## Use an alternate scheduler, e.g. "stork".
+## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
+##
+# schedulerName:
+
+rbac:
+  # Specifies whether RBAC resources should be created
+  create: true
+
+serviceAccount:
+  # Specifies whether a ServiceAccount should be created
+  create: true
+  # The name of the ServiceAccount to use.
+  # If not set and create is true, a name is generated using the fullname template
+  name:
-- 
2.25.1

